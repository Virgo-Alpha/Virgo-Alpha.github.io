[
  {
    "id": "doc-0",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "In this article, I discuss Feature Selection On Zindi Starter Notebook On the path to making a predictive model, we are sometimes faced with the choice to cherry pick amongst our list of features. (If the term cherrypick still gives you nightmares from your adventures in gitland then here‚Äôs a fitbump üëä). Perhaps this is because of the high dimensionality of our data or just in the cyclic model hyperparameter finetuning. Regardless of the reason, learning the different ways you can employ to decide which features to use and which to not use can end up improving model performance or even reducing computational time and complexity. EDA, Cleaning and Preprocessing For this exercise, I employed the Financial Inclusion in Africa Competition in Zindi because of 2 reasons: Readily available data A starter notebook that deals with EDA and basic data cleaning As such, the very next step from these offerings in the competition is to do feature engineering.",
    "section": 0
  },
  {
    "id": "doc-1",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "If you want to join in for a follow-along kind of reading, feel free to download the data and starter Notebook from Zindi here (You might need to create a Zindi account though). The starter notebook makes use of the following beautiful function that is used to transform both the test and the train datasets.",
    "section": 1
  },
  {
    "id": "doc-2",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "function to preprocess our data from train models def preprocessing_data(data):      Convert the following numerical labels from interger to float     float_array = data[[\"household_size\", \"age_of_respondent\", \"year\"]].values.astype(float)      categorical features to be onverted to One Hot Encoding     categ = [\"relationship_with_head\",              \"marital_status\",              \"education_level\",              \"job_type\",              \"country\"]      One Hot Encoding conversion     data = pd.get_dummies(data, prefix_sep=\"_\", columns=categ)      Label Encoder conversion     data[\"location_type\"] = le.fit_transform(data[\"location_type\"])     data[\"cellphone_access\"] = le.fit_transform(data[\"cellphone_access\"])     data[\"gender_of_respondent\"] = le.fit_transform(data[\"gender_of_respondent\"])      drop uniquid column     data = data.drop([\"uniqueid\"], axis=1)      scale our data into range of 0 and 1     scaler = MinMaxScaler(feature_range=(0, 1))     data = scaler.fit_transform(data)     return data                   Unfortunately, after the transformations, we have a long list of 37 features.",
    "section": 2
  },
  {
    "id": "doc-3",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "Although we can use all of them it is advisable to select the best features which would help us best predict the target variable. After the processing, we are left with two numpy array sets. preprocess the train data  processed_train = preprocessing_data(X_train) processed_test = preprocessing_data(test) I converted the arrays to dataframes and then saved them to CSV files for easy processing in another notebook. Save to csv processed_train = pd.DataFrame(processed_train) processed_test = pd.DataFrame(processed_test) processed_train.to_csv('data/preprocessed_train.csv', index = False) processed_test.to_csv('data/preprocessed_test.csv', index = False) I could have simply used them in the starter Notebook but I wanted to have a saved version of my pre-processed data for future experimentation with feature Engineering and other techniques that would better model performance.",
    "section": 3
  },
  {
    "id": "doc-4",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "Feature Selection In another notebook, I imported the required libraries as well as the datasets: Code block Afterwards, I started the different ways to select features. 1. Univariate Statistics Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features. Many different statistical test scan be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data. This can be used via the f_classif() function.",
    "section": 4
  },
  {
    "id": "doc-5",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "from sklearn.feature_selection import SelectKBest from numpy import set_printoptions from sklearn.feature_selection import f_classif  feature extraction test = SelectKBest(score_func=f_classif, k=4) fit = test.fit(X, y)  summarize scores set_printoptions(precision=3) print(fit.scores_) features = fit.transform(X)  summarize selected features print(features[0:5,:]) We will use the chi method to select the 10 best features using this method in the example below. Code Block Alternatives to ch-squared and ANOVA F-value (all imported from sklearn.feature_selection) Mutual Information: Measures the mutual dependence between two variables. Information Gain: Measures the reduction in entropy achieved by splitting data on a particular feature. Correlation Coefficient: Measures the linear relationship between two numerical variables. Distance Correlation: Measures the dependence between two random variables.",
    "section": 5
  },
  {
    "id": "doc-6",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "ReliefF: Computes feature importance based on the ability to distinguish between instances of different classes. 2. Feature Importance Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. Code Block 3. Recursive Feature Elimination The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. You can learn more about the RFE class in the scikit-learn documentation. The example below uses RFE with the logistic regression algorithm to select the top 10 features.",
    "section": 6
  },
  {
    "id": "doc-7",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "The choice of algorithm does not matter too much as long as it is skillful and consistent. Code Block 4. Principal Component Analysis (PCA) Principal Component Analysis(or PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result. In the example below, we use PCA and select 3 principal components. Learn more about the PCA class in scikit-learn by reviewing the PCA API. Dive deeper into the math behind PCA on the Principal Component Analysis Wikipedia article. PCA Usefulness: Dimension reduction: When dealing with datasets containing a large number of features, PCA can help reduce the dimensionality while preserving most of the variability in the data. This can lead to simpler models, reduced computational complexity, and alleviation of the curse of dimensionality.",
    "section": 7
  },
  {
    "id": "doc-8",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "An example is here where we have 37 features Data exploration/visualization: PCA can be used to visualize high-dimensional data in lower-dimensional space (e.g., 2D or 3D) for exploratory data analysis and visualization. This can help uncover patterns, clusters, and relationships between variables. Noise reduction: PCA identifies and removes redundant information (noise) in the data by focusing on the directions of maximum variance. This can lead to improved model performance by reducing overfitting and improving generalization Feature Creation: PCA can be used to create new composite features (principal components) that capture the most important information in the original features. These components may be more informative or less correlated than the original features, potentially enhancing the performance of machine learning algorithms.",
    "section": 8
  },
  {
    "id": "doc-9",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "Reducing computational Complexity: In cases where the original dataset is large and computationally expensive to process, PCA can be used to reduce the size of the dataset without sacrificing much information. This can lead to faster training and inference times for machine learning models. Addressing multicollinearity in the features: PCA can mitigate multicollinearity issues by transforming correlated features into orthogonal principal components. This can improve the stability and interpretability of regression models. Code Block 5. Correlation Matrix With HeatMap A correlation matrix is a square matrix that shows the correlation coefficients between pairs of variables in a dataset. Each cell in the matrix represents the correlation coefficient between two variables. The correlation coefficient ranges from -1 to 1, where: 1 indicates a perfect positive correlation, 0 indicates no correlation, and -1 indicates a perfect negative correlation.",
    "section": 9
  },
  {
    "id": "doc-10",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "A heatmap is a graphical representation of the correlation matrix, where each cell's color indicates the strength and direction of the correlation between two variables. Darker colors (e.g., red) represent stronger positive correlations, while lighter colors (e.g., blue) represent stronger negative correlations. The diagonal of the heatmap typically shows correlation values of 1, as each variable is perfectly correlated with itself. By visualizing the correlation matrix as a heatmap, you can quickly identify patterns of correlation between features. In the heatmap, you can look for clusters of high correlation (e.g., dark squares) to identify groups of features that are highly correlated with each other. Once identified, you can decide whether to keep, remove, or transform these features based on their importance to the model and their contribution to multicollinearity.",
    "section": 10
  },
  {
    "id": "doc-11",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "get correlations of each features in dataset corrmat = X.corr() top_corr_features = corrmat.index plt.figure(figsize=(37,37)) plot heat map g=sns.heatmap(X[top_corr_features].corr(),annot=True,cmap=\"coolwarm\") Conclusion In Conclusion, feature selection is an important part of the machine learning process which can have a myriad of benefits. There are a multitude of ways to go about it but depending on your particular use case, be it supervised or unsupervised, some may be better than others. For instance, in supervised learning as was our case above, we compared the features against their usefulness in determining a target variable. However, in unsupervised learning, there is no target variable. As such, most feature selection algorithms that are useful here compare the variables to each other, helping cull out the ones with high multicollinearity. Code Block Another important matrix in determining the Feature selection algorithm to use is the data types of your features.",
    "section": 11
  },
  {
    "id": "doc-12",
    "source": "zindi_nb_article",
    "title": "Feature Selection On Zindi Starter Notebook",
    "content": "In general, different data types need different selection algorithms. Please see below: Code Block Have a happy time exploring the other algorithms and may the force be with you!",
    "section": 12
  },
  {
    "id": "doc-13",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "In this article, I discuss Terraform on AWS: An introductory guide Terraform Overview When people hear the term terraform, they often think of terraforming planets‚Äîa concept popularized by scientists and visionaries like Elon Musk, who envisions making Mars habitable. In this case, terraform means developing a rock or dead planet that is inhabitable so that it can have the necessary conditions to be able to sustain life. Terraforming Mars would involve generating an atmosphere, introducing water sources, and fostering plant life to create conditions where humans could survive. Similarly, in the world of software development, HashiCorp‚Äôs Terraform follows the same principle‚Äîexcept instead of reshaping planets, it transforms cloud platforms like AWS, GCP, and vSphere, or on premise resources, into structured environments where applications can thrive.",
    "section": 0
  },
  {
    "id": "doc-14",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "Just as planetary terraforming establishes the foundation for life, Terraform as Infrastructure-as-Code (IaC) lays the groundwork for scalable and automated infrastructure where software can run seamlessly. What is Terraform? Terraform is an Infrastructure as Code (IaC) tool developed by HashiCorp. It allows users to define cloud and on-premises infrastructure using human-readable configuration files. The tool provides a consistent workflow to provision, manage, and automate infrastructure across its lifecycle. Why Use Terraform? Simplicity ‚Äì All infrastructure is defined in a single file, making it easy to track changes. Collaboration ‚Äì Code can be stored in version control systems like GitHub for team collaboration. Reproducibility ‚Äì Configurations can be reused for different environments (e.g., development and production). Resource Cleanup ‚Äì Ensures unused resources are properly destroyed to avoid unnecessary costs.",
    "section": 1
  },
  {
    "id": "doc-15",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "What Terraform is NOT Not a Software Deployment Tool ‚Äì It doesn‚Äôt manage or update software on existing infrastructure. Cannot Modify Immutable Resources ‚Äì Some changes (e.g., VM type) require destroying and recreating the resource. Does Not Manage External Resources ‚Äì Terraform only manages what is explicitly defined in its configuration files. Terraform Workflow Terraform Installed Locally ‚Äì The CLI runs on a user‚Äôs machine. Uses Providers ‚Äì These connect Terraform to cloud services (AWS, Azure, GCP, etc.). Authentication Required ‚Äì API keys or service accounts authenticate access to cloud platforms. Terraform Installation Go to this link and follow the command that match your system‚Äôs specification Key Terraform Commands terraform init ‚Äì Downloads provider plugins and initializes the working directory. terraform plan ‚Äì Shows what changes Terraform will make before applying them. terraform apply ‚Äì Provisions the defined infrastructure.",
    "section": 2
  },
  {
    "id": "doc-16",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "terraform destroy ‚Äì Removes all resources defined in the Terraform configuration. Terraform Files and Their Generation Terraform uses several files to manage your infrastructure state, configuration, and dependencies. Below is an overview of the key files, what they represent, and which Terraform command triggers their creation or update: Configuration Files (.tf files): Purpose: These files (such as main.tf, variables.tf, outputs.tf, etc.) are written by you to define your infrastructure. They describe the resources you wish to provision and how they interrelate. When They Are Created: You create these manually. They form the blueprint for Terraform to understand and manage your environment. Terraform State File (terraform.tfstate): Purpose: This file tracks the current state of your infrastructure. It maps your configuration to the real-world resources, ensuring that Terraform can determine what changes need to be made.",
    "section": 3
  },
  {
    "id": "doc-17",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "When It Is Generated/Updated: After terraform apply: When you run terraform apply, Terraform provisions your infrastructure based on your configuration. During this process, it creates or updates the terraform.tfstate file with the current state of the resources. After terraform destroy: Similarly, when you destroy resources, the state file is updated to reflect that the resources no longer exist. Terraform Lock File (.terraform.lock.hcl): Purpose: This file locks the versions of the provider plugins used in your configuration to ensure consistency and prevent unexpected changes from newer versions. When It Is Generated/Updated: After terraform init: Running terraform init downloads the required provider plugins and creates the .terraform.lock.hcl file. This ensures that every team member or CI/CD pipeline uses the same provider versions. Terraform Directory (.terraform): Purpose: This hidden directory stores downloaded provider plugins, module sources, and backend configuration.",
    "section": 4
  },
  {
    "id": "doc-18",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "It is essential for Terraform's operation. When It Is Generated: After terraform init: The .terraform directory is automatically created when you initialize your Terraform working directory using terraform init. Plan Output File (optional): Purpose: If you choose to save the execution plan to a file (using the -out flag with terraform plan), this binary file captures the set of changes Terraform intends to make. When It Is Generated: After terraform plan -out=<filename>: Running this command generates a plan file that can later be applied using terraform apply <filename>. This is useful for reviewing changes or automating deployment workflows. Configure aws credentials locally This is important if you want to be able to access your aws account and resources on your terminal or inside your code using an sdk such as boto3. There might be other ways to do this but I will list 2 here: aws configure Export the credentials as environment variables 1.",
    "section": 5
  },
  {
    "id": "doc-19",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "aws configure To use this, you will need to install aws-cli which you can do from here. I found that sudo apt install aws-cli or pip3 install aws-cli worked just as well for me. You can confirm that you have installed it by checking it‚Äôs version as below: aws --version To configure your credentials locally, you will need to create an IAM user and give them some permissions. Log onto your aws console Navigate to the IAM section Create a new user and grant them the required permissions Download the access key and secret key as csv of that user and store it securely. I prefer this rather than just copying them from the console. Just make sure not to commit them publicly. For example, if you‚Äôre working on a repository that has a remote version in github/gitlab/bitbucket etc then consider adding the csv to your git ignore before adding, committing and pushing changes Now, run the following command to configure your credentials locally.",
    "section": 6
  },
  {
    "id": "doc-20",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "aws configure You will be prompted to enter the access key id, secret access key, region, and output format (choose json, text, or table, default is json). Once you fill them, it creates 2 files in the ~/.aws directory: credentials and config. The credentials file contains the access key and secret key. The config file contains the region and output format. To test if you have access to your aws account from your local terminal, create a dummy s3 bucket then run the following command to list your buckets: aws s3 ls 2.",
    "section": 7
  },
  {
    "id": "doc-21",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "Export the credentials as environment variables Run the following command on your terminal, replacing the stringed text with your actual values: export AWS_ACCESS_KEY_ID=\"your-access-key-id\" export AWS_SECRET_ACCESS_KEY=\"your-secret-access-key\" export AWS_DEFAULT_REGION=\"your-region\" Terraform, Boto3, and AWS CLI will automatically use these from the environment variables or the files in the ~/.aws directory Managing resources on terraform  Create a main.tf file in the folder you are working in and save the following code in the file: terraform {   required_providers {     aws = {       source = \"hashicorp/aws\"       version = \"5.85.0\"     }   } } provider \"aws\" {    Configuration options   region = \"your-region\" }  Variable definitions variable \"aws_region\" {   description = \"AWS region for resources\"   type        = string   default     = \"your-region\" } resource \"aws_s3_bucket\" \"example\" {   bucket = \"my-tf-test-bucket-${random_id.bucket_suffix.hex}\"  Make bucket name unique   tags = {     Name        = \"My bucket\"     Environment = \"Dev\"   } }  Add a random suffix to ensure bucket name uniqueness resource \"random_id\" \"bucket_suffix\" {   byte_length = 4 } Replace ‚Äúyour-region‚Äù with your actual region.",
    "section": 8
  },
  {
    "id": "doc-22",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "Now we can run the terraform commands. 1. Initialize terraform in your folder Run the following command: terraform init It initializes the backend and provider plugins. It also creates a lock file .terraform.lock.hcl to record the provider selections. It also creates a .terraform folder. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. 2. Plan Run the following command to see any changes that are required for your infrastructure: terraform plan It generates an execution plan based on the code in main.tf. At the end of the output there is a summary that looks like the below: Plan: 2 to add, 0 to change, 0 to destroy. Changes will be suggested which you can agree to by running the apply command explained below. You can save the plan by using the out flag: terraform plan -out=filepath-to-save-file 3.",
    "section": 9
  },
  {
    "id": "doc-23",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "Apply To apply the changes suggested run the command: terraform apply You will be prompted to confirm by typing yes. Be careful as this does create the resources thus you will incur costs on your aws account unless you have cloud credits or are using the free tier. After typing yes, go to the console and navigate to the s3 section. Check if you have a new bucket created. Alternatively, you can run the following command to list your buckets: aws s3 ls If you had previously saved your plan to a file, please run the following command to apply that plan: terraform apply ‚Äúfilename‚Äù 4. Delete Run the following command to delete the resources provisioned by terraform terraform destroy Your resources marked for deletion will be listed and you will again be prompted for confirmation. Confirm by typing yes.",
    "section": 10
  },
  {
    "id": "doc-24",
    "source": "terraform_on_aws_article",
    "title": "Terraform on AWS: An introductory guide",
    "content": "Summary This guide walks you through setting up AWS credentials locally using both the AWS CLI configuration and environment variables, and demonstrates how to manage AWS resources with Terraform. You learned how to write a basic Terraform configuration to create an S3 bucket, initialize your project, preview changes with a plan, apply those changes, and ultimately destroy the resources when they are no longer needed. This systematic approach to infrastructure management not only ensures consistency and repeatability but also aligns with modern DevOps best practices.",
    "section": 11
  },
  {
    "id": "doc-25",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "In this article, I discuss how Keeping your Streamlit app awake using Selenium and Github Actions TLDR; Streamlit apps sleep after a period of inactivity if hosted on Streamlit Community Cloud (free tier) To wake your app up, you need to click a button We can use Github actions + Selenium to automate this button clicking every couple of hours Table of Contents Introduction Step 1: Create Repo Step 2: Create Python Script Step 3: Create Github Workflow Step 4: Commit and Push Step 5: Run the workflow manually Conclusion Introduction Streamlit apps hosted on the Community Edition (free tier) go to sleep after some period of inactivity. This used to be 24 hours during weekdays and 72 hours on weekends but it had been cut down to 12 hours and that number might even be lower now. Given how we have used Streamlit for project demos and portfolio pages, this is quite a concern.",
    "section": 0
  },
  {
    "id": "doc-26",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "In this article, I explore how to use Github actions to run a script every 4 hours that keep your Streamlit app from going to sleep. Step 1: Create Repo On your Github account, create a new repo and pull it locally in your desired folder.",
    "section": 1
  },
  {
    "id": "doc-27",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "Step 2: Create Python Script In your local repository, paste the following code in main.py file: from selenium import webdriver from selenium.webdriver.chrome.service import Service from selenium.webdriver.chrome.options import Options from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from webdriver_manager.chrome import ChromeDriverManager from selenium.common.exceptions import TimeoutException import os  Streamlit app URL from environment variable (or default) STREAMLIT_URL = os.environ.get(\"STREAMLIT_APP_URL\", \"https://benson-mugure-portfolio.streamlit.app/\") def main():     options = Options()     options.add_argument('--headless=new')     options.add_argument('--no-sandbox')     options.add_argument('--disable-dev-shm-usage')     options.add_argument('--disable-gpu')     options.add_argument('--window-size=1920,1080')     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)     try:         driver.get(STREAMLIT_URL)         print(f\"Opened {STREAMLIT_URL}\")         wait = WebDriverWait(driver, 15)         try:              Look for the wake-up button             button = wait.until(                 EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Yes, get this app back up')]\"))             )             print(\"Wake-up button found.",
    "section": 2
  },
  {
    "id": "doc-28",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "Clicking...\")             button.click()              After clicking, check if it disappears             try:                 wait.until(EC.invisibility_of_element_located((By.XPATH, \"//button[contains(text(),'Yes, get this app back up')]\")))                 print(\"Button clicked and disappeared ‚úÖ (app should be waking up)\")             except TimeoutException:                 print(\"Button was clicked but did NOT disappear ‚ùå (possible failure)\")                 exit(1)         except TimeoutException:              No button at all ‚Üí app is assumed to be awake             print(\"No wake-up button found. Assuming app is already awake ‚úÖ\")     except Exception as e:         print(f\"Unexpected error: {e}\")         exit(1)     finally:         driver.quit()         print(\"Script finished.\") if __name__ == \"__main__\":     main() Replace the value of the STREAMLIT_URL variable with your own app‚Äôs URL as a string.",
    "section": 3
  },
  {
    "id": "doc-29",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "Step 3: Create Github Workflow Also in your local repository, create the file .github/workflows/wake.yml and paste the code below: name: Wake Streamlit App on:   schedule:     - cron: \"0 /4   \"    every 4 hours   workflow_dispatch:          allow manual trigger jobs:   wake:     runs-on: ubuntu-latest     steps:       - name: Checkout repo         uses: actions/checkout@v4       - name: Set up Python         uses: actions/setup-python@v5         with:           python-version: \"3.10\"       - name: Install dependencies         run: |           python -m pip install --upgrade pip           pip install -r requirements.txt       - name: Run Selenium script         run: python main.py At the base of your repository, add a requirements.txt file with the following contents: selenium webdriver-manager Step 4: Commit and Push Run the following commands to add and commit your changes: git add .",
    "section": 4
  },
  {
    "id": "doc-30",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "git commit -m ‚Äúadd files‚Äù git push Needless to say, you can modify the commit message as you wish Step 5: Run the workflow manually Go to your repository on Github and confirm that the changes have been made, i.e., your files have been pushed. On that repo, click on the Actions tab as can be seen below: Github Menu You should be able to see the Wake Streamlit App workflow on the right, right under All Workflows. Click on the Wake Streamlit App. On the right, you should see a button that says Run Workflow. Click it. Another green button with the same label appears. Click it too. You will now see the workflow is in progress. This takes about 2 minutes. After the time has lapsed, check if your Streamlit app is awake. If not, check the logs on Github and debug. Conclusion And there you have it, a simple and free way to keep your Streamlit app awake (hopefully forever). You can find the full code example in this repository here.",
    "section": 5
  },
  {
    "id": "doc-31",
    "source": "streamlit_app_awake_article",
    "title": "Keeping your Streamlit app awake using Selenium and Github Actions",
    "content": "There is also an option to modify this workflow so that it makes an empty commit to the repository that the Streamlit app is deployed from in that same branch. Perhaps you can explore this option as well if you are feeling adventurous. Empty commits do not necessarily wake your app up but they do reset the clock that is counting down idle time to set your app to sleep. Let me know if you found this article helpful.",
    "section": 6
  },
  {
    "id": "doc-32",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "In this article, I discuss Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS. TLDR; I built Sentinel, a serverless, agentic pipeline that turns noisy RSS feeds into actionable cybersecurity intel (dedup, triage, publish). Deterministic first, agentic later: Step Functions ‚Üí Lambdas, then flipped a feature flag to Bedrock AgentCore (Strands) without changing contracts. Reliability by design: SQS buffering, DLQs, idempotency keys, guardrails, and graceful degradation (semantic ‚Üí heuristic). Search that scales: OpenSearch Serverless with BM25 + vectors, cached embeddings, clusters for near-dupes. Secure & observable: Cognito + least privilege, KMS, WAF, VPC endpoints, JSON logs + X-Ray, SLOs & cost alarms.",
    "section": 0
  },
  {
    "id": "doc-33",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Table Of Contents Introduction Problem statement My solution The architecture Use of Strands Use of Bedrock Human in the loop Challenges and breakthroughs Key learnings Future Plans Conclusion Introduction Exploring AWS AI offerings has been on my TODO list for the longest time. I was particularly interested in Strands, Bedrock and Nova Act. Thus, for this AI Engineering month, I decided to take on the challenge to solve a practical problem that I have seen in my industry using these tools, while learning and exploring in the process. I recently earned the AWS Certified Solutions Architect Associate certification and also got access to Kiro so this project allowed me to play the part of a technical PM and apply my system design skills. I hope you learn something that may aid you in your work. Enjoy.",
    "section": 1
  },
  {
    "id": "doc-34",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Problem statement As my company‚Äôs CISO, I would like to develop an internal cybersecurity newsletter that collates news from different RSS feeds, filters out those relevant to my organization based on a list of keywords and shares them with fellow employees either via email or published on an internal site. I wanted to be kept abreast of the latest happenings in the industry but I want to automatically share anything that may be relevant so that‚Äôs why I expanded my requirements. My solution Sentinel is an AWS-native, multi-agent cybersecurity news triage and publishing system that autonomously ingests, processes, and publishes cybersecurity intelligence from RSS feeds and news sources. The system reduces analyst workload by automatically deduplicating content, extracting relevant entities, and intelligently routing items for human review or auto-publication.",
    "section": 2
  },
  {
    "id": "doc-35",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "For the full code, visit my repository here Information flow The architecture I designed a decoupled, serverless microservices architecture that scales cleanly and kept costs predictable. The core remained a set of Lambda functions behind Step Functions, with EventBridge schedules kicking off ingestion. I routed all content through a buffered pipeline: EventBridge fanned into SQS so bursts of feeds didn‚Äôt cascade into failures, and every consumer Lambda processed messages idempotently using a canonical-URL SHA-256 as the key. I attached DLQs at each hop (EventBridge, SQS consumers, Step Functions tasks) and wrote compensation paths so partial successes (e.g., stored raw content but failed dedup) re-queued safely. My architecture I modeled storage deliberately. In DynamoDB I used a primary table for articles with GSIs for statepublished_at (queues and dashboards), cluster_idpublished_at (duplicates), and tagspublished_at (topic browsing).",
    "section": 3
  },
  {
    "id": "doc-36",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "I enabled TTL for short-term session memory and configured PITR for recovery. For search, I provisioned OpenSearch Serverless with a BM25 collection for keyword queries and a k-NN vector collection for semantic near-duplicate detection. I cached embeddings by content hash to avoid recomputation and cut latency. On the agent side, I started with direct orchestration so I could validate the pipeline deterministically. Step Functions called Lambdas (FeedParser ‚Üí Relevancy ‚Üí Dedup ‚Üí Summarize ‚Üí Guardrail ‚Üí Decision), and a thin ‚Äúagent shim‚Äù Lambda exposed the same interface I knew I‚Äôd use later. When I was ready, I deployed my Strands-defined Ingestor and Analyst Assistant agents to Bedrock AgentCore and flipped a feature flag so Step Functions invoked the agents instead. If AgentCore became unavailable or too slow, the same flag let me fall back instantly to direct Lambda orchestration. I treated configuration and behavior as data.",
    "section": 4
  },
  {
    "id": "doc-37",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "I moved feed lists, keyword taxonomies, similarity thresholds, guardrail strictness, and rollout switches into SSM Parameter Store, and I read them at runtime. I kept a small but explicit flags matrix: enable_agents (direct vs AgentCore), enable_opensearch (heuristic vs semantic dedup), enable_amplify (backend-only vs full stack), enable_guardrails_strict, and enable_digest_email. This let me ship incrementally without redeploys. For Bedrock usage, I separated concerns. I used LLM calls to score relevance to my taxonomy and extract entities (CVE IDs, threat actors, malware, vendors/products) with confidences and rationales. I generated two summaries per item (executive two-liner and analyst card) and enforced a reflection checklist so outputs consistently included who/what/impact/source. I produced embeddings for semantic search and dedup, and I versioned prompts and model IDs in SSM, logging token usage per call.",
    "section": 5
  },
  {
    "id": "doc-38",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Every LLM output passed a JSON-Schema validation step; failures, PII findings, or suspicious CVE formats triggered Human Escalation automatically. I also kept a small ‚Äúgolden set‚Äù of seeded dupes and fake CVEs to regression-test prompts and thresholds. Security and networking were explicit. I authenticated users through Cognito user and identity pools and authorized them with group roles (Analyst/Admin) mapped to least-privilege IAM policies. I stored secrets in Secrets Manager (and encrypted everything with KMS) and placed WAF in front of Amplify/API Gateway, with usage plans and rate limits. I used Gateway VPC endpoints for S3 and DynamoDB and added interface endpoints selectively (Bedrock/OpenSearch) where the security benefit outweighed their per-hour cost. I documented a PII policy: I kept raw HTML in a restricted S3 prefix, stored normalized/redacted text separately, applied tight access controls, and retained artifacts only as long as needed.",
    "section": 6
  },
  {
    "id": "doc-39",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Cognito and Amplify architecture Observability and operations were top priority. I standardized on structured JSON logs with correlation IDs flowing from EventBridge through Step Functions, Lambdas, and agent tool calls, and I enabled X-Ray tracing end-to-end. I tracked SLOs and KPIs‚Äîduplicate detection precision, auto-publish precision, p95 latency per stage, and cost per article‚Äîand I wired CloudWatch alarms to anomalies and DLQs. I added daily and monthly cost monitors, and I wrote short runbooks for common incidents (OpenSearch throttling, SES sandbox, token bursts). I also covered reliability and data protection. I enabled DynamoDB PITR, S3 versioning, and OpenSearch snapshots, and I documented RPO/RTO targets (‚â§15 minutes for metadata, ‚â§24 hours for search if I restored from snapshots). In a degraded state, I allowed the system to bypass semantic dedup and fall back to heuristic matching so ingestion never fully stalled. On the product and API side, I clarified contracts.",
    "section": 7
  },
  {
    "id": "doc-40",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "I exposed clean endpoints with pagination, filters, and error schemas. For exports, I generated XLSX in a worker pattern that wrote to S3 and returned pre-signed URLs, so large batches didn‚Äôt hit Lambda memory/timeouts. In the Amplify app I added a chat UI for natural-language queries with citations, a review queue with decision traces, and threaded commentary. I hardened the NL interface against prompt injection by allow-listing data sources, stripping HTML/JS from prompts, and refusing unsafe actions. Finally, I shipped it as code. I organized Terraform into modules with remote state and environment isolation, hashed Lambda artifacts for deterministic deploys, and used canary releases for riskier changes. I tagged everything for cost allocation and preserved a full audit trail‚Äîwho approved or rejected, which prompt/version ran, and the exact tool call sequence and parameters‚Äîfor a defined retention window.",
    "section": 8
  },
  {
    "id": "doc-41",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "The result read well in a demo and behaved like production: buffered, idempotent, observable, secure, and ready to toggle between deterministic pipelines and fully agentic orchestration. Use of Strands I used Strands as the authoring/orchestration layer to define two agents‚ÄîIngestor Agent and Analyst Assistant Agent‚Äîincluding their roles, instructions, and the tool bindings to my Lambda ‚Äútools‚Äù (FeedParser, RelevancyEvaluator, DedupTool, GuardrailTool, StorageTool, HumanEscalation, Notifier, QueryKB, etc.). Strands packages those definitions and deploys them to Bedrock AgentCore so they can run at scale with standardized tool I/O, built-in observability, and clean A2A (agent-to-agent) patterns. In short: Strands is where I declare what each agent knows and which tools it can call; AgentCore is where they run. I was using feature flags in my deployment to allow easy rollback as well as phased deployment of the features.",
    "section": 9
  },
  {
    "id": "doc-42",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Here is how my agents worked before and after deployment to Agentcore: Before AgentCore (direct orchestration): Step Functions call my Lambdas directly in a deterministic pipeline (fetch ‚Üí normalize ‚Üí evaluate relevance/entities ‚Üí dedupe ‚Üí summarize/guardrail ‚Üí decide publish/review/drop). This let me validate logic, data models, and infra without introducing another moving part. The ‚Äúagent shim‚Äù simply proxied to those Lambdas so the Step Functions contract never changes. After AgentCore (agentic orchestration): I flipped a flag and Step Functions (or API Gateway for chat) invokes the Strands-defined agents on Bedrock AgentCore. The Ingestor Agent plans and chooses which Lambda tools to call (ReAct + reflection), applies guardrails, clusters duplicates, and returns a triage decision; the Analyst Assistant Agent serves NL queries from Amplify, pulling from DynamoDB/OpenSearch, posting commentary, and even coordinating with the Ingestor via A2A for duplicate context.",
    "section": 10
  },
  {
    "id": "doc-43",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Functionally it‚Äôs the same tools, but now the agent decides when/why to call them. Use of Bedrock Bedrock underpins every intelligent step: Reasoning + tool use (Agents): Both Strands-defined agents run on Bedrock AgentCore to plan, call tools, and maintain context (ReAct + reflection). Relevance & entity extraction: LLM calls score relevance to your topic taxonomy and extract structured entities (CVEs, actors, malware, vendors, products), emitting JSON with confidence and rationale. Summarization with reflection: The agent (or a summarizer tool) produces an executive 2-liner and an analyst card; a reflection checklist enforces ‚Äúwho/what/impact/source‚Äù and validates entity formatting. Embeddings for semantic dedup/search: Bedrock embeddings vectorize normalized content; OpenSearch Serverless k-NN handles near-duplicate detection and semantic retrieval.",
    "section": 11
  },
  {
    "id": "doc-44",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Guardrails support: While PII and schema checks run in Lambda, the LLM is steered to reduce sensationalism and format errors; suspect outputs route to review. Conversational NL queries: The Analyst Assistant uses Bedrock to interpret questions, translate to DynamoDB/OpenSearch queries, and generate cited answers (and optionally initiate exports). Bedrock infra architecture diagram Human in the loop When the Ingestor Agent (or the direct pipeline pre-AgentCore) isn‚Äôt fully confident‚Äîe.g., borderline relevance, suspected hallucinated CVE, PII detection‚Äîit escalates to review. Those items land in the Amplify review queue where an analyst can: open the decision trace (what tools were called and why), approve/reject or edit tags/summaries, leave threaded commentary (stored in DynamoDB), and provide thumbs up/down feedback that is logged for continuous improvement. Approved items publish immediately; rejected items are archived with rationale for future training/tuning.",
    "section": 12
  },
  {
    "id": "doc-45",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "The Analyst Assistant Agent also helps humans explore dup clusters, ask trend questions, and post comments via Natural Language. Dashboard Challenges and breakthroughs Bursty feeds & reliability: Initial direct triggers caused cascading failures under load. Introducing SQS between stages, DLQs, and idempotency via URL hash stabilized the pipeline. Near-duplicate detection: Title/URL heuristics weren‚Äôt enough. Pairing Bedrock embeddings with OpenSearch k-NN and clustering solved syndicated/rewritten stories; caching by content hash cut cost/latency. Guardrails that matter: Early LLM runs occasionally hallucinated CVEs and included stray PII. A JSON Schema validator, PII filters, and a reflection checklist reduced errors and routed edge cases to review. Agent flipover: Moving from Step Functions ‚Üí Lambdas to AgentCore risked churn. A thin agent shim and a simple feature flag delivered a zero-drama cutover (and instant fallback). Exports at scale: XLSX generation hit Lambda limits.",
    "section": 13
  },
  {
    "id": "doc-46",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Switching to an async export worker that writes to S3 and returns a pre-signed URL made large reports reliable. Cost visibility: Token use and vector storage spiked during spikes. Adding token budgets, embedding caching, and cost per article metrics made FinOps actionable. Kiro hooks in practice: Instrumenting prompts and tool calls with Kiro hooks gave clean traceability for demos and debugging. Key learnings Ship boring first: A deterministic pipeline (without agents) is the best baseline for correctness, tests, and rollbacks. Agents as an overlay: Treat agents as pluggable orchestrators over stable tools; keep I/O contracts tight and versioned. Feature flags are product features: enable_agents, enable_opensearch, guardrail levels, and digests let you canary safely and roll back instantly. Reliability is a graph problem: Backpressure, retries, DLQs, and idempotency must be end-to-end, not per function.",
    "section": 14
  },
  {
    "id": "doc-47",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Measure what you promise: SLOs (dup precision, auto-publish precision, p95 latency, cost/article) drive better architectural choices than gut feel. Security posture is layered: Cognito authZ + least privilege, KMS everywhere, WAF, Secrets Manager rotation, and clear PII retention policies matter in real orgs. Search is product, not plumbing: Hybrid BM25 + vectors, synonyms, and recency boosts directly impact analyst happiness. Small golden datasets pay off: A handful of labeled dupes, fake CVEs, and PII cases catch regressions early and keep prompts honest. Future Plans Enrichment & intel quality: Integrate KEV/EPSS/NVD lookups, vendor advisories, and STIX/TAXII feeds; auto-normalize vendors/products; add IOC extraction and de-dup across entities, not just articles. Evaluation & guardrails maturity: Build a small gold dataset (true dupes, fake CVEs, PII cases) and run scheduled evals; add prompt A/B testing, drift detection for embeddings, and policy-as-code for guardrails.",
    "section": 15
  },
  {
    "id": "doc-48",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Agentic depth: Introduce planning memory (per-topic context), multi-turn self-verification (‚Äúsecond opinion‚Äù model), and a research sub-agent to cross-source claims before auto-publish. Human workflow & governance: Add SLAs/priority queues, multi-approver rules for high-impact stories, granular roles/permissions, and full audit export (JSONL) to S3 for compliance. Product UX: Faceted search (tags/entities/time/source), cluster views for dup families, inline diff of similar articles, saved queries, and per-team digests; async XLSX/CSV with presets. Search relevance: OpenSearch synonyms for vendor/product aliases, recency boosting, hybrid BM25+vector reranking, and feedback-driven learning-to-rank. Cost & FinOps: Track cost per processed article and per published item; autoscale OpenSearch collections; cache embeddings by hash; token budgets per source; nightly right-sizing reports.",
    "section": 16
  },
  {
    "id": "doc-49",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "Multi-tenancy & data boundaries: Partition DynamoDB/OpenSearch by tenant (PK prefix), isolate KMS keys, and add per-tenant throttles/quotas for fairness. Platform & delivery: Canary deploys for Lambdas/agents, blue-green for Step Functions, schema registry + contract tests for tool I/O, and one-click backfill/replay tooling. Security posture: Secrets Manager rotation, WAF rules for bot mitigation, DLP on raw S3 prefixes with automated quarantine, SBOM/supply-chain scanning, and optional private CA for mTLS between services. Integrations: Slack/Teams notifications with approve/reject actions, Jira/ServiceNow ticket hooks for critical items, and webhooks for downstream dashboards. Conclusion Sentinel proved that you can take a messy, high-volume RSS firehose and ship a reliable, secure, and explainable pipeline that analysts actually want to use.",
    "section": 17
  },
  {
    "id": "doc-50",
    "source": "sentinel_article",
    "title": "Building ‚ÄúSentinel‚Äù: multi-agent cybersecurity news triage and publishing system on AWS",
    "content": "The key was sequencing: build a buffered, idempotent backbone; define clear tool contracts; then layer on agentic behavior for planning and tool use. With Strands + Bedrock AgentCore, I kept autonomy where it helps (reasoning, tool selection) and guardrails where it counts (schemas, PII checks, human review). From here, the roadmap is about depth, not breadth: richer enrichment (KEV/EPSS/NVD), stronger evaluation loops, hybrid search relevance, and governance (SLAs, multi-approver flows). The system is already production-shaped‚Äînow it‚Äôs about making it smarter, cheaper, and harder to break.",
    "section": 18
  },
  {
    "id": "doc-51",
    "source": "resume",
    "title": "Resume",
    "content": "<!-- ===================== Resume ===================== --> <section id=\"resume\" class=\"section\" data-aos=\"fade-up\">   <div class=\"section-header\">     <h2><i class=\"fa-solid fa-file-pdf\"></i> Resume</h2>     <a class=\"view-all\" href=\"{{ site.baseurl }}/assets/docs/Benson_Mugure_Resume.pdf\" target=\"_blank\" rel=\"noopener\">Download PDF ‚Üí</a>   </div>   <div class=\"gallery\">     {% for item in site.data.resume %}     <article class=\"card\" data-aos=\"zoom-in\" data-aos-delay=\"100\">       <a class=\"thumb\" href=\"{{ item.pdf }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"View Resume\">         <img src=\"{{ item.image | default: '/assets/images/placeholder_resume.jpg' | relative_url }}\"              alt=\"{{ item.title | escape }} thumbnail\" loading=\"lazy\">       </a>       <div class=\"card-body\">         <h3 class=\"card-title\"><a href=\"{{ item.pdf }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>         <p class=\"card-text\">{{ item.description }}</p>         <div class=\"card-actions\">           <a class=\"btn\" href=\"{{ item.pdf }}\" target=\"_blank\" rel=\"noopener\">View</a>           <a class=\"btn ghost\" href=\"{{ item.pdf }}\" download>Download</a>         </div>       </div>     </article>     {% endfor %}   </div> </section>",
    "section": 0
  },
  {
    "id": "doc-52",
    "source": "projects",
    "title": "Projects",
    "content": "<!-- ===================== Projects ===================== --> <section id=\"projects\" class=\"section\" data-aos=\"fade-up\">   <div class=\"section-header\">     <h2>üöÄ Projects</h2>     <!-- TODO: Maybe have the below link point to a different page where all the projects are but are subdivided by category -->     <a class=\"view-all\" href=\"https://github.com/{{ site.github_username }}\" target=\"_blank\" rel=\"noopener\">All repos ‚Üí</a>   </div>   {% assign projects_count = site.data.projects | size %}   {% if projects_count > 4 %}     <div class=\"carousel-wrapper\">       <button class=\"scroll-btn left\" data-target=\"projects-track\" aria-label=\"Scroll projects left\">‚Äπ</button>       <div id=\"projects-track\" class=\"carousel-track\" role=\"region\" aria-label=\"Projects list\">         {% for item in site.data.projects %}         <article class=\"card\" data-aos=\"zoom-in\" data-aos-delay=\"100\">           <a class=\"thumb\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"Open project\">             <img src=\"{{ item.image | default: '/assets/images/placeholder_project.jpg' | relative_url }}\"                  alt=\"{{ item.title | escape }} thumbnail\"                  loading=\"lazy\"                  {% if item.preview_gif %}data-preview=\"{{ item.preview_gif | relative_url }}\"{% endif %}>           </a>           <div class=\"card-body\">             <h3 class=\"card-title\"><a href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>             <p class=\"card-text\">{{ item.description }}</p>             {% if item.stack %}<p class=\"card-tags\">{{ item.stack }}</p>{% endif %}             <div class=\"card-actions\">               {% if item.screenshot %}<a href=\"\" class=\"btn ghost\" data-lightbox-src=\"{{ item.screenshot | relative_url }}\">Preview</a>{% endif %}               {% if item.live %}               <a class=\"icon-link\" href=\"{{ item.live }}\" target=\"_blank\" aria-label=\"Live site\">                 <i class=\"fa-solid fa-globe\"></i>               </a>             {% endif %}             {% if item.github %}               <a class=\"icon-link\" href=\"{{ item.github }}\" target=\"_blank\" aria-label=\"GitHub repository\">                 <i class=\"fa-brands fa-github\"></i>               </a>             {% endif %}             {% if item.devto %}               <a class=\"icon-link\" href=\"{{ item.devto }}\" target=\"_blank\" aria-label=\"Dev.to article\">                 <i class=\"fa-brands fa-dev\"></i>               </a>             {% endif %}             {% if item.slides %}               <a class=\"icon-link\" href=\"{{ item.slides }}\" target=\"_blank\" aria-label=\"Presentation slides\">                 <i class=\"fa-solid fa-person-chalkboard\"></i>               </a>             {% endif %}               <a class=\"btn\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">Open</a>             </div>           </div>         </article>         {% endfor %}       </div>       <button class=\"scroll-btn right\" data-target=\"projects-track\" aria-label=\"Scroll projects right\">‚Ä∫</button>     </div>   {% else %}     <div class=\"gallery\">       {% for item in site.data.projects %}       <article class=\"card\">         <a class=\"thumb\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"Open project\">           <img src=\"{{ item.image | default: '/assets/images/placeholder_project.jpg' | relative_url }}\"                alt=\"{{ item.title | escape }} thumbnail\" loading=\"lazy\">         </a>         <div class=\"card-body\">           <h3 class=\"card-title\"><a href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>           <p class=\"card-text\">{{ item.description }}</p>           {% if item.stack %}<p class=\"card-tags\">{{ item.stack }}</p>{% endif %}           <div class=\"card-actions\">             {% if item.screenshot %}<a href=\"\" class=\"btn ghost\" data-lightbox-src=\"{{ item.screenshot | relative_url }}\">Preview</a>{% endif %}             {% if item.live %}             <a class=\"icon-link\" href=\"{{ item.live }}\" target=\"_blank\" aria-label=\"Live site\">               <i class=\"fa-solid fa-globe\"></i>             </a>           {% endif %}           {% if item.github %}             <a class=\"icon-link\" href=\"{{ item.github }}\" target=\"_blank\" aria-label=\"GitHub repository\">               <i class=\"fa-brands fa-github\"></i>             </a>           {% endif %}           {% if item.devto %}             <a class=\"icon-link\" href=\"{{ item.devto }}\" target=\"_blank\" aria-label=\"Dev.to article\">               <i class=\"fa-brands fa-dev\"></i>             </a>           {% endif %}           {% if item.slides %}             <a class=\"icon-link\" href=\"{{ item.slides }}\" target=\"_blank\" aria-label=\"Presentation slides\">               <i class=\"fa-solid fa-person-chalkboard\"></i>             </a>           {% endif %}             <a class=\"btn\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">Open</a>           </div>         </div>       </article>       {% endfor %}     </div>   {% endif %} </section>",
    "section": 0
  },
  {
    "id": "doc-53",
    "source": "menta_project",
    "title": "Menta Project",
    "content": "Web Platform Development Coursework: An Application that matches mentors and mentees for mentorship sessions The application should allow the following: Users who are not logged in can see the About Us page with the details of the coaching platform. Students should be able to log in and perform the following: ‚óè View the available coaching/mentoring opportunities based on the three categories (Career advice, Resume Review, and Mock Interview) ‚óè Add, remove, and modify the coaching/mentoring opportunities they plan to participate in. ‚óè There should be at least three categories of coaching/mentoring: Career advice, Resume Review, and Mock Interview. Admin should be able to log in and perform the following: ‚óè View all the student's records. ‚óè Add, modify, and delete students' records.",
    "section": 0
  },
  {
    "id": "doc-54",
    "source": "menta_project",
    "title": "Menta Project",
    "content": "‚óè Add and delete mentors including mentoring Link to report Link to Live site Demo Video  menta.mp4  Features App functionality: Complex Implementation of CRUD functionality and display of data Authentication: Multi-level user access accomodated for admin and students. Middleware used to restrct routes' access Deployment: Site was Successfully deployed and well structured code as available via remote repository with readme file, commits, etc. Student and Admin Dashboards User Profiles Reports Setup Clone this repository: git clone https://github.com/Virgo-Alpha/Menta.git Navigate to the project directory cd Menta Install project dependencies: npm install Usage Contributions We welcome contributions from the community! If you encounter issues or have ideas for improvements, please follow the steps below: Raising Issues Go to the Issues section of this repository. Click on the \"New Issue\" button. Provide a detailed description of the issue or feature request. Submit the issue.",
    "section": 1
  },
  {
    "id": "doc-55",
    "source": "menta_project",
    "title": "Menta Project",
    "content": "Making Pull Requests Fork this repository. Create a new branch for your feature or bug fix. git checkout -b feature-or-bug-fix Implement your changes and make sure your code is well-documented. Push your changes to your fork: git push origin feature-or-bug-fix Create a Pull Request (PR) on the original repository: Go to the Pull Requests section. Click the \"New Pull Request\" button. Select your branch and provide a detailed description of your changes. Submit the PR. Your PR will be reviewed, and once it's approved, it will be merged into the main project. Contributor's Code of Conduct Please note that as a contributor, you are expected to follow our Code of Conduct. Make sure to review and adhere to these guidelines. Dependencies bcrypt==5.1.1 bcryptjs==2.4.3 cookie-parser==1.4.6 dotenv==16.3.1 express==4.18.2 express-route-list-cli==2.0.6 express-session==1.17.3 jsonwebtoken==9.0.2 mustache-express==1.3.2 nedb==1.8.0 passport==0.7.0 passport-local==1.0.0",
    "section": 2
  },
  {
    "id": "doc-56",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "In this article, I discuss how Market creating innovations can save Africa and the world There exists a prosperity Paradox that states that: Countries don‚Äôt create lasting prosperity by trying to solve acute signs of poverty. When money is flooded into poor systems and economies in an attempt to better the conditions for the populace, it ends up getting mismanaged and having little, if any, impact. One often wonders if Africa and the rest of the third world are, thus, doomed to an ill cycle of poverty, corruption, ignorance and disease. I mean, for us to have strong institutions we need credible leaders, and yet credible leaders are made by having strong institutions? There is a way out of this seemingly endless cycle and, funny enough, it‚Äôs not through politics. It‚Äôs through Market Creating Innovations. What are MCIs and How do they Work? Innovation is a change in the process by which an organization transforms inputs of lower value into products and services of greater value.",
    "section": 0
  },
  {
    "id": "doc-57",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "It is a new-to-the world idea in the technical, market, or business model domain. According to the Clayton Christensen Institute, in terms of economic value there are three types of innovation: Sustaining innovations ‚Äî improvements made to existing solutions on the market and are typically targeted at customers who require better performance from a product or service Efficiency innovations ‚Äî enable companies to do more with fewer resources Market Creating innovations ‚Äî serves non-consumers Press enter or click to view image in full size Market-creating innovations transform complicated and expensive products into products that are simple and affordable so that many more people are able to buy and use them. In a sense, market creating innovations democratize previously exclusive products and services and, in many cases, they even create entirely new product categories.",
    "section": 1
  },
  {
    "id": "doc-58",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Impact of MCIs: create profits or at least have the prospect of profit generation in the future Creation of jobs Trigger & reinforce an entrepreneurial culture in a certain industry segment leading to more innovation because of competition MCI pulls in the needed resources, investment, infrastructure, institutions etc instead of the conventional pushing of the same into struggling economies. Examples of MCIs: Mobile telecom in Africa: the story of Celtel Diabetes in Mexico: Javier Lozano ChotuKool: the $69 fridge for rural India Insurance Case Study: Richard Leftley Identifying MCI Opportunities The economy can be divided by: income (high, middle, low) which are proxies for what people can afford, whether companies should invest, and what opportunities are available in certain regions By innovation (consumption vs non-consumption) The consumption economy is composed of consumers who have the income, time, and expertise to purchase and use existing products and services in the market.",
    "section": 2
  },
  {
    "id": "doc-59",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "This economy is relatively easy to assess by conventional metrics, and it‚Äôs the part of the economy that economists, forecasters, and marketing managers often use to predict the growth of a product in a region. Conventional wisdom suggests that investors look for growth in the consumption economy. But the non-consumption economy provides a much more fertile ground for market-creating innovation opportunities. Non-consumers: The segment of the population for whom there was always underlying demand for products and services, but no adequate solution on the market.",
    "section": 3
  },
  {
    "id": "doc-60",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Press enter or click to view image in full size Techniques to identify MCI opportunities: Identify common barriers to consumption (money, access, time, skill) Look for workarounds (e.g., chamas where there are no banks) Consider common aversions (built-in desire to avoid a painful/costly alternative) Examine your own life experiences (products & services that you enjoy that aren‚Äôt available to many/things that you don‚Äôt enjoy that are shared experiences) Evaluating the non-consumption market size Select a few economies in which a product or service has widespread consumption. Calculate the average percent of income spent on the solution in these economies (you can use GDP per capita as a proxy for income). Approximate what a non-consumer in an emerging market can spend on the solution by multiplying the GDP per capita by the estimate calculated in step 2. Estimate the number of non-consumers in the emerging market. Multiply the estimate from step 3 by the estimate from step 4.",
    "section": 4
  },
  {
    "id": "doc-61",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "The result is the approximate size of the non-consumption market From this relatively simple calculation, we see that the market for non-consumption is vast. But even these numbers underestimate the non-consumption opportunity because we‚Äôve assumed a similar percentage of income for both consumers in wealthy countries and non-consumers in emerging economies. In fact, after analysing expenditures in several sectors, we learn that even though the absolute amount non-consumers in emerging economies spend is smaller than what consumers in wealthy countries spend, emerging-market non-consumers often spend a greater percentage of their income on products and services. Respectively, emerging market consumers in upper-middle and low- and lower-middle income countries spend around 2.2 times and 7.6 times the percent of income spent by consumers in high-income countries. Not surprisingly, the more democratized a product is, the less of a percentage of income people spend on it.",
    "section": 5
  },
  {
    "id": "doc-62",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Press enter or click to view image in full size Which product/service will people really buy? At its core, the concept of Jobs to Be Done asserts that people don‚Äôt simply buy products. Instead, people hire products that help them make progress when they find themselves in a particular circumstance of struggle. As a result, successful innovations must consider the people, the progress they‚Äôre trying to make, and the circumstance of struggle in which people find themselves. People don‚Äôt simply buy products or services; they ‚Äúhire‚Äù them to make progress in specific circumstances.",
    "section": 6
  },
  {
    "id": "doc-63",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "We call this progress their ‚ÄúJob to Be Done,‚Äù and understanding this opens a world of innovation possibilities Levels in the architecture of a job: Uncovering the job to be done (functional, social & emotional dimensions) Determining which experiences (in purchase & use) are needed to solve the job Deciding what (resources) & how (processes) to integrate A customer‚Äôs decision on deciding to hire a product/service for a job depends on forces such as: Push of the situation ‚Äî how dire the problem/inconvenience is Pull of your product ‚Äî Desirability Habits of the present ‚Äî Inertia of comfortability/familiarity Anxiety of the new solution ‚Äî concerns of cost, learning new stuff, or just the unknown Press enter or click to view image in full size Bussinessification of MCIs It is not enough to come up with an MCI product/service.",
    "section": 7
  },
  {
    "id": "doc-64",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "One must go the extra mile of creating a system (business) that ensures that the products/services can get to the intended consumer in a manner that is both timely and sustainable. In this regard, there are two types of ventures that emerge: SMEs (also known as SMBs) ‚Äî Small and medium Enterprises or Businesses that exist to serve local markets with traditional, well-understood business ideas but with limited competitive advantage IDEs (also referred to as start-ups) ‚Äî Innovation driven Enterprises that pursue large scale/global opportunities based on bringing to customers new innovations that have a clear competitive advantage and high growth potential (scalability). Keep in mind that IDEs are dependent on scalability and innovation, not technology. Many startups have risen without a significant development in technology, simply because they utilized a new distinct combination of either technical, market or business model dynamics.",
    "section": 8
  },
  {
    "id": "doc-65",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Think of neobanks and the unbundling/rebundling that happens. The sweet spot is when an IDE‚Äôs innovation is not just of any type but a market creating one. I call this an MCIDE (Market-Creating Innovation-driven Enterprise) MCIDE (Market-Creating Innovation-driven Enterprise) is the sweet spot where an IDE‚Äôs innovation is not just of any type but a market creating one. Press enter or click to view image in full size The advantages of IDEs are numerous and obvious, from the great revenue streams that can be created to the potential impact it can have on society (think of how everyone spends most of their time online nowadays). The downsides, however, also merit as much concern, if not more.",
    "section": 9
  },
  {
    "id": "doc-66",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "For an SME entrepreneur to succeed, it will depend on his business acumen, his ability to execute his project, and prevailing local demand, but he does not confront the multifaceted set of technical, market, and business risks faced by IDE entrepreneurs that render the execution challenge for IDEs even more imposing. Become a member These challenges that are associated with the formation of an IDE make it difficult to start one as an individual. Indeed, throughout history, very few start-ups have had a single founder. Most are started by teams. Think of Google, Apple, Microsoft, Uber, AirBnB, Stripe etc. You might have noticed that the number of co-founders (as we refer to them) range. In some cases, two might be enough. It is not as much the number as the skills that the co-founders bring to the table. These skills need to be complimentary.",
    "section": 10
  },
  {
    "id": "doc-67",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "The 2 most important skills, which your team needs to have an abundance of are: Selling ‚Äî can involve both large scale retail and wholesale selling to consumers (better for extroverts) or even in HR recruitment and investor relations (better for introverts). Building ‚Äî can involve coding, UI/UX, project management, content creation, writing, singing, shooting, acting, etc. If a single individual has these two, then they‚Äôre unstoppable. Though that‚Äôs one of the rarest occurrences in our world, and thus the need for teams early on. Skills are important but so is character.",
    "section": 11
  },
  {
    "id": "doc-68",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Aim to partner with people who are: Smart ‚Äî so that they can complement your skills and work in the right direction Hardworking ‚Äî they need high energy so as to get stuff done Have high integrity ‚Äî not just so that they don‚Äôt rip you off but also so that they do what they say (remember that ideas are a dime a dozen, execution is everything) Now that you have assembled your team based on their skill sets and character, it‚Äôs time to come up with your theory of business. It consists of: assumptions about the environment of the organization: society and its structure, the market, the customer, and technology. assumptions about the specific mission of the organization assumptions about the core competencies needed to accomplish the organization‚Äôs mission A theory of business allows you to identify your assumptions so that as you execute your MCIDE business idea, you have a checklist to tick or cross appropriately. Click here to learn more on the theory of business.",
    "section": 12
  },
  {
    "id": "doc-69",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Once you have built your product (or service) and are ready to sell it, you‚Äôll need to price it appropriately. This is a headache to most entrepreneurs as they start off with too high or too low a price. How do you know what‚Äôs the best price to start off with? Well, there are many ways to figure this out but I, personally, consider the pricing thermometer the best. Instead of price skimming, which I honestly think is a rip off, the pricing thermometer allows you to set a price either based on the value or cost of your product. Press enter or click to view image in full size Value-based pricing ‚Äî As a rule of the thumb, try and make sure your product gives your consumers 10X the value they are paying for. The higher the difference between the price and value is, the higher the incentive to buy your product. This is the best way to use the thermometer (You‚Äôll need to know the value first of course) Cost based pricing ‚Äî Where you manually set your margins for each product.",
    "section": 13
  },
  {
    "id": "doc-70",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "The higher the difference between the cost and price is, the higher the incentive to sell your product and the stronger a sales team you need. Branding has a significant part to play here as Apple products have such high margins (sometimes called Apple tax) and yet they still have significant market share. It might be risky to put a lot of investment into an idea not knowing if it will work. This is why the lean start-up method is recommended. You build an MVP (minimum viable product) that satisfies your assumptions, based on the MoSCoW analysis or other method, but one that is very basic and requires very few resources. After your assumptions are confirmed then you can move on to building the business since the product has a market. Keep in mind that just because you have confirmed the market likes your product does not mean that you‚Äôll succeed. To do that you‚Äôll still need to iterate towards product-market fit.",
    "section": 14
  },
  {
    "id": "doc-71",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "Here‚Äôs how Andreessen defines product/market fit: The customers are buying the product just as fast as you can make it ‚Äî or usage is growing just as fast as you can add more servers. Money from customers is piling up in your company checking account. You‚Äôre hiring sales and customer support staff as fast as you can. After pricing, you‚Äôll need to come up with a business model for your venture. Keep in mind that a business model involves a revenue model but also more. A revenue model is how the business will make money. A business model is how the business creates, delivers and captures value. Think of Facebook: It‚Äôs revenue model is ad driven but its business model is content, connection and community driven. It only captures a fraction of the value it creates as very few community members will use or consume the ads. For a comprehensive list of business models, click here.",
    "section": 15
  },
  {
    "id": "doc-72",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "The three checkboxes of a business model are: User desirability Business viability Technology feasibility Product-market fit only tells you that your product will succeed but not your business. To figure that out, you‚Äôll need to determine whether you are default alive or default dead. This is done by using unit economics (one customer).",
    "section": 16
  },
  {
    "id": "doc-73",
    "source": "mcis_article",
    "title": "MCIs: How to save Africa & the rest of the 3rd World",
    "content": "The following equation tells you that: CLV ‚Äî CAC When you take the Customer Lifetime Value (Total amount of money the customer will give you for as long as they are your customers) and you take away the Customer Acquisition Cost (Total amount needed to turn a prospective customer into an actual customer), you should get: A positive number ‚Äî This means you are default alive and you only need to optimize your process to get higher margins A negative number ‚Äî This means you are default dead and you need to rethink your business model or pricing If all the above steps are followed and or considered, then your venture should be one that is on its way to success while creating a significant positive impact on society. As a parting shot, I‚Äôll leave you with the first and second runners up ideas on saving the third world. One was to educate women and the other was to feed kids. I hope you enjoyed reading and learnt at least one new thing.",
    "section": 17
  },
  {
    "id": "doc-74",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "In this article, I discuss If African Fintech Monopolies are Guaranteed To Be Unicorns Competition is for losers.‚Äô This prescriptive pronouncement of looming failure for companies without an apparent monopoly was made by Peter Thiel, Paypal & Palantir Founder, in his 2014 book, ‚ÄòZero to One‚Äô. In an attempt to generalize the trajectories of various African financial technology companies (popularly called ‚ÄòFintechs‚Äô) to Peter Thiel‚Äôs monopolistic thesis for success, it begs the question as to whether growing in singular ways that are hard to compete with is the only way to attain sufficient escape velocity into coveted unicorn territory as a startup (achieving a valuation of at least USD 1 billion as a startup). What does being an African startup mean for this supposed rule?",
    "section": 0
  },
  {
    "id": "doc-75",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Can seizing critical market share in a unique customer segment, innovating a profitable monetization and distribution strategy then scaling rapidly be identified as defining paths to optimal profitability for African Fintechs? This will be the subject of exploration through the cross-examination of the journeys of successful companies including Interswitch, Paystack, Safaricom‚Äôs M-pesa & Mono as well as the subsequent emergence of distinct theories of change across these fintech startups today. Will the losers and winners in this competition for the African, billion-node user base depend on the establishment of monopolies? Interswitch Payments is arguably the foundation of financial technology infrastructure in Africa for the establishment of crucial bank ‚Äòswitching‚Äô technology.",
    "section": 1
  },
  {
    "id": "doc-76",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Founded in 2002 hot off the heels of the March 2000 dotcom bubble burst; Interswitch was the earliest known establishment that sought to disseminate the means to participate in digital commerce for Africans by Africans; at a time when having dialup broadband internet was the reserve of government offices and big-name foreign company offices. At a time when ATMs were unheard of in Nigeria, with customers queueing for long hours in banking halls to withdraw their money. The dire need for inter-bank connectivity within a secure, end-to-end infrastructure was a real problem with big numbers attached to it: connect 10 nationally licensed banks serving 120 million people. This asymmetrical risk clearly paid off for Interswitch, whose timely execution of a valid solution paved the way for brick and mortar payments infrastructure like ATM machines that could be used regardless of the bank that held the customers‚Äô money.",
    "section": 2
  },
  {
    "id": "doc-77",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "This solution was the basis of all of their subsequent ventures, like the Card Issuing technology, Interswitch Verve, which was licensed to banks for use in their ATM machines and later, Point of Sale payment solutions for small and medium enterprises to leverage growth off of. Become a member Handling USD 2.5 billion monthly through their 190,000 transactions daily count, Interswitch‚Äôs first-mover advantage clearly paid off. Their 90 percent market share of all electronic payments in Nigeria is a clear monopoly that can be attributed to, in part, existing early on in the market and solving a problem well enough for it not to need any future iteration. It is no wonder then, that global payments processing behemoth Visa moved to acquire a 20 percent stake in Interswitch for USD 200 Million at a USD 1 Billion valuation in November 2019. Interswitch‚Äôs monopoly status very much guaranteed that it would one day be a unicorn and in hindsight, it was never a question of if, but rather, when.",
    "section": 3
  },
  {
    "id": "doc-78",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Paystack enables the integration of payment gateways into mobile and web applications where users can make payments using either debit, credit or mobile money transfer. Founded in 2016, Paystack made waves in October 2020 after being acquired by Stripe for USD 200 million after just 5 years of being in operation. Paystack‚Äôs creation and accumulation of value in the African continent stems from being the lifeblood of entrepreneurial African ventures based on online operations by powering the customer-to-business payments through their robust application programming interfaces (APIs). With over 60,000 live merchants transacting online using Paystack‚Äôs technology, the early makings of a monopolistic system based off of technological prowess are present. They certainly have the resources, manpower and intent to accomplish continental prominence now more than ever with the backing of Stripe, global payments processor.",
    "section": 4
  },
  {
    "id": "doc-79",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Paystack still conforms to Peter Thiel‚Äôs competition-averse philosophy through their hyperfocus on a single market segment; payment APIs, and repeatedly iterating within this segment until optimal earnings are achieved. This strategy has worked remarkably well so far, with their 2000 percent return on investment, comparing their USD 10 million seed funding as reported by Crunchbase, to their USD 200 million Stripe acquisition windfall. With their best-in-class focus on technological advancements as a moat for monopoly, Paystack lives up to their claim as ‚ÄòStripe for Africa‚Äô. Mpesa is marshalling its resources, product development and expansion strategies towards becoming Africa‚Äôs fintech ‚Äòsuper-app‚Äô. Mpesa is a Kenyan peer to peer mobile money transfer service owned by Safaricom, a Kenyan mobile network operator.",
    "section": 5
  },
  {
    "id": "doc-80",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Mpesa was publicly launched in 2007 by Safaricom and has since become synonymous with mobile money in Kenya, enjoying a comfortable 95 percent brand recognition rate as reported by a Worldbank study in 2014 (Mas, Radcliffe, 2014). The Kenyan government‚Äôs 35 percent stake in Safaricom has enabled Mpesa to smoothly navigate the tenuous challenges of bureaucracy when it comes to accessing customers‚Äô biometric data held in government sanctioned repositories such as identification records. Safaricom has further bankrolled aggressive marketing campaigns disclosed in their public financial records (as they are publicly traded in the Nairobi Securities Exchange) to amount to as high as USD 6 million as of the 2019/2020 financial year.",
    "section": 6
  },
  {
    "id": "doc-81",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "The customized use-cases on offer for every segment of the market, from informal traders who can only use feature phones, to the country‚Äôs employed bourgeoisie who in lieu of expensive, bank-enabled transaction methods, highlight Mpesa‚Äôs preferred versatility for instances ranging widely from point of sale payments to online purchases. This is in no small part thanks to Safaricom‚Äôs dominance in the mobile network scene where their 63.6 percent market share according to the Communications Authority of Kenya bankrolls their advances. Mpesa‚Äôs recently released mobile application offers their users a mobile wallet in addition to its already robust mobile money ecosystem in a bid to claim ubiquitous presence in their users‚Äô lives and in so doing, become a super application. At the close of the 2020 financial year, Mpesa marked 5 years of undeterred, year on year growth, closing with a USD 748.36 million turnover.",
    "section": 7
  },
  {
    "id": "doc-82",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Mpesa‚Äôs success can only get bigger as operations spread across the neighbouring East African countries. Mpesa‚Äôs success is a true testament to Peter Thiel‚Äôs rule of monopolies, being a monopoly within a monopoly. Mono is a fledgling African fintech whose approach to expansion into the African continent is based on streamlining access to aggregated African customer data as opposed to the monopolization of a set of unique value offerings. Founded in August 2020, Mono heralds the most recent stirrings of the global financial technology scene: Open Banking within Neobank-like financial frameworks as opposed to closely guarded technologies that shepherd transactions within legacy financial frameworks. Open banking is poised to be the key to consolidating the African continent‚Äôs billion-user strong user base fragmented across as many as 171 variant offerings in mobile money alone, not accounting for other customer segments like virtual cards and payment gateway integrations.",
    "section": 8
  },
  {
    "id": "doc-83",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Through aggregating the financial information of African customer data into a singular, secure dataset; Mono‚Äôs success will come from their datasets being used by as many African (and non-African) fintechs as a base to build better abstractions of best practices for the African customer to transact on a global scale. This method is clearly getting traction within the African scene, since fragmentation is a real problem whose potential to ease the pain points of the problem solving process has translated into Mono processing 5 million datasets per hour as of February 2021, according to company reports in a TechCrunch interview.",
    "section": 9
  },
  {
    "id": "doc-84",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Mono‚Äôs vision to carry forward the ideas of success through collaboration within the financial technology scene in Africa fly in the face of Peter Thiel‚Äôs competition-averse rule, having garnered USD 2.6 million so far after completing their series C round in March 2021 by joining the famed startup accelerator, Y Combinator, in their Winter 2021 batch and securing a USD 125,000 investment for the standard 7 percent equity. Mono‚Äôs eventual success pursuing this ideal of open collaboration as opposed to the monopolization of a technical advantage remains to be seen but objectively modelling their future based on their current condition ends up with their APIs being an integral part of the aggregation of the African market into a more accessible, streamlined user base.",
    "section": 10
  },
  {
    "id": "doc-85",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "There is a case to be made as to whether Mono‚Äôs ‚Äòone API to rule them all‚Äô approach touting their hyper-optimized application programming interface as the last word in access to customer financial data in Africa has been a monopoly all along. This claim, however, fails to take into account the simple fact that even if Mono‚Äôs success will depend on singular use of their API widely within the African fintech space, the flipside of this mass adoption is a symbiotic collaboration between Mono and each one of their adopters to tailor data as is suited for each of their use cases. In this way, Mono is not a monopoly in all the ways that matter for them to scale up with little marginal cost and possibly evolve into the next African unicorn. This will inevitably lead to more startups being created collaboratively around this unfettered access to data and as such, even more success for Mono.",
    "section": 11
  },
  {
    "id": "doc-86",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Mono‚Äôs success is collaborative, and it will make all the difference, both for them and the African fintech scene. Monopolizing whatever advantages a startup possesses, be they technological, having been first movers or unique governmental access is a nearly straightforward path to outsized success in the African fintech space. This has been seen in the case of Interswitch, who became the first African unicorn startup leveraging their value as unassailable leaders of electronic payments infrastructure with monopolies of time and technology. From Paystack, the rewards of seizing critical market share in a neglected market segment with technology that is promising enough to garner interest for acquisitions by global giants like Stripe have been exemplified.",
    "section": 12
  },
  {
    "id": "doc-87",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Mpesa‚Äôs unique predisposition as a monopoly mobile payments venture owned by a monopoly mobile network operator is a clear example of how isolated market segments like an entire country can be dominated by a single product offering when a (double) monopoly is duly leveraged. In the face of all these vindications of billionaire investor Peter Thiel‚Äôs competitor-averse rule for startup success, Mono‚Äôs massive adoption and USD 2.6 million venture capital gain despite being 10 months old is a contrarian example of success through collaboration. Taking all the available information into account, it is clear that monopolies are powerful multipliers of growth for fintech startups in Africa, helping them gain enough users to not petter out while at the same time offering them the chance to attempt varied monetization models on such massive swathes of the population, earning massive returns when the formula is tweaked just right.",
    "section": 13
  },
  {
    "id": "doc-88",
    "source": "mafh_fintech_article",
    "title": "Are African Fintech Monopolies Guaranteed To Be Unicorns?",
    "content": "Even then, monopolies are not always the answer, and as such, they should only be pursued where the opportunity cost to collaborate does not exceed what the startups stand to gain by building moats around their singular advantages. So are African fintechs who compete losers? Well yes, but not all the time.",
    "section": 14
  },
  {
    "id": "doc-89",
    "source": "linkedin_job_posts_project",
    "title": "LinkedIn Job Posts Insights",
    "content": "This is a Data Engineering project that uses Docker, Terraform, GCS, BigQuery, Airflow, dbt and Looker studio to turn data into actionable insights in terms of career choice and job prospects using LinkedIn job postings from 2023-2024. Screenshot from 2025-04-20 15-54-19 üîç 1. Problem Description Choosing a career can be overwhelming. This project helps make informed decisions using real job data. Questions Answered: Which job titles offer the highest salaries? Which companies, industries, and skills are most lucrative? What percentage of companies offer remote work (for work-life balance)? What are the highest salaries and average experience levels? Which countries have the most job postings? ‚òÅÔ∏è 2. Cloud Infrastructure (Terraform) The project was developed in Google Cloud platform using Terraform as an IaC tool. I modularized the different aspects and had outputs to show the resources created.",
    "section": 0
  },
  {
    "id": "doc-90",
    "source": "linkedin_job_posts_project",
    "title": "LinkedIn Job Posts Insights",
    "content": "We create everything starting from the project itself to the service account, gcs bucket and big query, iam management for permissions and even enabling the apis. I also make use of variables. In the outputs displayed on the terminal, copy the project id, bucket name and service account name for use in airflow's docker-compose.yml file. All Terraform code is in 0_cloud_infra/terraform. Commands: cd 0_cloud_infra/terraform terraform init terraform apply Copy the project_id, bucket name, and service account for Airflow configuration. üöö 3. Data Ingestion (GCS via Airflow) I use airflow as my work orchestration tool to fetch and upload the data to a datalake. I build airflow using a docker-compose.yml file and Dockerfile which installs google sdk. I then create a dag that has multiple steps which include downloading, unzipping and uploading the data to the created gcs bucket.",
    "section": 1
  },
  {
    "id": "doc-91",
    "source": "linkedin_job_posts_project",
    "title": "LinkedIn Job Posts Insights",
    "content": "To run the dag, please replace the BUCKET_NAME variable in the dag file with the one that was output when you run terraform apply. Airflow Setup: docker-compose build docker-compose up airflow-init docker-compose up -d Then visit http://localhost:8080 to run the DAG. Screenshot from 2025-04-14 11-42-02 üìä 4. Data Warehouse (BigQuery) BigQuery is provisioned via Terraform. External tables are defined using dbt. For performance: Partitioned by month: TIMESTAMP_TRUNC(original_listed_time, MONTH) Clustered by: company_id, formatted_work_type, formatted_experience_level, company_country üèãÔ∏è Why Partitioning & Clustering? Partitioning helps scan only relevant time ranges. Clustering optimizes filter operations, joins, and aggregations by organizing data on disk. ‚öñÔ∏è 5. Transformations (dbt) I used dbt to model my data and apply transformations necessary for use in making of the dashboard. I made models, staging and dimension tables as well as tests and exposures.",
    "section": 2
  },
  {
    "id": "doc-92",
    "source": "linkedin_job_posts_project",
    "title": "LinkedIn Job Posts Insights",
    "content": "All transformations are done using dbt Cloud IDE: Models: staging/, marts/core/ Sources: Defined in sources.yml Tests & schema validation Key Models: stg_linkedin__postings, stg_linkedin__salaries, stg_linkedin__benefits, etc. fct_postings joins staging + dimension tables for dashboard consumption. Run Commands: dbt run-operation stage_external_sources dbt build Please see my models and tables below: models Use dbt Cloud IDE for dependencies & permissions. Project uses GCS + BigQuery access via service account. üìä 6. Dashboard (Looker Studio) The dashboard visualizes insights based on the fct_postings fact table. Screenshot from 2025-04-14 11-31-33 Link to Dashboard: Open in Looker Studio üîÑ 7. Reproducibility Step-by-step to reproduce the project:  Step 1: Provision Infra cd 0_cloud_infra/terraform terraform init terraform apply  Note down the project_id, bucket name, and service account name  Step 2: Run Airflow for data ingestion cd ../..",
    "section": 3
  },
  {
    "id": "doc-93",
    "source": "linkedin_job_posts_project",
    "title": "LinkedIn Job Posts Insights",
    "content": "docker-compose build docker-compose up airflow-init docker-compose up -d  Visit localhost:8080 and trigger the DAG  Step 3: Run dbt transformations (in dbt Cloud) cd 2_dbt_transformations dbt run-operation stage_external_sources dbt build ‚úÖ 8. Testing Some dbt tests added for schema + uniqueness + nulls. Screenshot from 2025-04-14 11-35-07 Future: Add CI testing via GitHub Actions and unit tests for DAGs. üìÉ 9. Makefile An alternative to the DAG for manual local testing: download   downloads zip file to ./data/.data/ unzip      unzips contents delete     deletes .data folder Used during local dev before uploading to GCS. ‚öôÔ∏è 10. Further Work Add ingestion into local Postgres (test mode) Expand dbt test coverage Add CI/CD with GitHub Actions Use dbt seed for static dimension tables Snapshotting for slowly changing dimensions Generate dbt documentation Improve Looker Studio with parameters and filters",
    "section": 4
  },
  {
    "id": "doc-94",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "In this article, I discuss how I Resurrected Google Reader for the modern web using Kiro TLDR; Google Reader solved content tracking for the open web, but died when RSS could not keep up with SPAs. Watcher resurrects the Google Reader experience for the modern web. Instead of relying on RSS, Watcher creates RSS by monitoring live webpages. Users define ‚Äúhaunts‚Äù using natural language; AI generates selectors and structure. The UI intentionally mirrors Google Reader‚Äôs three-column layout and power-user workflow. AI + scraping + RSS unlocks a new class of user-controlled web monitoring tools.",
    "section": 0
  },
  {
    "id": "doc-95",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Table of Contents Introduction Kiro and its features Spec driven development Vibe coding Agent hooks Steering docs MCP Google Reader Watcher How we built it Similarities Between Watcher and Google Reader Differences Between Watcher and Google Reader Lessons learnt Conclusion Introduction I was honored to participate in the Kiroween Hackathon on Devpost, an event that challenged participants to build ambitious projects using Kiro, AWS‚Äôs newly released AI-native IDE. The hackathon encouraged not just technical execution, but creative re-thinking across four themes: resurrecting dead technologies, stitching together unlikely systems, building flexible foundations, or delivering unforgettable interfaces. For my submission, I chose Resurrection. Over a decade ago, Google Reader quietly disappeared from the web. Its shutdown marked more than the loss of a product, it signaled a shift away from user-controlled, open content consumption toward algorithmically curated feeds.",
    "section": 1
  },
  {
    "id": "doc-96",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Yet the problem Reader solved never went away. In fact, it became harder: the modern web moved to dynamic, JavaScript-heavy applications that no longer expose RSS at all. This project explores a simple question: What would Google Reader look like if it were rebuilt for today‚Äôs web? The result is Watcher, a system that haunts modern websites, detects meaningful change, and resurrects the RSS model using AI, scraping, and a deliberately nostalgic interface. Kiro made this possible. Kiro and its features Kiro is an IDE that AWS released this year. It has several cool features such as: Spec driven development Spec-driven development in Kiro places formal specifications at the center of the workflow. Instead of writing code first and documenting later, developers define structured specs that describe intent, constraints, and expected behavior. These specs are then used by the IDE and its agents to guide implementation, validation, and refactoring.",
    "section": 2
  },
  {
    "id": "doc-97",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "This approach reduces ambiguity, improves alignment between stakeholders, and creates a durable source of truth that evolves alongside the codebase. For AI-assisted development, specs act as guardrails, ensuring that generated code remains consistent with the system‚Äôs design goals. To use this mode, I first wrote the PRD (Product Requirements Document) by hand and added it as a spec in Kiro. Kiro then used it to generate the requirements, design and tasks file. I then executed the tasks one by one and checked to ensure that the code generated met my standards and creative vision. Vibe coding Vibe coding is Kiro‚Äôs term for an exploratory, conversational style of development where developers work at the level of intent rather than syntax. Instead of issuing narrowly scoped prompts, developers express what they are trying to achieve,architecturally or experientially, and allow the IDE to propose implementations that fit the broader context of the project.",
    "section": 3
  },
  {
    "id": "doc-98",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "This mode is particularly effective in early-stage prototyping, where requirements are fluid and rapid iteration is essential. Vibe coding prioritizes flow and momentum while still grounding outputs in the project‚Äôs specifications and constraints. I used vie coding to debug and refine the UI components to my needs. It proved useful in understanding the code generated as well as the cause of errors. Agent hooks Agent hooks allow developers to attach AI agents to specific lifecycle events such as file changes, test failures, or deployment steps. These agents can observe state, reason about deltas, and take targeted actions,ranging from suggesting fixes to generating artifacts or alerts. Rather than operating as a monolithic assistant, Kiro‚Äôs agents are modular and event-driven, which makes them predictable and composable. This model mirrors how modern systems are built: loosely coupled components reacting to well-defined signals.",
    "section": 4
  },
  {
    "id": "doc-99",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "I created agent hooks for security, performance and unit testing goals. These ensured that I had the basics covered as I continued to iteratively develop my project. Steering docs Steering documents in Kiro are lightweight, high-leverage artifacts that encode architectural principles, design philosophies, and non-functional requirements. They serve as long-lived guidance for both humans and AI agents, shaping decisions without prescribing implementation details. In practice, steering docs help maintain coherence as a project grows, especially when multiple contributors or agents are involved. They are particularly valuable in AI-assisted environments, where consistent direction is necessary to avoid fragmentation and unintended complexity. I used steering docs to set guardrails for the design and set up. I wanted to try and mimic Google Reader‚Äôs UI and functionality as much as possible and this came in handy.",
    "section": 5
  },
  {
    "id": "doc-100",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "MCP The Model Context Protocol (MCP) provides a standardized way to supply structured context,such as schemas, APIs, domain models, and external tools,to AI agents. By formalizing how context is shared, MCP reduces hallucinations and increases the reliability of agent outputs. It enables agents to operate with a clear understanding of the system‚Äôs boundaries and available capabilities, making them more effective collaborators rather than generic text generators. MCP is a critical enabler for building production-grade, AI-native developer workflows. Google Reader Google Reader was a web-based RSS and Atom feed aggregator launched by Google in 2005. At its core, it allowed users to subscribe to content feeds,blogs, news sites, academic journals, forums,and consume updates in a single, unified interface.",
    "section": 6
  },
  {
    "id": "doc-101",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Rather than visiting dozens of websites individually, users could rely on Google Reader to surface new content as it was published, ordered chronologically and optimized for rapid scanning. Its minimal, text-first interface emphasized efficiency over distraction, enabling power users to process large volumes of information quickly. Google Reader was important because it embodied an open, decentralized model of the web. It rewarded publishers who exposed structured feeds and gave users direct control over how and where they consumed information, independent of proprietary algorithms. For researchers, journalists, developers, and analysts, it became an indispensable tool for monitoring changes across many sources. It also pioneered interaction patterns,such as keyboard shortcuts, starring, tagging, and sharing,that influenced later content consumption tools.",
    "section": 7
  },
  {
    "id": "doc-102",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Despite its loyal user base, Google shut down Reader in 2013, citing declining usage and a strategic shift toward fewer, more focused products. In practice, its closure reflected a broader industry transition away from open syndication toward algorithmically curated social feeds. While platforms like Twitter and Facebook offered scale and engagement, they replaced user intent with opaque ranking systems. The shutdown left a lasting gap for users who valued transparency, control, and signal over noise,a gap that many modern tools, including Watcher, aim to address. Watcher Google Reader was one of the most beloved tools on the web: simple, fast, and incredibly efficient at keeping people updated. But as the web shifted to SPAs and dynamic content, most of it without RSS, Reader‚Äôs death left a real gap. Watcher was born from the idea: What if we resurrected Google Reader, but upgraded it to haunt the modern web?",
    "section": 8
  },
  {
    "id": "doc-103",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Meaning: It can watch any page, including SPAs It understands natural language It detects meaningful changes And it exposes everything again as RSS, just like the old days That mix of nostalgia + modern constraints was the spark. Watcher resurrects the Google Reader experience for the modern web. GIFWatcher GIF You can view the deployed website here. Use the below credentials: Email: demo@watcher.local Password: demo123 It lets users: Define a haunt by giving a URL + natural language description like: ‚ÄúTell me when the admissions page says applications are open for 2026.‚Äù Behind the scenes, an LLM generates selectors, keys, and normalization rules. A headless browser (Playwright) scrapes the target on a schedule. Watcher tracks key/value state diffs, not raw HTML, and generates structured change events. Each haunt produces an RSS feed. The UI is a faithful rebirth of the 3-column Google Reader layout, complete with folders, unread counts, stars, refresh, and keyboard shortcuts.",
    "section": 9
  },
  {
    "id": "doc-104",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "In short: Watcher turns any webpage, even SPAs, into a live RSS source. How we built it Watcher is built as a Django-based system with: Spec-driven functional requirements covering scraping, diffing, RSS construction, and a Reader-style UI. Playwright for SPA rendering and key extraction. Celery for periodic haunting and change detection. A fully modeled haunt configuration, derived via LLM from natural language. Structured state tracking, storing only key/value diffs and summaries. RSS feed generation for both private and public haunts. A Google Reader‚Äìinspired front-end, implemented to feel as close as possible to the original. Kiro powered the development loop, particularly around specs, architecture constraints, steering for UI generation, and consistency between backend and frontend layers.",
    "section": 10
  },
  {
    "id": "doc-105",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "You can find the code base on github Similarities Between Watcher and Google Reader Feed-Oriented Information Consumption Both Watcher and Google Reader organize information in a feed-like format that lets users see updates from multiple sources in a unified view. RSS Integration and Support Both systems can work with RSS sources: Google Reader was built around RSS/Atom feed aggregation, while Watcher supports adding RSS cybersecurity sources into its monitoring. Three-Panel Interface and Navigation Watcher‚Äôs interface intentionally draws on the three-panel layout that was characteristic of Google Reader, navigation pane, feed list, and content view. Unread/Read Tracking Both platforms include mechanisms to mark items as read or unread, enabling users to track what they have and have not seen. Keyboard Shortcuts and Power User Features Google Reader popularized keyboard shortcuts (J/K/M/S) and Watcher includes similar navigation controls inspired by Reader.",
    "section": 11
  },
  {
    "id": "doc-106",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Subscription Model for Content Google Reader let users subscribe to feeds; Watcher lets users subscribe to monitoring configurations (‚Äúhaunts‚Äù) and view updates similarly. Differences Between Watcher and Google Reader Dimension\tGoogle Reader\tWatcher Primary Purpose\tGeneral-purpose RSS/Atom feed aggregator for web content and news. Website change monitoring and alerting with AI-assisted context. Core Functionality\tAggregates syndicated feeds and surfaces updates for reading. Continuously monitors pages (including SPAs) and detects meaningful changes. AI Integration\tNone; designed as a human-driven feed reader. Uses AI to interpret change relevance and generate selectors from natural language. Update Detection Mechanism\tPulls standardized feed entries as published by websites. Uses headless browsers (e.g., Playwright) to detect changes beyond RSS. Notification Types\tIn-app unread counts and keyword search; limited alerts.",
    "section": 12
  },
  {
    "id": "doc-107",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Email alerts and structured summaries when defined conditions trigger. User Interaction Model\tUsers subscribe to feeds and consume published entries. Users define what to monitor (‚Äúhaunts‚Äù); the system proactively watches for changes. Social Features\tExperimental sharing features (later removed). Public haunts and subscriptions to other users‚Äô monitoring configurations. Scope of Content\tLimited to content explicitly exposed via RSS/Atom. Can monitor arbitrary webpages, including dynamic and JavaScript-rendered content. Historical Status\tDiscontinued in 2013. Actively developed and deployable. Lessons learnt LLMs work exceptionally well when guided by tight specs + steering documents. The web‚Äôs move to SPAs made RSS impossible, but not undetectable. State diffs matter more than raw HTML when building meaningful alerts. Nostalgia is a powerful design force, porting old UX patterns into modern stacks teaches discipline. Combining AI + scraping + RSS can create genuinely new value.",
    "section": 13
  },
  {
    "id": "doc-108",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "Conclusion Watcher began as an exercise in nostalgia, but it ended as a statement about the modern web. While RSS disappeared not because it was flawed, but because the web outgrew it, the underlying need, to know when something meaningfully changes, never went away. By combining AI-driven interpretation, structured state diffing, and headless browser scraping, Watcher turns even the most dynamic SPA into a first-class, queryable feed. In doing so, it restores user intent, transparency, and control, values that defined tools like Google Reader but are largely absent today. Kiro proved to be more than an IDE in this process. Its emphasis on specs, steering documents, and agent-driven workflows enabled a level of architectural consistency that would have been difficult to maintain in an AI-assisted build. Rather than fighting the model, the system was shaped by constraints. The broader lesson is this: AI does not replace structure, it amplifies it.",
    "section": 14
  },
  {
    "id": "doc-109",
    "source": "kiroween_article",
    "title": "Resurrecting Google Reader for the modern web using Kiro",
    "content": "When paired with clear specs, thoughtful design, and a respect for proven UX patterns, it enables entirely new classes of systems. Watcher is one such system. A resurrection, not of a product, but of an idea: that the web should work for its users, not the other way around.",
    "section": 15
  },
  {
    "id": "doc-110",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "In this article, I discuss How to Organically Grow the Kenyan Film Industry It is public knowledge that the Kenyan Film Industry faces a myriad of challenges. One filmmaker even went to the extent of saying that we don‚Äôt yet have an industry as of yet; What we have is more of a film scene. Perhaps that is why we are yet to have a hood name like the woods (Holly-, Nolly-, Bolly-). Be that as it may, we cannot fail to acknowledge the overwhelming talent, creativity and effort that different creatives in the country have contributed to making the industry move forward. Quite honestly, we are the frontier film industry in East and Central Africa. In this commentary, I will use the movie A Grand Little Lie (GLL) which can be accessed from Philittv.com (@ Ksh 200 only) to shed more light on the problems, current solutions and what more I think can be done.",
    "section": 0
  },
  {
    "id": "doc-111",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "Problems facing the Kenyan Film Industry How GLL Addressed some of these challenges What more can be done (Personal Recommendations) Conclusion Press enter or click to view image in full size Problems facing the Kenyan Film Industry A problem shared is a problem halved and yet to share the problem you have to understand it. Some of the challenges that the Kenyan film industry faces include: High cost of production ‚Äî costs include transport, shooting, cast and crew fees in production and pre-production as well as editing, marketing & distribution costs in post-production Lack of financing ‚Äî Local financial support from individuals and corporates for film production has been low over the years; Foreign support might be more forthcoming but it frequently has demands that need the story to be remodelled to fit a certain perspective hence altering the original narrative Lack of an Efficient Distribution channel ‚Äî Kenyan cinemas rarely show Kenyan made films.",
    "section": 1
  },
  {
    "id": "doc-112",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "It is only in recent years that a few cinemas in the major cities have tried to support the local industry. Little support from local consumers ‚Äî Local consumers prefer foreign content from Hollywood and Nollywood as opposed to locally made content; This could be a matter of quality, or pricing or even just access Lack of a marketing strategy ‚Äî Most filmmakers don‚Äôt have a clear target market for the film by the time they are done with production and thus fail to market their film efficiently which leads to low sales How GLL Addressed some of these challenges A Grand Little Lie (better known as GLL) is a film scripted by Abel Mutua and directed by Phillip Karanja from Phil-it Productions. I believe it to be a pioneer piece of art in the market because of more than a few aspects of its production and marketing strategy. Become a member GLL sprouted from one of the tales Abel Mutua tells his fans (known as Wakurugenzi) on his YouTube channel.",
    "section": 2
  },
  {
    "id": "doc-113",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "As Phil, the director, admits, the film was shot in just 8 days! In addition to this, the last scene wasn‚Äôt even out until the day before they started filming. ‚ÄúScript-writers will be the death of directors and producers,‚Äù Phil said in an interview by Yvonne Mwikali which was also attended by Abel Mutua and Michael Munyoki, the lead actor. Press enter or click to view image in full size The following are some of the ways that GLL and Phil-it productions have creatively addresses the problems faced by the film industry in Kenya: High cost of production ‚Äî The production team used what they had at hand and called in favours so as to shoot the film in the shortest time possible. They leveraged already created networks and relationships to make sure the film is shot within budget Lack of financing ‚Äî The film was low budget, for as the director explained, they did not have a budget for marketing.",
    "section": 3
  },
  {
    "id": "doc-114",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "Knowing that marketing is a very important part of film production, Phil-it productions took a very creative low-cost albeit dramatic approach to it. They took it to the streets, literally! Little support from local consumers ‚Äî GLL leveraged the immense support from Abel Mutua‚Äôs Wakurugenzi as well as very creative approaches to marketing that were tailor made for the typical Kenyan Lack of a marketing strategy ‚Äî As mentioned above, GLL did not have a marketing budget but that did not stop them from marketing the Kenyan way. They held billboards that advertise their film out into the streets. The billboards made ridiculous claims about what the film can do for you, which ranged from increasing sexual stamina for males to making husbands return home early. Another route that they took was showing the film in one cinema and thus converting those early adopters into their ambassadors for their other creative distribution channels. They also had a red carpet premiere.",
    "section": 4
  },
  {
    "id": "doc-115",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "Lack of an Efficient Distribution channel ‚Äî Apart from the first cinema and red carpet premieres, the film is being sold at Philittv.com for Ksh 200 (less than USD $ 2). Once you pay, via m-pesa or credit/debit card, you‚Äôll receive the watch link on your email. You can only access it from one device and therefore, one has to be careful. One can, however, access it an infinite number of times upon receiving the link. Compared to a cinema citing or a traditional CD/DVD model, this is much more affordable. Director Phil admitted that with only 50, 000 purchases he will be able to recoup his investment, pay the cast and crew and have a budget (inclusive of marketing) for the next film so make sure you check out the trailer and if you like it then buy it. What more can be done In modest terms, what GLL has done is a great step in the right direction. We, however, need to develop more efficient and sustainable ways to fund, produce and market our films.",
    "section": 5
  },
  {
    "id": "doc-116",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "In the short term and for a small number of films, the GLL way works just fine but for dozens and hundreds of films, new cracks emerge: Pricing ‚Äî USD $ 2 is quite affordable but that‚Äôs only when compared to the in person cinema experience. If you consider the digital model that Netflix adopts, 5 such movies would be worth greater than a month‚Äôs worth of subscription fees which is the equivalent of unlimited movies, tv shows, documentaries and much more. One might argue that at least you‚Äôll own the right to watch it forever but if you really think it through, how many of us re-watch our movies? Besides, there‚Äôs something hard to explain about the in person cinema experience. Marketing ‚Äî More efficient ways, including regulation, need to be kept in place. Think forward to when we have 20 GLL films, will we still go out to the streets to market them?",
    "section": 6
  },
  {
    "id": "doc-117",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "Think of the noise pollution and confusion this would cost Funding ‚Äî One might think that at least we can work with overdraft in the creative industry if it‚Äôs on a large scale, right? Well, wrong. Not many individuals and corporates who can afford the investment are willing to take the risk (Which in itself is quite high since most investments in the film industry are not recouped) In light of the above conditions once we have extrapolated the situation to the future, I have some personal recommendations on how the industry can be modelled so as to be more efficient and self-sustaining: Development of a centralized platform where one can get Kenyan content. I frequently struggle on where I can find Kenyan films without paying a subscription fee that I might never fully utilize. A centralized platform could solve issues of accessibility and could even contribute to the reduction in price of the film. We could explore the buy/rent/subscribe models and see where each gets us.",
    "section": 7
  },
  {
    "id": "doc-118",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "Development of a crowdfunding platform where the public can invest in a film idea and get part ownership of the film or as debt financing for interest. One could choose to support films that push certain themes such as peace, women empowerment, anti-discrimination and many more. Shared ownership with the public will gather up support from the masses that invested in terms of Marketing and even legitimacy so as to avoid the film from being banned the way I am Samuel was. Film producers could market their films on this marketplace using the names of famous casts and crew who have already signed up for their project. This will more than likely solve the problem of a lack of financing. I am not sure how much GLL cost but 40 sticks, which is now on Netflix, cost around Ksh 11 million, which is just a little over USD 100, 000.",
    "section": 8
  },
  {
    "id": "doc-119",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "If one raises just USD 100, 000 then they could use the raised amount as collateral and borrow from a traditional financial institution such as a bank, and give them a share of the film if the loan is too big. In this way, the film could guarantee itself. Development of more studios, talent agencies and structures that support all players in the industry. This will surely lead to an increase to the amount of content that we are putting out and this will surely attract both local and international interest, for the more films we make the more we learn from them and the better we can make future ones. Increased government support in terms of regulation and subsidies. Kenya is one of the countries on the continent where the government has actually shown great interest and support for the film industry but even more is needed. The KFC (Kenya Film Commission) chairman Timothy Owase has spoken of filming treaties between Kenya and other countries, establishment of a film fund and much more.",
    "section": 9
  },
  {
    "id": "doc-120",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "We can only hope that these plans come to fruition. Development of a Lean Film Production methodology (Yup! Just made that up) for films just like the way we do with Lean Start-ups. We could, for example, only include the beats of Save the Cat that are relevant the particular genre concerned. We could also stock up scripts and other resources and personnel in a decentralized manner so that when the opportunity presents itself shooting can be done in a matter of days (Maybe even less than the 8 days that GLL took). This could start with film adaptations of books for our literature, too, is not getting its fair share of the limelight In conclusion Kenya has a budding new industry in film, with the potential of generating up to Ksh 40 billion in revenues annually. We have very talented, creative, hardworking and committed people working towards making the industry profitable, self-sustaining and easily accessible.",
    "section": 10
  },
  {
    "id": "doc-121",
    "source": "kenyan_film_article",
    "title": "How to Organically Grow the Kenyan Film Industry: A GLL Case Study",
    "content": "The next time you have both the time and the cash to spare, check out one of the Kenyan films you are yet to watch. This will go a long way in supporting local filmmakers.",
    "section": 11
  },
  {
    "id": "doc-122",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "In this article, I discuss how to think originally Countless philosophers and successful startup founders frequently advise us to ‚Äòthink originally‚Äô. It‚Äôs weird to think that most of us think unoriginally or in a fake way. It‚Äôs even more peculiar to call anything original in our ever increasingly complex and interconnected world. We pick up on ideas and information, actively or passively, and some trickle down into our subconscious and may emerge again veiled as a flame of creative outburst. Therefore, some of the ideas, ideals and ethics that we credit to ourselves may have never been ours to begin with. Yet it is not entirely impossible, because nothing ever really is, to independently come up with a most common idea. In his book, Sapiens, Yuval Noah Harari states that agriculture may have begun independently along fertile river valleys all over the world from Mesopotamia to the Yellow river in China. So how do you know if your ideas are original?",
    "section": 0
  },
  {
    "id": "doc-123",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "How do you know that you have gone beyond the combined experiences of human life, which Carl Jung called the Collective Unconscious, and have begun to generate original ideas? I guess an even more important question is this: How does one start to think originally? Paul Buchheit, the founder of Gmail and investor of JustinTV which later became Twitch, gave a talk recently reflecting on his startup experiences and gifting us with what he considers essentials of original thinking. I have outlined the key points below. I don‚Äôt know anything Back in High School, our Deputy Principal gave us a quote which we all laughed at. The quote was: ‚ÄúI am the wisest man alive, for I know one thing, and that is that I know nothing. ‚Äù To his benefit, we did change his nickname from the previously boring ones to Wiseman.",
    "section": 1
  },
  {
    "id": "doc-124",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "The quote by Socrates has many interpretations but the one I like most is that he was contemplating the vastness and infinity of knowledge that exists in the world compared with what he knew. It would seem insignificantly small, to put it bluntly. Most of us fear knowing too little. We guess and pretend and use lots of words instead of stating the truth which is simply: We do not know. In Swahili we have the proverb Kuuliza si ujinga which means to question is not to be foolish. One who asks might be a fool for a moment but the one who doesn‚Äôt will be a fool forever. The next time you catch yourself using jargon and other means to try and cover up a lack of knowledge just use the Feynman razor and question instead of assuming and pretending. And yet this is only one half of not knowing anything. The other half is about constancy. It takes 10, 000 hours of deliberate practice to become an expert on something.",
    "section": 2
  },
  {
    "id": "doc-125",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "However, time and intentionality are not the only parts that exist in that equation. The system must be predictable, there must be a way to receive feedback on most recent performance, there must be a way to compare results with precedents in the past or with others‚Äô in the present and many more factors. Most people know this; They know they should question, they know they should remain hungry for knowledge and strive to be lifelong learners but what is unknown to most is the danger that lies in becoming an expert. Paul called it the danger of experience and expertise. You become an expert by recognizing patterns and recognizing and applying rules. To be originally creative, one has to break these rules. Yet as you work on becoming an expert, you follow them for thousands of hours and thus why experts are mostly in a prison of their own design.",
    "section": 3
  },
  {
    "id": "doc-126",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Jeff Bezos says that you must maintain the curiosity of a novice even as you become more proficient as an expert, for that is the sweet spot where magical innovation happens. For those thinking that one can only either be a novice or an expert to succeed, that is what we refer to as the Tyranny of the OR (a kind of false dichotomy). It is a mental fallacy that clouds judgment and impedes progress. To counter the tyranny of the OR, we must embrace the genius of the AND. The genius of the AND is not a balance. It is both X and Y, not X sometime and Y some other time. It is extreme X and extreme Y. We must become both great in depth experts in our topics but also have the curiosity and imagination associated with novices. A good example of the genius of the AND is when Jeff Bezos was asked about work-life balance. His answer, which I consider a masterpiece, is that you can never attain a work-life balance. He sees it as more of a work-life cycle.",
    "section": 4
  },
  {
    "id": "doc-127",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "You need to be happy and satisfied at home for you to perform optimally in the workplace and vice versa. It is more of a cycle than a simple trade off of time and attention. In the same manner, successful startup founders are a dichotomy of seemingly contrasting characteristics. They might be billions but low on liquidity. They might be arrogant when it comes to ideas and humble when it comes to market-proven facts. Great companies are the same too. They pursue profits AND ideals AND impact. Not OR. Not sometimes this and the other that. One has to have both short term and long term goals and deliverables and pursue both. Let the short-term be subject to feedback and pivoting but remember to keep the long term ones constant no matter what. That‚Äôs how you remain dynamic and consistent at the same time. The danger of experience and expertise is the reason why young founders tend to be more successful companies.",
    "section": 5
  },
  {
    "id": "doc-128",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "I will caveat this with the fact that I mean young and heart and mind and not just in age and experience or lack thereof. Truthfully speaking, success requires a certain level of arrogance that is demanded by contrarian thinking. At some point, you have to stop listening to even those who know, for knowledge is a prison of its own kind. Do not be limited by other people‚Äôs experiences. Just because something worked before doesn‚Äôt mean it will now. Similarly, just because something has never worked doesn‚Äôt mean it never will. Technology has brought us closer, allowing for people to rent spaces (via AirBnB) half a world away in places they might never visit in their lifetime in order to donate money to Ukraine war victims. The possibilities are infinite. However, we have to remember that the best opportunities live in our collective blind spots (and they seem either bad or unimportant). Something that might be even more dangerous than experience, is dogma and ideology.",
    "section": 6
  },
  {
    "id": "doc-129",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "This is the reason why ideological societies are less innovative. I guess science has to kill god if progress is to be imminent. If you think that you are rational then you are already in the trap for a lot goes on underneath our consciousness both with neither our knowledge nor our permission. In terms of valuing a team (for investors) or a prospective hire/co-founder (for founders), I would suggest valuing the rate of improvement more than experience and current accomplishments. ‚ÄúI am the wisest man alive, for I know one thing, and that is that I know nothing.‚Äù Socrates Thinking modern man 2. Kill all daemon processes Daemons are OS background programs that perform services which are often invisible to the user. They consume memory, processor and other resources. This is the reason why computers and mobile phones work better after a reboot. In our context, daemons include doubt, anger, self-loathing, the internalized voices of our parents, media, the internet and many more.",
    "section": 7
  },
  {
    "id": "doc-130",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "If you catch yourself in a daemon process, take some time to find clarity of mind. Meditate, walk in nature, draw, play, read, write etc. Do not assume to know what is pulling you back for as already stated, you know nothing. Question the daemon out of existence. The stoics had such an MO for anger when insulted. They would ask one of two questions. If the said words were true, why should I be angered by the truth? If the said words were false, why should lies anger me? Thinking logically on these two are sure ways to conclude that anger due to insults is unjustified. It is a result of ego and ego is the enemy. The daemon is just a symptom. 3. Accept and move Forward (Yes and Thank You) Even if you avoid risk all your life, life will still not go as planned. Setbacks in life and startups are inevitable. As a matter of fact, reaction to setbacks is a predictor of success. Some of the best things and ideas are rooted in some of the worst of events.",
    "section": 8
  },
  {
    "id": "doc-131",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "When asked how founders should go about pivoting, the great entrepreneur and billionaire Peter Thiel said that no one should anticipate pivoting. Pivoting is a signal that you have done something wrong or you were wrong in your assumptions and no one plans for that. You should, however, build tenacity and be willing to accept facts. To succeed, you need such strong self-belief that it could be confused with delusion. This should, however, be caveated with a fast feedback loop that proves what is true and which assumptions are wrong. Reid Hoffman likens this loop to the OODA (Observe-Orient-Decide-Act) loop that is used in piloting. Become a member Naval Ravikant says to work as hard as you can even if what you work on and who you work with is more important. In the startup landscape, team is slightly more important than idea because the team can always pivot. Ideas are a dime a dozen. In fact you can find a couple if you go on TiBA right now.",
    "section": 9
  },
  {
    "id": "doc-132",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "The speed of execution and learning is essential for success. Once you decide what to work on, you must move as fast as is humanly possible be it in a team or alone. Momentum compounds and success begets success. You can get to the 90th percentile of your area of expertise by working hard or working smart. However, to get to the 99th percentile, you must do both. Focus with all your strength on your chosen path for focus is a multiplier of effort. Work stamina is actually a predictor of long term success. The concept of Knowledge-execution gap can be used to check how much of the knowledge we are acquiring is just in time knowledge as compared to just in case knowledge. Almost everyone has this gap. There are some things that are nice to know and these are just in case knowledge which do in fact come in handy when climbing mountains, as we will see in the next point.",
    "section": 10
  },
  {
    "id": "doc-133",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "There are also those things that we need to know to pass exams, be better friends and partners or run ventures successfully. This is the just in time information that leads to a lesser knowledge-execution gap is what one is encouraged to get more of for you to execute on them almost instantly testing their validity, relevance and usefulness. One might ask: When will you know if you have learnt enough to execute? To be honest, I have no idea. I, however, doubt that you will ever get to that point. Execute as a means of learning and testing hypotheses for that is the way of science. Be that as it may, it might interest you to learn that there‚Äôs actually an algorithm that can help with this dilemma. It is called the explore-exploit algorithm. Exploration is the act of figuring out new stuff, of learning and adventure whereas exploitation is the stage where you execute on what you have already learnt and capitalize on it.",
    "section": 11
  },
  {
    "id": "doc-134",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Both these stages are very important but you can choose which one you want to be in depending on a number of factors. The first is time. Exploration needs a lot of time. You need to set aside tons of unaccountable time because you are getting into a new field and you must crawl before you can walk then run. As a rule of the thumb, explore when you have time and exploit when you don‚Äôt. You should be optimistic in exploration so that you can minimize regret, the regret of having not explored enough. You should also be thorough in exploring so that it can benefit you in your exploitation face. Remember, failures are just information that we can use to make better explore-exploit decisions in the future. The exploration stage gives long term returns in terms of memory and satisfaction whereas the exploitation stage gives the same on a short term basis. This is thus a trade-off between the long term and the short term.",
    "section": 12
  },
  {
    "id": "doc-135",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "In his book, Algorithms to live By, Brian Christian states that: ‚ÄúLife is a balance between novelty and tradition, between the latest and the greatest, between taking risk and souring what we know and love.‚Äù The concept of Knowledge-execution gap can be used to check how much of the knowledge we are acquiring is just in time knowledge as compared to just in case knowledge. 4. Choose the more interesting Path (The Path Not Taken) Interestingness is a sign of unexplored/under-explored territory. Great startups exist in a state of productive uncertainty. Regardless of success or failure, they learn something interesting. If you have only one definition of success then you are setting yourself up for failure. You can actually guarantee success by redefining it to mean ‚Äòlearn something interesting.‚Äô Then you will both succeed and learn a lot. Learning should, however, be built into your day-to-day activities and not just a word that you use to justify a failed endeavour.",
    "section": 13
  },
  {
    "id": "doc-136",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Try and set yourself up for learning by integrating an OODA loop into your work, project, startup and even life. Start with an open ended question, make some assumptions and then test these, doubling down on what works and cutting down on what doesn‚Äôt. The easiest way to measure value and impact is numbers and that is why Nicki Minaj sure knows what she‚Äôs saying when she sings, ‚ÄúWe made the greatest impact check the spreadsheet‚Ä¶‚Äù Back when I was doing my college admissions, my college Counsellor advised us to be bold in highlighting our uniqueness. She used to say that college admissions officers admit students the way florists pick flowers for a bouquet: The more unique the better the chances of being picked. This is where contrarian thinking comes in. If we all had the same experiences then we would come to the same conclusions, right?",
    "section": 14
  },
  {
    "id": "doc-137",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Steve Jobs once defined the word smart using a city analogy: Guys are down there with maps trying to make sense of routes yet you can see all of it from the 80th floor of a building making connections where the rest can‚Äôt. To do this, you need different experiences than anyone else. In life we‚Äôre always climbing mountains, moving from one challenge to another be it socially, academically or professionally. However, with a land full of mountains made possible by the internet, how do we know that we are climbing the very highest one? How do you, for instance, know that getting your undergrad degree will lead to the satisfaction that you want and increase your employability? How do you know that that relationship you are chasing will allow you to become the best version of yourself?",
    "section": 15
  },
  {
    "id": "doc-138",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Fortunately, this is a problem that has been pondered on before and we now know that the best way to increase the chances that you are climbing the highest mountain is to drop yourself in different landscapes then climb the highest mountain that you see. If you do this a couple of times, then chances only increase much more. So, you don‚Äôt drink nor smoke but are wondering how to network? Pay a visit to that bar. Are you all about climate change and social impact? Visit the next club meeting on making money and growing your net worth. The most exciting things pass us by because they are hidden within plain sight. There are worlds upon worlds. This is not an invitation to do everything that can be done. This is just a suggestion to immerse yourself in unfamiliar territory every now and then and then go with the flow. The flow in this case is what you like, enjoy or completely love. That is your highest mountain in sight.",
    "section": 16
  },
  {
    "id": "doc-139",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Only by pursuing our genuine curiosity can we find specific knowledge. Specific knowledge is knowledge so unique to our persona that only we can replicate it. It‚Äôs like certain music or public speech, or even creativity woven into a film, a podcast, a TV or radio show or just a book, blog or software. ‚ÄúWe made the greatest impact check the spreadsheet‚Ä¶‚Äù Nicki Minaj 5. Love What You Do The advice being peddled everywhere is that you should do what you love. This is wrong because of a number of reasons. First, you might never find that thing that you love. Everyone is drawn to something but this is mostly when it is new and unfamiliar. Once we get used to it, the initial glow of unfamiliarity fades and it becomes a chore, a bother or an outright burden. Jeff Bezos reminds us that although we have been granted gifts and talents, we must never think that these determine our lives.",
    "section": 17
  },
  {
    "id": "doc-140",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Yes, to an extent, our passions choose us more than we choose them but that doesn‚Äôt necessarily mean that we have to pursue them. Our decisions, not our passions, are what determines what direction our lives will take. Another reason why you shouldn‚Äôt do what you love is that this advice assumes that what you love is fixed. When I was young, I used to love suits. Right now, I can‚Äôt stand being in one. Interests change, passions change, desires fade‚Ä¶ Such is the nature of life. We will find ourselves, now or in the future, where we despise what is required of us. Should we then not do it so as to follow this advice? Of course not! Responsibility is part of maturity. What we need is to change how we do what we are doing rather than changing what we are actually doing or neglecting it altogether. Being goal oriented might sound nice in theory but in reality it treats the time between now and task completion as an obstacle, a period to suffer and labour through.",
    "section": 18
  },
  {
    "id": "doc-141",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "This is tough particularly in startups. An intrinsic approach to motivation and satisfaction ultimately increases the quality of our effort. You should do stuff to impress yourself. I know of a founder who ranks ideas based on how happy he would be if they worked, not on viability, scalability or potential. Focus on the next step rather than the end goal; Though you should keep it at the back of your mind since life and startups are long term games. You should maintain a meaningful and inspiring vision to hack the grind dull moments because a startup is just like a baby (5% cute and adorable; 95% diapers and vomit). Just Like a Baby, a startup is 5% Cute and adorable and 95% diapers and vomit. 6. Maintain a healthy disregard for the impossible Nothing is impossible. I doubt that is true for two reasons. First, no one has ever tried everything that can be tried to find out if one of them is impossible. And second, we can‚Äôt use the past to predict the future.",
    "section": 19
  },
  {
    "id": "doc-142",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "The future is sooner and more peculiar than we can predict. Things that seemed, and most probably were, impossible just a couple hundred years back are now the new norm. However, to succeed, you need to maintain a healthy disregard for the impossible. This is why you should hire and spend time with rational optimists. Like Thanos, you must make yourself inevitable. Build leverage and competitive advantage so much that you are impossible to compete with. Compound yourself by letting go of small linear opportunities. This can be done by moving from zero to one every time rather than one to n, which is just copying or adding unto what already exists in the world/market. You should have insanely great ideas in your startup that sound like you want to take over the world. Rather than succeed at something inconsequential, it is much better to fail at something awesome. Encourage doing impossible things. Impossible becomes the new normal quite quickly.",
    "section": 20
  },
  {
    "id": "doc-143",
    "source": "journey_to_original_thought_article",
    "title": "Journey To Original Thought",
    "content": "Startups are machines for harnessing the fire of self interest, creating a self sustaining reaction capable of transforming the world. Economically, we don‚Äôt need more jobs, we need more Steve Jobs.",
    "section": 21
  },
  {
    "id": "doc-144",
    "source": "index",
    "title": "Home",
    "content": "<!-- Inline carousel-only styles (kept here so they don't clash with grid) --> <section id=\"about\" class=\"section\" data-aos=\"fade-up\">   <div class=\"about-container\">     <img src=\"{{ '/assets/images/me.jpeg' | relative_url }}\"          alt=\"Benson Mugure\"          class=\"profile-pic\">     <div class=\"about-text\">       <h1>Hello World!",
    "section": 0
  },
  {
    "id": "doc-145",
    "source": "index",
    "title": "Home",
    "content": "I'm Benson Mugure</h1>       <p>Software Engineer | AWS Certified | Writer</p>       <p>I build secure, resilient, high-performing, cost-optimized cloud-native solutions.</p>       <p><strong>Core skills:</strong> AWS ¬∑ Python ¬∑ Linux ¬∑ Celery ¬∑ Django ¬∑ Docker ¬∑ GCP</p>       <p class=\"social-links\">         <a href=\"https://www.linkedin.com/in/benson-mugure-017153196\" target=\"_blank\" aria-label=\"LinkedIn\">           <i class=\"fa-brands fa-linkedin\"></i>         </a>         <a href=\"https://dev.to/virgoalpha\" target=\"_blank\" aria-label=\"Dev.to\">           <i class=\"fa-brands fa-dev\"></i>         </a>         <a href=\"https://github.com/Virgo-Alpha\" target=\"_blank\" aria-label=\"GitHub\">           <i class=\"fa-brands fa-github\"></i>         </a>         <a href=\"https://medium.com/@b.mugure\" target=\"_blank\" aria-label=\"Medium\">           <i class=\"fa-brands fa-medium\"></i>         </a>         <a href=\"https://www.omprakash.org/citizen/benson-mugure\" target=\"_blank\" aria-label=\"Omprakash\">           <img src=\"assets/images/omprakash.jpeg\" alt=\"Omprakash\" style=\"width:36px;height:36px;\">         </a>       </p>     </div>   </div> </section>",
    "section": 1
  },
  {
    "id": "doc-146",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "In this article, I discuss What happens when you type https://www.google.com in your browser and press Enter? Computers and other devices communicate using IP addresses to identify each other on the internet. But humans can‚Äôt remember IP addresses, so they use words (URLs) instead: such as www.google.com. The domain name system (DNS) brings the two together and gets you to your destination. DNS is, in simple words, the technology that translates human-adapted, text-based domain names to machine-adapted, numerical-based IP. Once you type the URL in the browser, the browser first checks if it knows that URL then it asks the computer‚Äôs OS. If the OS doesn‚Äôt know then it asks the resolver server (which is usually the Internet Service Provider). All resolvers must know one thing: where to locate the root server.At this stage, your local computer (the client) is making requests over the internet. To do this, it follows certain protocols.",
    "section": 0
  },
  {
    "id": "doc-147",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "TCP/IP is a suite of protocols used by devices to communicate over the Internet and most local networks. It is named after two of it‚Äôs original protocols ‚Äî the Transmission Control Protocol (TCP) and the Internet Protocol (IP). TCP provides apps a way to deliver (and receive) an ordered and error-checked stream of information packets over the network.The term ‚Äúpackets‚Äù describes the format in which the data is sent from server to client. What do we mean here? Basically, when data is sent across the web, it is sent in thousands of small chunks. There are multiple reasons why data is sent in small packets. They are sometimes dropped or corrupted, and it‚Äôs easier to replace small chunks when this happens. Additionally, the packets can be routed along different paths, making the exchange faster and allowing many different users to download the same website at the same time.",
    "section": 1
  },
  {
    "id": "doc-148",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "If each website was sent as a single big chunk, only one user could download it at a time, which obviously would make the web very inefficient and not much fun to use. The resolver then asks the root server. (The root server knows where to locate the .COM TLD server. TLD stands for Top-Level Domain.)The root server gives the resolver the location of the .COM TLD server & the resolver saves it so that it doesn‚Äôt need to come back to the root server again. There are 13 root servers that exist today. They are scattered around the globe and operated by 12 independent organizations.They are named [letter].root-servers.net where [letter] ranges from A to M. This doesn‚Äôt mean that we have only 13 physical servers to support the whole internet! Each organization provides multiple physical servers distributed around the globe. The resolver then heads to the .COM TLD server and asks about the URL.",
    "section": 2
  },
  {
    "id": "doc-149",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "The coordination of most top-level domains (TLDs) belong to the Internet Corporation for Assigned Names and Numbers (ICANN). The .COM TLD was one of the first created in 1985 and today it is the largest TLD in the internet. Other types of TLDs include: ‚Üí Country code TLDs.Usually, their 2 letter ISO code such as .au for Australia & .uk for the United Kingdom ‚Üí Internationalized country code TLDs. (TLDs written in native languages) ‚Üí Generic TLDs: .NET, .ORG, .EDU, etc‚Ä¶(Usually with 3 or more letters) Become a member ‚Üí Infrastructure TLDs: .ARPA, mostly used for reverse DNS lookups. ‚Üí (And today, many new generic TLDs are being created!) The . COM TLD server directs the resolver to the name server and the resolver saves this location. The .COM TLD server is able to make this connection with the help of the Domain registrar.",
    "section": 3
  },
  {
    "id": "doc-150",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "When a domain is purchased, the domain registrar reserves the name and communicates to the TLD registry the authoritative name servers.The authoritative name server gives the resolver the IP address of the URL and the resolver starts its way back after saving that information. The resolver communicates its findings (The .COM server from the root, the authoritative name server address from the .COM TLD server and finally the IP address of the URL from the authoritative name server) to the OS which then relays it to the browser. The browser then queries the IP address given using a http request for the code (HTML, CSS, JS). HyperText Transfer Protocol Secure (HTTPS) is the secure version of HTTP, the protocol over which data is sent between your browser and the website that you are connected to. The ‚ÄòS‚Äô at the end of HTTPS stands for ‚ÄòSecure‚Äô. It means all communications between your browser and the website are encrypted.",
    "section": 4
  },
  {
    "id": "doc-151",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "HTTPS is often used to protect highly confidential online transactions like online banking and online shopping order forms. When you request a HTTPS connection to a webpage, the website will initially send its SSL certificate to your browser. This certificate contains the public key needed to begin the secure session. Based on this initial exchange, your browser and the website then initiate the ‚ÄòSSL handshake‚Äô. The SSL handshake involves the generation of shared secrets to establish a uniquely secure connection between yourself and the website. When a trusted SSL Digital Certificate is used during a HTTPS connection, users will see a padlock icon in the browser address bar. When an Extended Validation Certificate is installed on a web site, the address bar will turn green.",
    "section": 5
  },
  {
    "id": "doc-152",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "A web server is a software that responds to HTTP requests from client computers and delivers web pages whereas an application server is system software that resides between the operating system (OS) on one side, the external resources (such as a database management system [DBMS], communications and Internet services) on another side and the users‚Äô applications on the third side. Ever wonder how Facebook, Linkedin, Twitter and other web giants are handling such huge amounts of traffic? They don‚Äôt have just one server, but tens of thousands of them. In order to achieve this, web traffic needs to be distributed to these servers, and that is the role of a load-balancer. Press enter or click to view image in full size Apart from the files containing the static code (codebase), most websites have a database that collects and stores data from users. This database together with the application server makes the dynamic part of the website.",
    "section": 6
  },
  {
    "id": "doc-153",
    "source": "google_com_browser_article",
    "title": "What happens when you type https://www.google.com in your browser and press Enter?",
    "content": "The results of the http requests are then loaded onto the webpage depending on the settings of any available firewall. A firewall is a division between a private network and an outer network, often the internet, that manages traffic passing between the two networks. It‚Äôs implemented through either hardware or software. Firewalls allow, limit, and block network traffic based on preconfigured rules in the hardware or software, analysing data packets that request entry to the network. In addition to limiting access to computers and networks, a firewall is also useful for allowing remote access to a private network through secure authentication certificates and logins.",
    "section": 7
  },
  {
    "id": "doc-154",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "In this article, I discuss Mastering Google Apps Script: Free Automation in Google Workspace TLDR; Google Apps Script is a serverless service by Google that facilitates automation of workflows in Google suite It uses Javascript and the scripts can be run using triggers It is limited by various factors such as lack of a package manager and execution time limits Apps Script projects can be Bound Scripts, directly linked to a specific Google file, or Standalone Scripts, existing independently in Google Drive, with each type suited for different purposes For practical usage without the theory, you can skip right onto the case study. Table Of Contents Introduction Sample Use Cases Triggers Core Services Bound vs Stand Alone Scripts Handling Environment Variables Project Management Limitations Case Study Conclusion Introduction I was looking for a way to automate some processes in Google Workspace and thought this was a good use case to try out n8n or Zapier.",
    "section": 0
  },
  {
    "id": "doc-155",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "However, I took a step back and wondered if there was a free solution within the Google suite. That‚Äôs how I stumbled upon Google Apps Script and decided to explore it. In this article, I go through the main features, limitations and use cases of Google Apps Script then I demonstrate using sample code how one would go about automating a certain monthly financial process. What is Google Apps Script? Google Apps Script is a cloud-based scripting platform based on JavaScript that lets you automate, integrate, and extend Google Workspace applications. It's \"serverless,\" meaning you don't need to worry about hosting or infrastructure. Google handles it all. This makes it incredibly accessible. Sample Use Cases The big 3 use cases for Google Apps Script are: Automation: The most common use. Automate repetitive tasks like sending templated emails, generating reports in Sheets from data, or organizing files in Drive. Integration: Connect different Google services.",
    "section": 1
  },
  {
    "id": "doc-156",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "For example, automatically create a Calendar event from a Google Form submission, or log Gmail attachments into a Google Sheet. Customization: Extend the user interface of Google Workspace. You can create custom menus, dialogs, and sidebars in Sheets, Docs, and Forms to build custom workflows for users. The above offer endless possibilities in both business and personal areas.",
    "section": 2
  },
  {
    "id": "doc-157",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "A few of the ones that I thought of were: Automatically saving email attachments to a folder and alerting the user, as well as updating a spreadsheet Making daily API calls to a weather site to see if there is a torrential rain warning or cyclone and alerting the user via an email if there is Creating custom menu in google sheets where a single click generates a report and sends it to clients in a customized mail merge kind of way Google form validation Automatically add invites from a certain email address into my calendar Triggers Triggers are what make your scripts run automatically in response to specific events. Types of triggers: Simple Triggers: Easy-to-use, built-in functions like onOpen() (runs when a document is opened) and onEdit() (runs when a cell is edited). Installable Triggers: More powerful and flexible. These can be time-driven (e.g., run a script every morning at 9 AM) or event-driven (e.g., run a script when a Google Form is submitted).",
    "section": 3
  },
  {
    "id": "doc-158",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "In order to automate your scripts, you will need to add a new trigger from the menu bar found on the left as can be seen below. Once you go to that page, on the bottom right, you will see a button to add a trigger. Clicking that button opens the modal below. The trigger can be time driven or calendar driven. The time driven option gives the following categories for timers: Specific date and time Minutes timer Hours timer Day timer Week timer Month timer These timers allow you to now select how often the script should run, e.g., every 5 minutes for the minutes timer or Every Monday for the week timer. Pitfalls to Watch Out For Time-driven triggers can fail silently if the script takes too long or errors out. Installable triggers require authorization‚Äîif not granted properly, they won‚Äôt run. Google may throttle or delay time-based executions under heavy load or policy violations. Always monitor the Executions panel for logs and failures.",
    "section": 4
  },
  {
    "id": "doc-159",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Core Services These are the built-in libraries that allow your script to interact with Google services. You don't need to import anything; they are just available. However, you may need to enable them or add them to your project. Key services include: SpreadsheetApp: For reading, writing, and formatting data in Google Sheets. GmailApp: For reading, searching, and sending emails. DocumentApp: For creating and editing Google Docs. DriveApp: For managing files and folders in Google Drive. UrlFetchApp: For connecting to external, third-party APIs on the internet. Bound Vs Stand Alone Scripts Bound Scripts: These are linked directly to a specific Google Sheet, Doc, or Form. They are best for scripts that are only meant to work with that one file. Standalone Scripts: These exist as their own independent files in Google Drive. They are better for general-purpose scripts or for building web apps and add-ons.",
    "section": 5
  },
  {
    "id": "doc-160",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Deployment Considerations Bound Scripts are easier to deploy for quick file-specific automations. Standalone Scripts are necessary for publishing web apps, libraries, or add-ons, and for handling broader integrations across multiple services and files. Handling Environment variables When working with sensitive data such as API keys or tokens, never hardcode credentials directly into your code. Doing so risks exposing them‚Äîespecially if your script is shared or published as a web app. Instead, use the PropertiesService to securely store and access secrets. This approach: Keeps your credentials separate from your code logic. Prevents accidental leaks in version control or shared scripts. Makes it easier to manage and rotate secrets without editing source files. Step 1: Store the Secret Create a separate function to set your secret. You only need to run this function once manually from the script editor to save the key.",
    "section": 6
  },
  {
    "id": "doc-161",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "function storeApiKey() {   // Get the script private properties store   const scriptProperties = PropertiesService.getScriptProperties();   // Set a key-value pair for your secret   scriptProperties.setProperty('MY_API_KEY', 'your-secret-api-key-goes-here');   Logger.log(\"API Key has been stored securely.\"); } Step 2: Retrieve the Secret in Your Code In your main functions, you can then retrieve the key without ever exposing it in the script itself.",
    "section": 7
  },
  {
    "id": "doc-162",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "function makeApiCall() {   // Get the script properties store   const scriptProperties = PropertiesService.getScriptProperties();   // Retrieve the stored secret by its key   const apiKey = scriptProperties.getProperty('MY_API_KEY');   // Now you can use the apiKey variable in your API call   const url = https://api.example.com/data?key=${apiKey};   const response = UrlFetchApp.fetch(url);   Logger.log(response.getContentText()); } This method ensures your sensitive information is kept separate from your code, which is essential for security. Project management One Apps script project can have multiple files which can be triggered separately but cannot have different declarations of variable names. All script files (.gs) within a single Apps Script project are executed in one shared global scope. Think of it as Google taking all your separate files, concatenating them into one large file behind the scenes, and then running it.",
    "section": 8
  },
  {
    "id": "doc-163",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "This is why you can't redeclare a variable with const or let in another file‚Äîfrom the engine's perspective, you're trying to declare the same variable twice in the same script. This global nature is also what makes calling functions between files so seamless. Considerations for Splitting a Project into Multiple Files Splitting your code is purely for organization and readability. It has no effect on how the code runs. Here are a few things to consider before you do it: Logical Separation: Group related functions into the same file. For example, have one file for all functions that interact with Google Sheets (sheets.gs), another for Gmail logic (gmail.gs), and a main file for the primary workflow (main.gs). Configuration: Keep global constants and configuration settings (like spreadsheet IDs, email addresses, or API keys stored in Properties Service) in a dedicated file (e.g., config.gs). This makes them easy to find and update.",
    "section": 9
  },
  {
    "id": "doc-164",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Maintainability: For large projects, splitting the code makes it much easier to navigate, debug, and for other people to understand. A single 1,000-line file is much harder to work with than five 200-line files with clear purposes. One Project vs. Multiple Projects The decision to keep code in one project or split it into different projects depends on the tasks you are automating. Keep it in one project if: The scripts are part of a single, cohesive workflow (e.g., reading from a Sheet, processing the data, and sending an email). The functions in different files need to call each other or share global variables. The entire workflow can operate under a single set of permissions (e.g., the whole project needs access to both Sheets and Gmail). Split it into multiple projects if: The automations are completely unrelated (e.g., one script organizes your Drive, and another sends you a daily weather report). The automations require different security permissions.",
    "section": 10
  },
  {
    "id": "doc-165",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Separating them ensures one script doesn't have access to services it doesn't need (e.g., one script only needs access to a specific Sheet, while another needs access to your entire Calendar). They run on completely independent triggers and have no logical connection. How Different Files Interact Because all files share the same global environment, calling a function from another file is effortless. You just call it directly by name as if it were in the same file. Limitations 1. Cannot Decrypt Password-Protected Files A script can see a password-protected file in Google Drive, but it cannot open or read its contents. The Apps Script environment has no built-in mechanism to supply a password to decrypt a file, which is why a more capable environment like a Python script or Google Cloud Function is required for this task. 2. Limited Native File Processing Apps Script cannot natively parse complex file formats like PDFs or Excel spreadsheets to extract data directly.",
    "section": 11
  },
  {
    "id": "doc-166",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "While it can convert some files to Google Workspace formats (e.g., PDF to Google Doc), it doesn't offer granular control to read the raw data or structure from the original file itself. 3. Execution Time Limits Scripts have a maximum execution time. For most standard Google accounts (including Gmail and Google Workspace), this limit is 6 minutes per run. For long-running tasks like processing hundreds of files or spreadsheet rows, your script may time out before it can finish. 4. Service Quotas and Rate Limits To prevent abuse, Google imposes daily quotas and rate limits on the use of its services. For example, there are limits on how many emails GmailApp can send per day, how many API calls you can make, or how many triggers can run. For large-scale automations, you can hit these limits. 5.",
    "section": 12
  },
  {
    "id": "doc-167",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Sandboxed Environment and No Package Manager Apps Script runs in a secure, sandboxed environment, which means: You cannot use standard package managers like npm to import external JavaScript libraries. You have no direct access to the server's file system or the ability to make arbitrary network connections. 6. Simple Trigger Restrictions Simple triggers like onOpen(e) and onEdit(e) run in a restricted mode. They cannot access any service that requires user authorization. For example, an onEdit trigger cannot send an email or create a calendar event, which is a common source of confusion for new developers. Case Study The original idea I wanted to automate was that of investing. Every time I get my payslip, I usually save it to a certain folder and then calculate how much of a particular stock I can buy for that month then I go ahead and place the order via email. The below step-by-step guide will show you how to automate this entire workflow. Now, let‚Äôs get coding.",
    "section": 13
  },
  {
    "id": "doc-168",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "If you want to just straight into the code, find the repository here. Step 1: Initial Setup (Do this first!) Create a New Google Sheet. Name it \"My Stock Portfolio\". Inside the sheet, create two tabs: Trading and Transactions. Go to Extensions > Apps Script to open the script editor. This will create a new Apps Script project that is bound to your spreadsheet. Get a Free API Key from Financial Modeling Prep. You'll need this for the stock price data. Create a Google Drive Folder where your payslips and contract notes will be saved. Right-click the folder and get its ID from the URL. Step 2: The Code (Create these files in your Apps Script project) In the Apps Script editor, create the following files by clicking the + icon next to \"Files\". Copy and paste the code for each one. Config.gs (Configuration) // --- CONFIGURATION FILE --- // Store all your personal settings here.",
    "section": 14
  },
  {
    "id": "doc-169",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "const CONFIG = {  // --- Email Settings ---  MY_EMAIL: \"your_email@example.com\", // Your primary email address  BROKER_EMAIL: \"broker@example.com\",   // Your stockbrokers email address   // --- Payslip Email Settings ---  PAYSLIP_SENDER: \"payslips@company.com\",  PAYSLIP_SUBJECT_CONTAINS: \"Your Monthly Payslip\",   // --- Contract Note Email Settings ---  CONTRACT_NOTE_SENDER: \"contracts@broker.com\",  CONTRACT_NOTE_SUBJECT_CONTAINS: \"Contract Note\",  // --- Drive Folder ---  FINANCE_FOLDER_ID: \"YOUR_GOOGLE_DRIVE_FOLDER_ID\",  // --- Financial Settings ---  MONTHLY_SALARY: 10000,  INVESTMENT_PERCENTAGE: 0.20, // 20%  STOCK_TICKER: \"AAPL\",  CDS_ACCOUNT: \"CDS123456FI00\" }; Secrets.gs (API Key Management) // --- API KEY MANAGEMENT --- // Use this file to securely store and retrieve your API key. /  Stores the API key in PropertiesService. Run this function ONCE MANUALLY from the editor after pasting your key.",
    "section": 15
  },
  {
    "id": "doc-170",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "/ function storeApiKey() {  const scriptProperties = PropertiesService.getScriptProperties();  // !!! PASTE YOUR API KEY HERE !!! scriptProperties.setProperty('FINANCE_API_KEY', 'YOUR_FINANCIAL_MODELING_PREP_API_KEY');  Logger.log(\"API Key has been stored securely.\"); } /  Retrieves the stored API key. @returns {string} The API key. / function getApiKey() {  const scriptProperties = PropertiesService.getScriptProperties();  return scriptProperties.getProperty('FINANCE_API_KEY'); } Main.gs (Triggers & Menus) // --- MAIN SCRIPT FILE --- // Contains the main triggers and UI functions. /  Creates a custom menu in the spreadsheet when its opened. / function onOpen() {  SpreadsheetApp.getUi()    .createMenu('Stock Trading')    .addItem('üìà Place New Trade Order', 'showTradeDialog')    .addToUi(); } /  Main function to process incoming emails. Set up a time-driven trigger to run this every 5-10 minutes.",
    "section": 16
  },
  {
    "id": "doc-171",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "/ function processAllEmails() {  Logger.log(\"--- Starting email processing cycle ---\");  processPayslipEmails();  processContractNoteEmails();  Logger.log(\"--- Finished email processing cycle ---\"); } GmailProcessing.gs (Email Handling) // --- EMAIL PROCESSING LOGIC --- /  Processes payslip emails, saves the attachment, and sends a notification. / function processPayslipEmails() {  const query = from:\"${CONFIG.PAYSLIP_SENDER}\" subject:(\"${CONFIG.PAYSLIP_SUBJECT_CONTAINS}\") is:unread;  Logger.log(Searching for payslips with query: \"${query}\");  const threads = GmailApp.search(query);  if (threads.length === 0) return;  for (const thread of threads) {    const message = thread.getMessages()[0]; // Process first message    if (message.isUnread()) {      // 1.",
    "section": 17
  },
  {
    "id": "doc-172",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Save attachment      const attachment = message.getAttachments()[0];      if (attachment && attachment.getContentType() === 'application/pdf') {        const folder = DriveApp.getFolderById(CONFIG.FINANCE_FOLDER_ID);        folder.createFile(attachment.copyBlob());        Logger.log(Saved payslip: ${attachment.getName()});      }      // 2. Get stock price and calculate investment      const stockData = getStockPrice(CONFIG.STOCK_TICKER);      let investmentInfo = \"Could not retrieve stock price at this time.\";      if (stockData) {        const investmentAmount = CONFIG.MONTHLY_SALARY  CONFIG.INVESTMENT_PERCENTAGE;        const sharesToBuy = Math.floor(investmentAmount / stockData.price);        investmentInfo = The current price of ${CONFIG.STOCK_TICKER} is $${stockData.price.toFixed(2)}. With 20% of your salary ($${investmentAmount.toFixed(2)}), you could buy approximately ${sharesToBuy} shares.;      }      // 3.",
    "section": 18
  },
  {
    "id": "doc-173",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Send notification email      const subject = \"‚úÖ Your Payslip Has Been Processed\";      const body = Hello,\\n\\nYour payslip has been saved to Google Drive.\\n\\n${investmentInfo}\\n\\nThank you.;      GmailApp.sendEmail(CONFIG.MY_EMAIL, subject, body);      // 4. Mark as read and check the box in the sheet      thread.markRead();      updateMonthlyChecklist();    }  } } /  Processes contract note emails.",
    "section": 19
  },
  {
    "id": "doc-174",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "/ function processContractNoteEmails() {  const query = from:\"${CONFIG.CONTRACT_NOTE_SENDER}\" subject:(\"${CONFIG.CONTRACT_NOTE_SUBJECT_CONTAINS}\") is:unread;  Logger.log(Searching for contract notes with query: \"${query}\");   const threads = GmailApp.search(query);  if (threads.length === 0) return;  for (const thread of threads) {     const message = thread.getMessages()[0];     if (message.isUnread()) {        const attachment = message.getAttachments()[0];        if (attachment) {           const folder = DriveApp.getFolderById(CONFIG.FINANCE_FOLDER_ID);           folder.createFile(attachment.copyBlob());           Logger.log(Saved contract note: ${attachment.getName()});        }        // As we cannot parse the PDF, we notify the user to update the sheet.",
    "section": 20
  },
  {
    "id": "doc-175",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "const subject = \"üìù Action Required: Log Your Recent Trade\";        const body = Hello,\\n\\nA new contract note has been saved to your Drive.\\n\\nPlease open your 'My Stock Portfolio' spreadsheet and log the details of this transaction in the 'Transactions' tab.\\n\\nThank you.;        GmailApp.sendEmail(CONFIG.MY_EMAIL, subject, body);        thread.markRead();     }  } } /  Finds the current month/year row in the 'Trading' sheet and checks the box.",
    "section": 21
  },
  {
    "id": "doc-176",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "/ function updateMonthlyChecklist() {  const sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(\"Trading\");  const data = sheet.getDataRange().getValues();  const now = new Date();  const monthYear = Utilities.formatDate(now, Session.getScriptTimeZone(), \"MMMM yyyy\");  for (let i = 1; i < data.length; i++) {    if (data[i][0] === monthYear) {      sheet.getRange(i + 1, 2).check(); // Check the box in column B      break;    }  } } StockTrading.gs (UI & Trading Logic) // --- STOCK TRADING UI AND LOGIC --- /  Shows the custom HTML dialog for placing a trade. / function showTradeDialog() {  const html = HtmlService.createHtmlOutputFromFile('Dialog.html')      .setWidth(400)      .setHeight(450);  SpreadsheetApp.getUi().showModalDialog(html, 'Place a Stock Trade Order'); } /  Fetches the current stock price to populate the dialog. This function is called from the client-side HTML.",
    "section": 22
  },
  {
    "id": "doc-177",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "/ function getLiveStockPrice() {  return getStockPrice(CONFIG.STOCK_TICKER); } /  Processes the trade order submitted from the dialog. @param {object} orderDetails An object from the dialog form. / function placeTradeOrder(orderDetails) {  const { tradeDirection, quantity, price } = orderDetails;   const subject = Trade Order: ${tradeDirection.toUpperCase()} ${quantity} ${CONFIG.STOCK_TICKER} @ ${price};  const body =     Hello,    Please execute the following trade order for my account (${CONFIG.CDS_ACCOUNT}):    ----------------------------------    Security Name:    ${CONFIG.STOCK_TICKER}    Trade Direction:  ${tradeDirection}    Number of Shares: ${quantity} Shares    Price:            Market or MUR ${price}    Validity:         Maximum 30 days    ----------------------------------    Thank you. ;  try {    // Send email to the broker and BCC self.",
    "section": 23
  },
  {
    "id": "doc-178",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "GmailApp.sendEmail(CONFIG.BROKER_EMAIL, subject, body, {      bcc: CONFIG.MY_EMAIL    });    // Log the transaction to the 'Transactions' sheet    const transactionsSheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(\"Transactions\");    transactionsSheet.appendRow([new Date(), CONFIG.STOCK_TICKER, tradeDirection.toUpperCase(), quantity, price, \"PLACED\"]);    return \"‚úÖ Success! Your trade order has been emailed to the broker and logged.\";  } catch (e) {    Logger.log(Failed to send trade email: ${e});    return ‚ùå Error: Could not send the trade order. Please check the logs.;  } } APIs.gs (External API Calls) // --- EXTERNAL API CALLS --- /  Fetches the latest stock price from Financial Modeling Prep. @param {string} ticker The stock symbol (e.g., \"AAPL\"). @returns {object|null} An object with price and volume, or null on error. / function getStockPrice(ticker) {  const apiKey = getApiKey();  if (!apiKey) {    Logger.log(\"API Key not found.",
    "section": 24
  },
  {
    "id": "doc-179",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Please run storeApiKey() first.\");    return null;  }   const url = https://financialmodelingprep.com/api/v3/quote-short/${ticker}?apikey=${apiKey};   try {    const response = UrlFetchApp.fetch(url, { muteHttpExceptions: true });    const responseCode = response.getResponseCode();    const content = response.getContentText();    if (responseCode === 200) {      const data = JSON.parse(content);      if (data && data.length > 0) {        return { price: data[0].price, volume: data[0].volume };      }    }    Logger.log(API Error: Response code ${responseCode}.",
    "section": 25
  },
  {
    "id": "doc-180",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Content: ${content});    return null;  } catch (e) {    Logger.log(Failed to fetch stock price: ${e});    return null;  } } Dialog.html (Custom UI) <!DOCTYPE html> <html>  <head>    <base target=\"_top\">    <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\">    <style>      body { padding: 20px; font-family: sans-serif; }      .loader {        border: 4px solid f3f3f3;        border-radius: 50%;        border-top: 4px solid 3498db;        width: 20px;        height: 20px;        animation: spin 2s linear infinite;        display: inline-block;      }      @keyframes spin {        0% { transform: rotate(0deg); }        100% { transform: rotate(360deg); }      }      status { margin-top: 15px; font-weight: bold; }    </style>  </head>  <body>    <h4>Place Trade Order</h4>    <p>Place a buy or sell order for <strong>AAPL</strong>.</p>    <form id=\"tradeForm\">      <div class=\"form-group\">        <label for=\"tradeDirection\">Action</label>        <select class=\"form-control\" id=\"tradeDirection\" name=\"tradeDirection\">          <option value=\"Buy\">Buy</option>          <option value=\"Sell\">Sell</option>        </select>      </div>      <div class=\"form-group\">        <label for=\"quantity\">Quantity (Number of Shares)</label>        <input type=\"number\" class=\"form-control\" id=\"quantity\" name=\"quantity\" required>      </div>      <div class=\"form-group\">        <label for=\"price\">Price (USD)</label>        <div class=\"input-group\">          <input type=\"number\" step=\"0.01\" class=\"form-control\" id=\"price\" name=\"price\" required>          <div class=\"input-group-append\">            <button class=\"btn btn-outline-secondary\" type=\"button\" id=\"fetchPriceBtn\">Get Live Price</button>          </div>        </div>        <small id=\"priceLoader\" style=\"display:none;\">Fetching...",
    "section": 26
  },
  {
    "id": "doc-181",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "<div class=\"loader\"></div></small>      </div>      <button type=\"submit\" class=\"btn btn-primary btn-block\" id=\"submitBtn\">Place Order</button>    </form>    <div id=\"status\" class=\"alert\" role=\"alert\" style=\"display:none;\"></div>    <script>      // Fetch live price when button is clicked      document.getElementById(\"fetchPriceBtn\").addEventListener(\"click\", () => {        document.getElementById(\"priceLoader\").style.display = \"block\";        google.script.run          .withSuccessHandler(priceData => {            document.getElementById(\"priceLoader\").style.display = \"none\";            if (priceData) {              document.getElementById(\"price\").value = priceData.price.toFixed(2);            } else {              alert(\"Could not fetch live price.\");            }          })          .getLiveStockPrice();      });      // Handle form submission      document.getElementById(\"tradeForm\").addEventListener(\"submit\", function(e) {        e.preventDefault();        document.getElementById(\"submitBtn\").disabled = true;        document.getElementById(\"status\").style.display = \"block\";        document.getElementById(\"status\").className = \"alert alert-info\";        document.getElementById(\"status\").innerText = \"Placing order...\";        google.script.run          .withSuccessHandler(response => {            document.getElementById(\"status\").innerText = response;            if (response.includes(\"Success\")) {              document.getElementById(\"status\").className = \"alert alert-success\";              setTimeout(google.script.host.close, 3000); // Close dialog on success            } else {              document.getElementById(\"status\").className = \"alert alert-danger\";              document.getElementById(\"submitBtn\").disabled = false;            }          })          .placeTradeOrder(this);      });    </script>  </body> </html> Step 3: How to Set It All Up Update Config.gs: Fill in all your personal details in the Config.gs file.",
    "section": 27
  },
  {
    "id": "doc-182",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Store Your API Key: In Secrets.gs, paste your API key from Financial Modeling Prep. Then, from the script editor, select the storeApiKey function from the dropdown menu and click Run. You only need to do this once. Set Up Triggers: In the script editor, go to the Triggers tab (clock icon). Click + Add Trigger. Choose function to run: processAllEmails. Select event source: Time-driven. Select type: Minutes timer. Select interval: Every 10 minutes. Click Save. You will be asked to authorize the script. Prepare Your Trading Sheet: In the Trading tab of your spreadsheet, set up two columns: Column A: Month (e.g., \"August 2025\", \"September 2025\") Column B: Payslip Received (Format this column as checkboxes via Insert > Checkbox) Prepare Your Transactions Sheet: In the Transactions tab, create these headers:Date, Ticker, Type, Quantity, Price, Status Reload the Spreadsheet: Refresh your Google Sheet. You should now see a new \"Stock Trading\" menu at the top.",
    "section": 28
  },
  {
    "id": "doc-183",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Step 4: Testing Scenario 1: The Automated Payslip Workflow This demonstrates the script's ability to react to incoming emails, save attachments, perform API calls, and update you. How to Test / Stimulate It: The script is looking for a new, unread email that matches the criteria in your Config.gs file. If you receive your payslip in outlook like I do and are wondering how to set this up, create an outlook rule to always forward the email with your payslip to your personal gmail account. To test this, you need to simulate receiving a payslip: Important: For testing, temporarily change the PAYSLIP_SENDER in your Config.gs file to your own email address (e.g., const PAYSLIP_SENDER = \"your_email@example.com\";). From that same email address, send a new email to yourself. Subject Line: The subject must contain the phrase \"Your Monthly Payslip\". Attachment: Attach any PDF file to the email. Send the email. Once it arrives in your inbox, make sure it is marked as unread.",
    "section": 29
  },
  {
    "id": "doc-184",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "What the Script Does (The Demo): Once you've sent the email, you can either wait for the 10-minute trigger to fire or manually run the processAllEmails function from the script editor to see the results immediately. The script will find your unread \"payslip\" email. It will save the PDF attachment to the Google Drive folder you specified. It will make an API call to get the latest price for AAPL. It will calculate how many shares you can buy with 20% of your $10,000 salary. It will find the current month in your \"Trading\" sheet and check the box in the \"Payslip Received\" column. Finally, it will mark the payslip email as read. What You Receive / See: You'll get a new email with the subject \"‚úÖ Your Payslip Has Been Processed\" containing the stock price information. The PDF attachment will appear in your designated Google Drive folder. The checkbox for the current month in your \"Trading\" sheet will be checked.",
    "section": 30
  },
  {
    "id": "doc-185",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Payslip saving email Scenario 2: The Manual Stock Trading Workflow This demonstrates the custom user interface you built into the spreadsheet for placing trades. How to Test / Stimulate It: This workflow is initiated manually by you. Open your \"My Stock Portfolio\" Google Sheet. A new menu item named \"Stock Trading\" should appear at the top. Google sheet menu with the custom menu item Stock trading Click on Stock Trading > üìà Place New Trade Order. Stock Trading modal in google sheet What the Script Does (The Demo): A custom dialog box titled \"Place a Stock Trade Order\" will appear. You can click the \"Get Live Price\" button to have the script fetch and populate the current AAPL stock price. Fill out the form: choose Buy or Sell, and enter a Quantity. Click the \"Place Order\" button. The script will compose an email with all the trade details and send it to your broker's email address. It will BCC you on that email.",
    "section": 31
  },
  {
    "id": "doc-186",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "It will add a new row to your \"Transactions\" sheet to log that the order has been placed. What You Receive / See: A confirmation message will appear in the dialog box. Stock Trading modal in google sheet showing order has been placed You will receive a BCC'd copy of the order email in your inbox. Email placing the buy order A new row will be added to the \"Transactions\" sheet. Google sheet with updated details on the buy order Scenario 3: The Automated Contract Note Workflow This demonstrates how the script handles incoming trade confirmations. How to Test / Stimulate It: Similar to the payslip, you need to simulate receiving a contract note: Change the CONTRACT_NOTE_SENDER in your Config.gs file to your own email address for the test. Send a new email to yourself. Subject Line: The subject must contain the phrase \"Contract Note\". Attachment: Attach any PDF file. Send the email and ensure it's unread.",
    "section": 32
  },
  {
    "id": "doc-187",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "What the Script Does (The Demo): When the processAllEmails function runs, it will: Find your unread \"Contract Note\" email. Save the PDF attachment to your Google Drive folder. Because the script cannot read the PDF's contents, it will send you a notification email. It will mark the contract note email as read. What You Receive / See: You'll get a new email with the subject \"üìù Action Required: Log Your Recent Trade\", prompting you to manually update your \"Transactions\" sheet with the final details from the PDF. Contract note saving confirmation email The contract note PDF will be saved in your Google Drive folder. Step 5: GitHub Integration with clasp Feel free to skip this step if you are not technical. clasp is a command-line tool that lets you manage your Apps Script projects locally and push/pull them to/from GitHub. Install Node.js: If you don't have it, install Node.js from nodejs.org.",
    "section": 33
  },
  {
    "id": "doc-188",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Install clasp: Open your terminal or command prompt and run: npm install -g @google/clasp Login to Google: clasp login This will open a browser window for you to authorize clasp. Enable the Apps Script API: Go to the Apps Script API page and turn it on. Clone Your Project: In your Apps Script editor, go to Project Settings (gear icon) and copy the Script ID. In your terminal, navigate to your desired folder (e.g., cd Documents/GitHub) and run: clasp clone \"YOUR_SCRIPT_ID_HERE\" This will download all your .gs and .html files into a new folder. Work with GitHub: You can now treat this folder as a standard Git repository. cd your-project-name git init git add . git commit -m \"Initial commit of finance automation script\"  Add your remote and push to GitHub Pushing Changes Back to Apps Script: After making changes locally, just run clasp push. This setup provides a powerful, automated workflow for managing your finances, all orchestrated from within your Google Workspace.",
    "section": 34
  },
  {
    "id": "doc-189",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "Step 6 (Bonus): Visualization using Looker Studio You can also take your stock tracking to the next level by building a portfolio dashboard using Looker Studio, with the ‚ÄúMy Stock Portfolio‚Äù Google Sheet as the data source. This dashboard can display key metrics such as total value bought and sold over time, monthly performance, and even stock-specific trends. By connecting your sheet directly to Looker Studio and visualizing your data through bar charts, line graphs, or scorecards, you gain a real-time, interactive view of your portfolio‚Äôs evolution. It‚Äôs a great way to stay informed and make data-driven investment decisions. The Apps script, however, cannot automate the making of the charts and so you will need to add, format and align them manually. Investment portfolio dashboard on looker studio Conclusion And there you have it! A simple project based introduction to Google Apps Script.",
    "section": 35
  },
  {
    "id": "doc-190",
    "source": "google_apps_script_article",
    "title": "Mastering Google Apps Script: Free Automation in Google Workspace",
    "content": "We‚Äôve covered how to set up triggers, interact with Gmail, parse attachments, and store secrets securely‚Äîwhile also touching on important limitations. The biggest takeaway? You don‚Äôt need external tools to start automating tasks right inside your Google Workspace‚ÄîApps Script gives you a surprisingly powerful head start. I am curious what you choose to automate first. Let me know in the comments. Also, let me know if I should deploy the automated stock ordering custom menu as a google sheet add on. It‚Äôs definitely a time saver for me. I will get to n8n and Zapier in due time but for now, Google Apps Script serves me well. Till next time, have fun.",
    "section": 36
  },
  {
    "id": "doc-191",
    "source": "goal_setting_article",
    "title": "Goal Setting & New Year Resolutions: A lesson in Systems Thinking",
    "content": "In this article, I discuss how to set new goals for the year ‚ÄúWe do not rise to the level of our goals, we fall to the level of our systems.‚Äù That explains a lot! It explains why we are back to the warmth of our beds after that first week of new year‚Äôs, contrary to our resolutions. It explains why you‚Äôre waking up in an unknown location on the second Sunday of the year after promising yourself this is the year you‚Äôd go clean. Why do we do that to ourselves? Why do we set ambitious goals only to fall down face up and start admiring the stars we were once reaching for? Is life that meaningless or is luck that strong as to determine who succeeds and who doesn‚Äôt? What is life‚Äôs purpose anyway? Now, now. Before you go down that rabbit hole of negative self degrading thoughts, here are a few things you should consider. First, was that goal really yours? Was it really you? Do you want a lean body for your long term health or just to flex with the lads and ladies?",
    "section": 0
  },
  {
    "id": "doc-192",
    "source": "goal_setting_article",
    "title": "Goal Setting & New Year Resolutions: A lesson in Systems Thinking",
    "content": "Second, did you set up the systems necessary for you to achieve your goals? Did you try sleeping early, cutting off friends that drink or even cooking to avoid the allure of junk food? Remember, we do not rise to the level of our goals, we fall to the level of our systems. Become a member That being said, here are a couple of techniques that can help with consistency in pursuing your goals: The 2-day rule: with whatever habit you‚Äôre trying to build, never allow yourself to skip more than one day in a row. Skipping one day won‚Äôt hurt your habit building, as long as you don‚Äôt skip the next one. The 30-for-30 approach: do 30 minutes of the thing you are trying to improve at for 30 days in a row. 30 minutes per day is short enough that you can mentally take it on. 30 days of 30 minutes per day is 900 total minutes of accumulated effort. This will yield surprisingly significant results.",
    "section": 1
  },
  {
    "id": "doc-193",
    "source": "goal_setting_article",
    "title": "Goal Setting & New Year Resolutions: A lesson in Systems Thinking",
    "content": "There‚Äôs almost nothing in the world that you won‚Äôt improve at if you spend 900 minutes of focused, dedicated effort on it. Minimum Viable Progress is the notion that you should not skip a day, but anything above zero counts. Remember the value of compound interest! Habit Stacking ‚Äî James Clear famously pointed out that we execute on Daily Systems most effectively when they are fixed to a time or action that makes them easy to structure and regiment. We call this a hook, a kind of trigger to get into the arena. Like vowing to do push ups every time you brush your teeth. I have also included a couple of wise words from a friend below that might help you stick to (or not) your resolutions this year: Don‚Äôt set them in stone. Be willing to pivot and change based on your circumstances. We grow everyday and the world changes every second. It is foolish to not be willing to change your mind when presented with new information, even if it is about your own goals.",
    "section": 2
  },
  {
    "id": "doc-194",
    "source": "goal_setting_article",
    "title": "Goal Setting & New Year Resolutions: A lesson in Systems Thinking",
    "content": "For a practical example, you could try saving your resolutions in an editable place such as in a notes app on your phone rather than manually writing them in a diary. This simple action could shift the way you view them entirely. Break them down into smaller, more direct actions. Sometimes we fail to achieve our goals because we stated them as this big audacious goal and never bothered to think deeper as to what the road there might look like. After you think of a goal, think of why you want it first then think of a way to achieve it. The easiest way that I have found is to look at the components of what my goals is made up of and see what I need to achieve these Be accountable. Once broken down, you should develop a mechanism to measure progress for each of your goals. You should set deadlines for both the main goal and its broken down components then set dates to check in. For finances, I simply use google sheet and log in personal financial statements at the end of the month.",
    "section": 3
  },
  {
    "id": "doc-195",
    "source": "goal_setting_article",
    "title": "Goal Setting & New Year Resolutions: A lesson in Systems Thinking",
    "content": "For the rest, I set calendar tasks on my Google calendar.",
    "section": 4
  },
  {
    "id": "doc-196",
    "source": "frisque_project",
    "title": "Frisque Project",
    "content": "AI-Powered Due Diligence Platform Frisque is an agentic AI system designed to revolutionize the venture capital due diligence process. It automates and augments critical research, culminating in comprehensive investment memos and actionable insights. Frisque comes from the French words Faux Risque meaning false risk, which is what we aim to eliminate in the due diligence process. Read more about Frisque in our blog post. The Problem Venture Capital (VC) firms dedicate a significant amount of time and resources to conducting due diligence on potential startup investments. This process is critical for assessing a startup's viability and growth potential, but it is incredibly time-intensive. Given that a fund's returns often come from a small percentage of its investments, streamlining this process is crucial for success.",
    "section": 0
  },
  {
    "id": "doc-197",
    "source": "frisque_project",
    "title": "Frisque Project",
    "content": "Our Solution  video1279770196.mp4  Frisque is an AI-powered platform that significantly reduces the time and effort VCs spend on initial due diligence while improving the depth and breadth of insights. The platform uses a system of specialized AI agents that can: Perform targeted research on a company based on inputs like pitch decks, financial documents, and founder profiles. Analyze technology stacks, market trends, legal documents, and even social media sentiment. Synthesize all findings into a structured investment memo and a summary dashboard. Provide real-time updates and notifications on scan progress and completion. Find an example of a completed report here. Ideally, the project's gaol is to generate investment memos such as this one about Paystack from Dream VC. Tech Stack Frisque is built with a modern, scalable technology stack designed for asynchronous workloads and intelligent processing.",
    "section": 1
  },
  {
    "id": "doc-198",
    "source": "frisque_project",
    "title": "Frisque Project",
    "content": "gallery (3) Category\tTechnology Backend\tDjango AI Agents\tGoogle Agent Development Kit (ADK) Database\tPostgreSQL Task Queue\tCelery Message Broker\tRabbitMQ Real-time Comms\tDjango Channels Containerization\tDocker, Docker Compose Cloud Storage\tGoogle Cloud Storage (GCS) Infrastructure\tTerraform Cloud Platform\tGoogle Cloud Platform (GCP) Getting Started Follow these instructions to get the Frisque development environment up and running on your local machine. Prerequisites You must have Docker and Docker Compose installed on your system. Install Docker Engine Install Docker Compose Installation & Setup Clone the Repository git clone https://github.com/Virgo-Alpha/frisque.git cd frisque Run the Application This single command will build the Docker images, create all the necessary services (web, database, broker, worker), and start the entire application stack. docker-compose up --build You will see logs from all the services in your terminal.",
    "section": 2
  },
  {
    "id": "doc-199",
    "source": "frisque_project",
    "title": "Frisque Project",
    "content": "Wait for the database to initialize and the web server to start. Access the Application Once the services are running, open your web browser and navigate to: http://localhost:8000 You should see the default Django \"Congratulations!\" page. This confirms that the entire stack is working correctly! Agents Configuration For agents instructions, please check out the file agents/instructions.md Development Workflow To ensure a consistent development environment, all commands should be run inside the web container using docker-compose exec. How to Run Management Commands Here are some examples of common manage.py commands.",
    "section": 3
  },
  {
    "id": "doc-200",
    "source": "frisque_project",
    "title": "Frisque Project",
    "content": "Apply Database Migrations docker-compose exec web python manage.py migrate Create a Superuser docker-compose exec web python manage.py createsuperuser Open a Django Shell docker-compose exec web python manage.py shell Enter a Bash Shell Inside the Container If you want an interactive shell inside the web container to look around: docker-compose exec web bash Testing the Application Runtests while inside the application Inside the Container While inside the web container,you can run the command pytest Runtests script While outside the web container, you can run the command: ./run_tests.sh You can also run the full command below: docker-compose exec web pytest Stopping the Environment To stop all running services, press Ctrl+C in the terminal where docker-compose up is running. To stop the services and remove the containers (useful for a clean restart), run: docker-compose down",
    "section": 4
  },
  {
    "id": "doc-201",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "In this article, I discuss how to use AI agents for Due Diligence TLDR; Frisque uses django, ai agents, celery and rabbitmq to automate due diligence on startups. It takes text, pitch decks, financial spreadsheets and even video as input and outputs an investment memo. It was built for the Agent Development Kit Hackathon with Google Cloud on DevPost. Table of Contents Introduction The Problem Our Solution Architecture and Stack Agentic AI ‚Äì Orchestration in Action Challenges and Revelations Key Learnings Future Plans DYI Conclusion Introduction In the dynamic world of Venture Capital (VC), conducting due diligence on potential startup investments is a critical yet cumbersome bottleneck. This process became painfully familiar through firsthand experience interning in VC, revealing the sheer volume of information, meticulous cross-referencing, and relentless pressure to identify both opportunity and risk within tight timeframes.",
    "section": 0
  },
  {
    "id": "doc-202",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "Frisque was born directly from this experience, aiming to automate and augment these very tasks and challenges. VC firms inherently dedicate significant time and resources to due diligence, as it is crucial for assessing a startup's viability and growth potential. Given that a fund's returns often originate from a small percentage of its investments, streamlining this process is absolutely crucial for success. Frisque aims to go beyond mere efficiency, enabling VCs to make smarter, faster, and more informed investment decisions. The Problem Having both spent time interning in the dynamic world of Venture Capital, we quickly became painfully familiar with a critical, yet cumbersome, bottleneck: due diligence. The sheer volume of information, the meticulous cross-referencing, and the relentless pressure to identify both opportunity and risk within a tight timeframe becomes incredibly apparent in such roles.",
    "section": 1
  },
  {
    "id": "doc-203",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "Frisque was born out of this firsthand experience, directly addressing the very tasks and challenges we faced, aiming to automate and augment the work we were doing. Venture Capital (VC) firms dedicate a significant amount of time and resources to conducting due diligence on potential startup investments. This process is critical for assessing a startup's viability and growth potential, but it is incredibly time-intensive. Given that a fund's returns often come from a small percentage of its investments (pareto principle), streamlining this process is absolutely crucial for success. It's about more than just efficiency; it's about making smarter, faster, and more informed investment decisions. Our Solution Frisque, aptly named from the French \"Faux Risque\" (false risk), is an AI-powered platform built to revolutionize the VC due diligence process by significantly reducing the time and effort VCs spend on initial assessments while dramatically improving the depth and breadth of insights.",
    "section": 2
  },
  {
    "id": "doc-204",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "At its core, Frisque is a web-based platform built on Django, uniquely leveraging Google's open-source Agent Development Kit (ADK) to create a sophisticated multi-agent AI system. This approach means Frisque isn't just one large AI, but a coordinated team of specialized AI \"agents\" working together, mirroring a human due diligence team. Here's how Frisque's agentic system streamlines the process: Comprehensive Input Collection: Analysts can initiate \"scans\" on target companies. They provide a wide array of inputs, including company names, website URLs, business plans, pitch decks, lean canvases, founder profiles, social media links, and even financial documents (like spreadsheets) and government registration documents. The system also allows users to select which specific types of scans they want to perform, such as Tech, Legal, or Financial analysis. Intelligent Agent Orchestration: Once a scan is initiated, a Master Bot, or Orchestrator Agent, takes charge.",
    "section": 3
  },
  {
    "id": "doc-205",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "It intelligently delegates specific sub-tasks to a team of specialized worker agents. This multi-agent by design approach is a core strength of Google's ADK, enabling complex coordination and task delegation. Specialized Agents in Action: The Tech Bot assesses a startup's technology stack, scalability, and potentially intellectual property. The Legal Bot sifts through provided legal documents to identify basic red flags or critical phrases in contracts and registrations. The Market Research Bot gathers crucial data on market size, industry trends, and competitor landscapes. The Social Media Sentiment Bot analyzes public sentiment around the company and its founders from various social media profiles. The Financial Bot performs basic analysis of financial statements, capable of detecting anomalies or inconsistencies in the data. These agents utilize Large Language Models (LLMs), Natural Language Processing (NLP) tools, and can integrate with external APIs or custom tools as needed.",
    "section": 4
  },
  {
    "id": "doc-206",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "A key learning was that ADK's inherent agency allows bots to choose their own tools, meaning we didn't need to explicitly direct them, which streamlined our development. We also adopted a \"Pipeline / Assembly line architecture\" or \"Dumb Worker, Smart Master\" pattern, where complex logic is handled by the master agent and a dedicated worker agent formats responses, effectively solving issues like prompt leakage and hallucination we initially encountered. This approach reinforces the benefits of a microservices design over a monolithic one for scalability and isolation. Comprehensive Output Generation: The system synthesizes the findings from all the specialized agents into a comprehensive and actionable suite of outputs. This includes a structured Investment Memo (a go/no-go document), a summary Dashboard with key findings and scores, and if financial data is provided, basic financial projections.",
    "section": 5
  },
  {
    "id": "doc-207",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "It also provides a valuable list of assumptions made, key questions to ask the startup, and all cited sources. Real-time Updates and Notifications: To keep analysts informed throughout the process, Frisque provides real-time updates on scan progress directly on the results page using Django Channels (WebSockets). Users also receive both in-app notifications and email notifications once a scan is complete. By leveraging Google's ADK and a modern stack including Django, PostgreSQL, Celery, RabbitMQ, and Google Cloud services like Google Cloud Storage and Vertex AI, Frisque is designed to be modular, scalable, and deployment-ready. This project is also a contribution to the Agent Development Kit Hackathon with Google Cloud, highlighting our use of Google Cloud technologies and the open-source ADK Architecture and Stack Frisque's architecture and technology stack are designed for modularity, scalability, and efficient AI-powered due diligence.",
    "section": 6
  },
  {
    "id": "doc-208",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "It aims to support asynchronous workloads and intelligent processing. Here's a breakdown of the key components: Our Architecture diagram Backend Framework (Django): Frisque is a web-based platform built on Django. Django provides a self-contained framework for the application's backend. Its ORM (Object-Relational Mapper) simplifies database interactions by managing models for users, companies, and scan jobs. This structure allows for quick integration with other technologies, such as Docker, for consistent development and deployment. AI Agents (Google Agent Development Kit - ADK): The platform leverages Google's Agent Development Kit (ADK) to create its multi-agent AI system. ADK is an open-source, code-first framework designed for building and deploying sophisticated AI agents. It is \"Multi-Agent by Design,\" enabling complex coordination and delegation of tasks within a team of agents. The Agent Starter Pack provides an easier way to quickly set up, customize, and deploy agents.",
    "section": 7
  },
  {
    "id": "doc-209",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This approach supports modular and scalable development, breaking down intricate problems into manageable sub-tasks handled by specialized agents. Asynchronous Task Queue (Celery) and Message Broker (RabbitMQ): Frisque uses Celery for background task processing. When a scan is initiated, a Celery task is dispatched to handle it asynchronously. This allows for scheduling and managing complex, time-consuming operations outside of the main web request flow. RabbitMQ (or Redis) serves as the message broker for Celery, facilitating communication between the application and the worker processes. Containerization (Docker and Docker Compose): Docker is used for containerization, ensuring that the application and all its dependencies are packaged into isolated units. Docker Compose simplifies the management of multi-container Docker applications for local development.",
    "section": 8
  },
  {
    "id": "doc-210",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This setup provides reproducibility across different environments, making it easy to get the development environment up and running consistently. All development commands are designed to be run inside the web container for a consistent workflow. Database (PostgreSQL): PostgreSQL is the chosen database for storing structured data. This includes details of target companies and scan job metadata. Object Storage (Google Cloud Storage - GCS): Google Cloud Storage (GCS) is integrated for storing unstructured data. This includes uploaded documents like pitch decks and financial spreadsheets, as well as generated reports and memos. Real-time Communication (Django Channels): Django Channels, utilizing WebSockets, enables real-time updates and notifications. This allows the scan results page to display live progress updates and provides in-app notifications upon scan completion. Infrastructure as Code (Terraform): Terraform is used for provisioning Google Cloud Platform (GCP) resources.",
    "section": 9
  },
  {
    "id": "doc-211",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This ensures that the cloud infrastructure is managed consistently and repeatably. Cloud Platform (Google Cloud Platform - GCP): The entire system is designed to leverage Google Cloud Platform services for deployment and scalability. This includes potential use of Vertex AI for Agent Engine and LLM hosting, Cloud Run for serverless agent deployment, and Cloud SQL for managed PostgreSQL. Frisque is also a contribution to the Agent Development Kit Hackathon with Google Cloud. This comprehensive stack allows Frisque to efficiently process complex due diligence tasks, manage data, and provide real-time insights to users. Agentic AI ‚Äì Orchestration in action Frisque's power lies in its agentic AI system, meticulously designed to replicate and enhance the collaborative nature of a human due diligence team.",
    "section": 10
  },
  {
    "id": "doc-212",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This sophisticated structure is made possible by leveraging Google's open-source Agent Development Kit (ADK), a framework built to develop, evaluate, and deploy sophisticated AI agents and multi-agent systems. ADK is inherently \"Multi-Agent by Design,\" which means it excels at enabling complex coordination and delegation of tasks within a hierarchy or team of agents. When an analyst initiates a \"scan\" on a target company within Frisque, a comprehensive process of intelligent orchestration begins. The Orchestrator Agent (Master Bot): At the core of this system is a Master Bot, acting as the Orchestrator Agent. Its primary role is to receive the initial scan request and intelligently delegate specific sub-tasks to a team of specialized worker agents. This delegation is crucial for breaking down intricate due diligence problems into manageable parts. Specialized Agents in a Pipeline: Frisque employs a diverse set of specialized worker agents, each with a distinct focus.",
    "section": 11
  },
  {
    "id": "doc-213",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "These include the Tech Bot, Legal Bot, Market Research Bot, Social Media Sentiment Bot, and Financial Bot. A key learning during development was the adoption of a \"Pipeline / Assembly line architecture\" or \"Dumb Worker, Smart Master\" pattern. In this architecture, the complex logic and coordination are handled by the master agent, while a dedicated worker agent is specifically responsible for formatting the responses. This separation of concerns proved vital in solving initial challenges like prompt leakage and hallucination, reinforcing the benefits of a microservices design for scalability and isolation. Intelligent Tool Selection and Inquiry: A significant aspect of the agents' intelligence lies in their inherent agency. The ADK's design allows bots to choose their own tools without explicit direction from the developer.",
    "section": 12
  },
  {
    "id": "doc-214",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This means agents can intelligently decide which resources to use for their tasks, whether it's utilizing Large Language Models (LLMs), Natural Language Processing (NLP) tools, integrating with external APIs, or even using other agents as tools. This self-directed tool selection, and the ability to inquire further after obtaining initial results, streamlines the development process and enhances the depth of research. For instance, the Market Research Bot might autonomously decide to use web search tools to gather market size data or a sentiment API to analyze social media. Synthesis and Output: As each specialized agent completes its analysis, it gathers, processes, and analyzes information based on its function and the provided inputs. The Orchestrator then synthesizes these findings from all the specialized agents into comprehensive and actionable outputs.",
    "section": 13
  },
  {
    "id": "doc-215",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This culminates in a structured Investment Memo, a summary Dashboard with key findings and scores, and potentially basic financial projections. The system also provides a valuable list of assumptions made, key questions to ask the startup, and all cited sources. This orchestrative, multi-agent approach allows Frisque to efficiently process complex due diligence tasks, manage vast amounts of data, and provide real-time, insightful analyses to VC firms. Challenges and Revelations One of the first and most critical challenges was agent hallucination. Agents were generating incorrect or fabricated information. Closely related was prompt leakage. This occurred due to difficulties in system integration. Initially, the master agent was responsible for both task delegation and response formatting. This design inadvertently led to the agents' tendency to hallucinate and expose prompts in unintended ways.",
    "section": 14
  },
  {
    "id": "doc-216",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "We fixed the above by creating a new agent to format the final output before it is returned by the master agent. Even in this we had to be explicit in terms of the fields we required in the output in both the master agent and the formatting agent. Otherwise we would get errors of missing fields. Another inherent challenge in building multi-agent systems, particularly with complex interactions, involves designing and debugging their orchestration. Ensuring the consistency and accuracy of Large Language Model (LLM) calls across various agent tasks, while also managing their associated costs, proved challenging. The overall quality and availability of input data for target companies also directly impacted the effectiveness of the agents.",
    "section": 15
  },
  {
    "id": "doc-217",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "Finally, effectively quantifying and training agents to achieve the depth of insight expected by experienced Venture Capitalists was a significant undertaking Key Learnings Embracing a Pipeline/Microservices Architecture for Agentic Systems: Our development journey revealed that complex multi-agent systems, especially those dealing with detailed outputs, can suffer from agent hallucination and prompt leakage. This was a profound revelation, teaching us the crucial importance of a pipeline or assembly line architecture. By introducing a new, dedicated agent solely for formatting the final output, we achieved a clearer separation of concerns. This \"Dumb Worker, Smart Master\" pattern proved far more effective for managing complex logic, allowing agents to specialize.",
    "section": 16
  },
  {
    "id": "doc-218",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This experience solidified our conviction that a microservices approach is generally superior to a monolith for deployment, offering benefits like enhanced scalability, technology flexibility, reduced single points of failure, and faster deployments. We saw how individual management of agents became possible, allowing us to explore other technologies if needed. Leveraging Google Agent Development Kit (ADK) for Multi-Agent Orchestration: ADK, as an open-source, code-first framework, became the backbone of Frisque, empowering us to build, evaluate, and deploy sophisticated AI agents. Its \"Multi-Agent by Design\" principle was instrumental for enabling the complex coordination and task delegation within our system. We learned to fully utilize ADK's flexible orchestration capabilities, including both workflow agents for predictable pipelines (like SequentialAgent, ParallelAgent, and LoopAgent) and LLM-Driven Dynamic Routing for adaptive behaviors.",
    "section": 17
  },
  {
    "id": "doc-219",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "The integrated developer experience, complete with a command-line interface (CLI) and a visual Web UI, significantly aided our development, allowing us to run agents, inspect execution steps, and debug interactions in real-time. The built-in observability and debugging tools, which log agent decisions, tool usage, and trace delegation paths, were invaluable for understanding and refining our agents' behavior. Precision in Prompting and Agent Tooling: A critical insight gained was that agents within ADK do not need explicit direction on which tools to use. Simply providing the prompt is sufficient, as ADK already exposes the available tools to the agent, and the agent's selection of tools is part of its inherent agency. However, we also learned the critical importance of being explicit in terms of required output fields (both in the master agent's instructions and the formatting agent's directives) to prevent errors and ensure consistent data.",
    "section": 18
  },
  {
    "id": "doc-220",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "The Paramount Importance of Data Quality: The effectiveness of our AI agents in due diligence directly correlated with the quality and availability of input data. This highlighted the absolute necessity of establishing robust data management processes and infrastructure for input collection and storage. We chose Google Cloud Storage (GCS) for securely housing uploaded documents and generated reports, with PostgreSQL maintaining structured data and references. Balancing AI Capabilities with Human Expectation: Quantifying and training agents to achieve the depth of insight expected by experienced Venture Capitalists proved a significant undertaking. Our learning here was the value of an iterative and specialized approach. By developing distinct agents for different domains‚Äîsuch as Tech Bot, Legal Bot, Market Research Bot, Social Media Sentiment Bot, and Financial Bot‚Äîwe could address specific analytical tasks.",
    "section": 19
  },
  {
    "id": "doc-221",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "This modularity, combined with a basic scoring mechanism, is our path to incrementally achieving sophisticated VC-level insights, understanding that AI augments, rather than replaces, human judgment in complex financial decisions Future Plans Collapse the results into a downloadable pdf document Add more ai agents Add email notification for when a scan is done Allow selective scans, e.g., security, sentiment analysis, social media, legal, etc Create a scan history and dashboard pages Integrate MCP, A2A and other integrations Scoring of startups for investments purposes DYI Please find the code here Follow the results in the README to reproduce the project. No api keys or envs needed. Conclusion The comprehensive technology stack employed by Frisque allows it to efficiently process complex due diligence tasks, manage data, and provide real-time insights to users. The process of due diligence mirrors that of fundraising.",
    "section": 20
  },
  {
    "id": "doc-222",
    "source": "frisque_article",
    "title": "Frisque ‚Äì Using AI agents for Due Diligence",
    "content": "Marc Andressen once compared fundraising rounds to removing the layers of an onion and we hope Frisque can help make this less tear-worthy for VCs.",
    "section": 21
  },
  {
    "id": "doc-223",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "Finamu is a groundbreaking blockchain-powered platform designed to democratize film investment. By integrating Ethereum smart contracts with the MERN stack, Finamu offers a transparent and secure avenue for individuals to invest in film productions. This platform aims to disrupt traditional film financing models, making it easier for both seasoned investors and everyday individuals to participate in film projects. Additionally, Finamu seeks to stimulate job creation and support the growth of the creator economy in the film industry. Table of Contents Features Technologies Used Installation Usage API Documentation Contributing License Contact Features Film Investment: Invest in film projects and own a stake in film productions. Blockchain Integration: Use Ethereum smart contracts for secure and transparent investment transactions. Progressive Web Application: Accessible on both desktop and mobile devices. User Authentication: Register and log in to access investment opportunities.",
    "section": 0
  },
  {
    "id": "doc-224",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "Project Management: Filmmakers can post new film projects and manage their funding. Technologies Used Frontend: React: A JavaScript library for building user interfaces. React Router v6: For routing and navigation in the application. Axios: For making HTTP requests to the backend. Material-UI: A React UI framework for building modern web applications. Backend: Node.js: JavaScript runtime for building server-side applications. Express.js: A minimalist web framework for Node.js. MongoDB: NoSQL database for flexible data storage. Mongoose: An ODM library for MongoDB. Ethereum: Blockchain platform for deploying smart contracts. Web3.js: Library for interacting with the Ethereum blockchain. DevOps: Docker: For containerization and deployment. GitHub: Version control and collaboration. AWS: Cloud services for hosting and scaling the application. Installation Prerequisites Ensure you have Node.js (version 14 or higher) and npm installed on your machine.",
    "section": 1
  },
  {
    "id": "doc-225",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "Install libraries Got to the backend folder then: Install Truffle and Ganache: npm install -g truffle npm install -g ganache-cli Initialize a Truffle Project: truffle init Note: Use npm 18 or higher installed via nvm so as to access truffle.",
    "section": 2
  },
  {
    "id": "doc-226",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "Install more depenndencies: npm init -y npm install dotenv web3 jwt-decode multer cookie-parser --save-dev jest supertest mongodb-memory-server Frontend Setup Clone the repository: git clone https://github.com/yourusername/finamu.git cd finamu cd frontend Install dependencies: npm install 2.5 Install the required libraries: npm install canvas-confetti --save disqus-react recharts react-router-dom react-plotly.js plotly.js Start the development server: npm start Backend Setup Navigate to the backend directory: cd ../backend Install dependencies: npm install Install the database wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc |  gpg --dearmor | sudo tee /usr/share/keyrings/mongodb.gpg > /dev/null  echo \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list sudo apt update sudo apt install mongodb-org sudo apt install mongoose Create a .env file and add your MongoDB connection string: MONGO_URI=your_mongodb_connection_string Start the server: npm start Test the models npm test Usage Visit http://localhost:3000 in your web browser to access the frontend application.",
    "section": 3
  },
  {
    "id": "doc-227",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "The backend server will run on http://localhost:5000. Run the Docker Container To run both the FE and BE in a Docker container, run the command: sudo docker-compose up --build This will build the images and start the containers for the frontend and backend. Later, you can just run: sudo docker-compose up -d Compiling and Deploy the Smart Contract Start Ganache (for local testing): ganache-cli Compile the Contract: truffle compile Deploy the Contract: truffle migrate --network development API Documentation For details on the API endpoints, see the API Documentation. Contributing We welcome contributions from the community! To contribute to Finamu, please follow these steps: Fork the repository on GitHub. Create a new branch for your changes. Commit your changes and push to your branch. Submit a pull request with a description of your changes. Please ensure your code adheres to the project's style guidelines and includes appropriate tests.",
    "section": 4
  },
  {
    "id": "doc-228",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "License This project is licensed under the MIT License. See the LICENSE file for details. Contact For questions or feedback, please reach out to your-email@example.com. Key Updates: Added the Features section to highlight the key functionalities of the application. Included the Technologies Used section to detail the tech stack and libraries used. Updated the Installation instructions for both frontend and backend. Added a Usage section to guide users on how to run the application locally. Added an API Documentation link placeholder for API details. Added Contributing, License, and Contact sections for community involvement and support. Feel free to adjust the contact details, contributing guidelines, and other sections to better fit your project's needs. References Here are some references for the technologies used: React: React. (2024). React: A JavaScript Library for Building User Interfaces. Available here Node.js: Node.js. (2024). Node.js Documentation.",
    "section": 5
  },
  {
    "id": "doc-229",
    "source": "finamu_project",
    "title": "Finamu Project",
    "content": "Available here Express.js: Express. (2024). Express.js Documentation. Available here MongoDB: MongoDB. (2024). MongoDB Documentation. Available here Mongoose: Mongoose. (2024). Mongoose Documentation. Available here Ethereum: Ethereum Foundation. (2024). Ethereum: A Decentralized Platform for Digital Currencies. Available here Web3.js: Web3 Foundation. (2024). Web3.js Documentation. Available here This README.md should help you document Finamu effectively and guide users and contributors through the setup and development processes.",
    "section": 6
  },
  {
    "id": "doc-230",
    "source": "deep_fake_easily_made_article",
    "title": "Deep Fake, Easily Made",
    "content": "In this article, I discuss how to make a deep fake easily Refacer is an open source library that allows easy replacement of faces in a video. In this article, I will detail how to use it in order to do exactly that. Prerequisites A laptop/desktop device (I suppose even a tablet could work) A Github Account You will also need to prepare the files you will need in the refacing process - Source Video: the original video that you wish to clone; for this tutorial, we‚Äôll use a scene from the ‚ÄúThe Harder They Fall.‚Äù For the initial test run, I would suggest you use a short video that is a minute-long or even shorter. Target Face Images: Images of the faces you intend to manipulate. These could be screenshots from the video or other sources. For this tutorial, my target face was that of Cherokee Bill played by LaKeith Stanfield. Replacement Faces Images: Images of the faces that will replace the original faces in the target video.",
    "section": 0
  },
  {
    "id": "doc-231",
    "source": "deep_fake_easily_made_article",
    "title": "Deep Fake, Easily Made",
    "content": "Please make sure you have the consent to use these pictures / faces from the person. For this tutorial I used my own face. Refacing To create the deep fake, we will use the refacer repository available on Github here. Before using the app make sure you have read the disclaimer at the end of this project or on the GitHub repository. You can access the Refacer using a Google colab notebook but since that did not work for me, this tutorial will guide you through the longer route - running the project locally. Setup I will assume that you are working on linux or any other unix-based system (as is the Godly thing to do as a developer). Navigate to the directory you want to store the project in (I recommend creating a new folder altogether) Download this file inswapper_128.onnx and place it inside the folder you just created.",
    "section": 1
  },
  {
    "id": "doc-232",
    "source": "deep_fake_easily_made_article",
    "title": "Deep Fake, Easily Made",
    "content": "In the terminal, navigate to the folder you just created Clone the Rafacer repository from GitHub using the below command: git clone https://github.com/xaviviro/refacer.git Navigate to the refacer directory by using the command cd refacer if you are on linux/mac or chdir refacer if you are on windows Open the requirements.txt file and replace gradio==3.33.1 with gradio==3.36.1 and save & close Install packages: pip install -r requirements.txt Run the app: python app.py Finally, open your web browser and navigate to the following address: http://127.0.0.1:7680 The Refacer Interface On the specified port, you should see an interface that looks like the one below: Reface User Interface on the Web The main sections are: Original Video Upload: Upload the source video to this section. Target Faces Placement: Place the faces in the video that you want to replace; up to five faces are supported.",
    "section": 2
  },
  {
    "id": "doc-233",
    "source": "deep_fake_easily_made_article",
    "title": "Deep Fake, Easily Made",
    "content": "Replacement Faces Placement: Position the faces that will replace the corresponding faces uploaded to section (2). Output File Display: The resulting file will be displayed here. Upload the required files Upload the respective files to their designated sections and click ‚ÄúReface‚Äù (The big orange button at the bottom of the page). This action initiates a process that will ‚Äúreface‚Äù all frames in the video using the provided faces. Please note that this process may take some time. Refacer Interface with uploaded media You can check the terminal to see progress being made. I would also recommend that if you have a light-weight device, you could close all other running apps and leave the device for refacing. Accessing your video Your output/refaced video will be in the /out folder. For the full path, please check your terminal. You can also view the refaced video on the browser. Sharing Please be sure to share your refaced video like I did here and if you can, tag me!",
    "section": 3
  },
  {
    "id": "doc-234",
    "source": "deep_fake_easily_made_article",
    "title": "Deep Fake, Easily Made",
    "content": "Till next time comrades, may the force be with you!",
    "section": 4
  },
  {
    "id": "doc-235",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "In this article, I discuss Data Engineering Concepts: A project based introduction I recently finished the Data Engineering Zoomcamp by DataTalks Club. For my certification, I was required to undertake a capstone project that would culminate in a dashboard showing insights from the data I had processed in my pipeline. Instead of a step-by-step guide (which can be easily found in the project‚Äôs README), this article explores data engineering concepts from a high-level view, explaining the decisions I made and the trade-offs I considered. My Project chart Table of Contents Data Sourcing and Problem Definition Containerization Infrastructure-as-Code (IaC) Orchestration vs Automation Data Lake and Data Warehouse Analytics Engineering and Data Modeling Batch vs Streaming Exposure: Visualization and Predictions Conclusion 1. Data Sourcing and Problem Definition Before coding, I sourced the data and defined the problem.",
    "section": 0
  },
  {
    "id": "doc-236",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "I chose the LinkedIn Job Postings dataset from Kaggle due to its richness and descriptive documentation. Problem Statement: How can data from LinkedIn job posts (2023‚Äì2024) help us make informed decisions on a career path? I then went ahead to break it down into the following issues, each of which would be addressed by a chart in my dashboard: Which job titles offer the highest salaries? Which companies, industries, and skills are the most lucrative? What percentage of companies offer remote work? What are the highest salaries and average experience levels? Which countries have the most job postings? These were, of course, not MECE-compliant. (MECE - mutually exclusive and collectively exhaustive) 2. Containerization Before I could think of extracting my data, I took a high level and long term view of my project and considered aspects such as collaboration and reproducibility.",
    "section": 1
  },
  {
    "id": "doc-237",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Whereas it is true that I could easily just create the pipelines and files needed on my local machine with no packaging whatsoever, this would pose a challenge to anyone who would be looking to evaluate or reproduce my project. I, hence, decided to use docker containers to package my project for replication either locally or even in the cloud. Docker containers also have other advantages such as being lightweight, easily replicable thus allowing scalability via horizontal scaling and load balancing, increases project maintainability since Dockerfiles simplify environment management, isolation between the containers prevents dependency conflicts and it supports version control via versioned images. 3. Infrastructure as Code (IaC) I would be using GCP (Google‚Äôs cloud wing) for my data lake, data warehouse and dashboard hosting and so I needed a reliable way to interact with the cloud.",
    "section": 2
  },
  {
    "id": "doc-238",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Infrastructure-as-Code (IaC) is the practice of managing and provisioning computing infrastructure‚Äîsuch as servers, networks, databases, and other resources‚Äîthrough machine-readable configuration files rather than through manual processes. The use of IaC tools simplifies the process of cloud infrastructure management and allows for scalability, version control, testability and automation. Apart from provisioning infrastructure, IaC tools can be used for other management activities such as enabling APIs in GCP and many more. It also allows reusability of resources since it avoids creating new resources if the defined ones already exist. Terraform is the IaC tool that I used due to how simple it is. I made it modular and included the use of variables and outputs to integrate terraform into my project‚Äôs workflow. An alternative to terraform is AWS Cloudformation which is used in AWS setups. 4.",
    "section": 3
  },
  {
    "id": "doc-239",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Orchestration vs Automation A data workflow is a sequence of automated data processing steps that specifies what are the steps, inputs, outputs, and dependencies in a data processing pipeline. Data workflows are also called DAGs (Directed Acyclic Graphs). Directed means they have direction, Acyclic means there are no cycles. There may be loops but no cycles are allowed. The difference between a loop and a cycle in this case is that in a loop, we know the starting and ending point. The loop ends based on whether a certain condition is met but a cycle has none. DAGs are run using tools / engines for orchestration like Apache Airflow, Luigi, Prefect, Dagster, Kestra, etc. Smaller workflows can be run using make and or cron jobs but this is usually done locally. In software engineering and data management, an orchestrator is a tool that automates, manages, and coordinates various workflows and tasks across different services, systems, or applications.",
    "section": 4
  },
  {
    "id": "doc-240",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Because an orchestrator allows everything to run smoothly without the need for manual intervention, it is easy to confuse orchestration with automation. Whereas automation refers to the execution of individual tasks or actions without manual intervention, orchestration goes beyond automation by managing the flow of multiple interconnected tasks or processes. Orchestration defines not only what happens but also when and how things happen, ensuring that all tasks (whether automated or not) are executed in the correct order, with the right dependencies and error handling in place. While automation focuses on individual tasks, orchestration ensures all those tasks are arranged and managed within a broader, cohesive system. This matters if you need to reliably handle complex processes with many interdependent steps. Use cases for automation include automated testing after code commits, automated backups and automated email notifications.",
    "section": 5
  },
  {
    "id": "doc-241",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Use cases for orchestration include data pipeline orchestration, CI/CD pipeline orchestration and cloud infrastructure orchestration. Advantages of workflow orchestration include: Scalability Error handling and resilience Improved monitoring and control Process standardization Faster time to value since no need to reinvent the wheel What‚Äôs the Difference? Category\tAutomation\tOrchestration Scope\tSingle task execution\tCoordination of multiple tasks Focus\tEfficiency of individual actions\tDependency management, error handling Example\tAutomated backups\tCI/CD pipelines, data pipeline scheduling For my workflow orchestration tool, I chose Apache Airflow because it is the most common in the industry. I built Airflow using a docker-compose.yml file and Dockerfile which installs google sdk (a way to interact with GCP). I then created a dag that had multiple steps which include downloading, unzipping and uploading the data to the created gcs bucket.",
    "section": 6
  },
  {
    "id": "doc-242",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Tasks and steps in my Airflow DAG Why I Used Apache Airflow Industry standard for DAG orchestration Allows complex workflows Supports retries, alerts, and dependency management Easily containerized using docker-compose.yml and custom Dockerfile 5. Data Lake vs Data Warehouse Data Lake A data lake is a centralized repository that allows you to store structured, semi-structured, and unstructured data at any scale, in its raw, native format until it's needed for analysis.",
    "section": 7
  },
  {
    "id": "doc-243",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Features of a data lake: Allows ingestion of structured and unstructured data Catalogs and indexes data for analysis without data movement Stores, secures and protects data at an unlimited scale Connects data with analytics and ML tools Why do we need a data lake: Companies realized the value of data Allows for quick storage and access of data (contingent on tier if S3) It is hard to always be able to define the structure of data at the onset Data usefulness is sometimes realized later in the project lifecycle R&D on data products requires huge amounts of data The need for cheap storage of Big Data Cloud providers of data lakes include Google Cloud Storage by GCP, S3 by AWS and Azure Blob by Azure.",
    "section": 8
  },
  {
    "id": "doc-244",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Dangers in a data lake: Conversion into a data swamp (disorganized, inaccessible, and untrustworthy data lake) No versioning Incompatible schemas for same data without versioning No metadata associated Joins not possible Data Warehouse A data warehouse is a centralized, structured repository designed to store, manage, and analyze large volumes of cleaned and organized data from multiple sources to support business intelligence (BI), reporting, and decision-making. This is where we have the partitioning and clustering capabilities. Feature\tData Lake\tData Warehouse Data Type\tRaw (structured + unstructured)\tRefined (structured only) Purpose\tStorage for future use\tFast analytics & reporting Design\tSchema-on-read\tSchema-on-write Example Tool\tGoogle Cloud Storage\tBigQuery, Redshift, Snowflake Data warehouse cloud providers include GCP BigQuery, Amazon Redshift and Snowflake by Azure.",
    "section": 9
  },
  {
    "id": "doc-245",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Additional Topics about Data Warehousing Unbundling Data warehouse unbundling is the process of breaking apart a traditional, monolithic data warehouse into distinct, independently scalable components. In practice, this involves decoupling ingestion, storage, processing and compute allowing the following: Scale the the two independently Adopt best-of-breed tools because of modularity leading to better performance, agility and innovation. Improve agility and maintenance Not all data warehouses are unbundled so be sure to check out if the one you want to use is. OLAP vs OLTP: Online analytical processing (OLAP) and online transaction processing (OLTP) are data processing systems that help you store and analyze business data. You can collect and store data from multiple sources‚Äîsuch as websites, applications, smart meters, and internal systems. OLAP combines and groups the data so you can analyze it from different points of view.",
    "section": 10
  },
  {
    "id": "doc-246",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Conversely, OLTP stores and updates transactional data reliably and efficiently in high volumes. OLTP databases can be one among several data sources for an OLAP system. Both online analytical processing (OLAP) and online transaction processing (OLTP) are database management systems for storing and processing data in large volumes. The primary purpose of online analytical processing (OLAP) is to analyze aggregated data, while the primary purpose of online transaction processing (OLTP) is to process database transactions. You use OLAP systems to generate reports, perform complex data analysis, and identify trends. In contrast, you use OLTP systems to process orders, update inventory, and manage customer accounts. A data warehouse is an OLAP solution.",
    "section": 11
  },
  {
    "id": "doc-247",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Feature\tOLTP\tOLAP Purpose\tManage and process real-time transactions/business operations\tAnalyze large volumes of data to support decision-making Data Updates\tShort, fast updates initiated by users\tData periodically refreshed with scheduled, long-running batch jobs Database Design\tNormalized databases for efficiency and consistency\tDenormalized databases using star/snowflake schemas for analytical queries Space Requirements\tGenerally small (if historical data is archived)\tGenerally large due to aggregating large datasets Response Time\tMilliseconds ‚Äì optimized for speed\tSeconds or minutes ‚Äì optimized for complex queries Backup and Recovery\tFrequent backups required for business continuity\tData can be reloaded from OLTP systems in lieu of regular backups Productivity\tIncreases productivity of end-users and transaction handlers\tIncreases productivity of analysts, executives, and decision-makers Data View\tDetailed, day-to-day business transactions\tAggregated, multi-dimensional view of enterprise data Example Applications\tOrder processing, payments, inventory updates\tTrend analysis, forecasting, executive dashboards User Examples\tCustomer-facing staff, clerks, online shoppers\tBusiness analysts, data scientists, senior management Data Structure\tRow-based storage\tColumnar storage (in most modern OLAP systems like BigQuery, Redshift, etc.) Examples by Provider\tGoogle Cloud SQL, Amazon Aurora, Cloud Spanner\tBigQuery, Amazon Redshift, Snowflake 6.",
    "section": 12
  },
  {
    "id": "doc-248",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Analytics Engineering and Data Modeling What Is Analytics Engineering? Analytics engineering is a field that seeks to bridge the gap between data engineering and data analysis. It introduces good software engineering practices (such as modularity, version control, testing, documentation and DRY) to the efforts of data analysts and data scientists. This is done by the use of tools such as dbt, dataform, aws glue and sqlmesh. Data Modeling: Data modelling is the process of defining and organizing the structure of data within a system or database to ensure consistency, clarity, and usability. It involves creating abstract representations (models) of how data is stored, connected, and processed. There are three levels of data models: Conceptual - High-level view of business entities and relationships (dimensions tables) Logical - Defines the structure and attributes of data without database-specific constraints.",
    "section": 13
  },
  {
    "id": "doc-249",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Measurements, metrics or facts (Facts tables) Physical - Maps the logical model to actual database schemas, tables, indexes, and storage. ETL vs ELT We can either use ELT or ETL when transforming data. The letters represent the same words (Extract, Load, Transform) but the order matters. Feature\tETL\tELT Data Volume\tSmall\tLarge Transformation Time\tBefore loading\tAfter loading Flexibility\tLower\tHigher Cost\tHigher compute\tLower overall In terms of data modelling, I used cloud dbt because it is easy to use and allows for integration with Github. It does have a limit of one dbt project for the non-premium account so I did have to delete a previous project. Instead of having data duplication by uploading all my data from the GCS bucket into BigQuery, I used dbt to create external tables which reference data without having to load it into BigQuery‚Äôs native storage. The trade off here is that performance in areas such as access and querying may be a little slow due to on-the-fly reading.",
    "section": 14
  },
  {
    "id": "doc-250",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "I also used dbt to partition and cluster my tables in bigQuery. Partitioning splits a table into segments (partitions) based on the values of a specific column (usually date/time or integer range). Each partition stores a subset of the table‚Äôs data, and queries can skip entire partitions that aren‚Äôt relevant. The same query processes less data for a partitioned table than for a non-partitioned table thus saving both time and money for queries that are frequently run. You can have a maximum of 4000 partitions in a table. Clustering, on the other hand, organizes rows within a partition (or unpartitioned table) based on the values in one or more columns. It enables fine-grained pruning of data during query execution and optimizes filter operations, joins and aggregations by organizing data on disk. You can specify up to 4 columns to cluster by; They must be top level and non-repeated fields. Big query performs automatic reclustering for newly added data at no cost.",
    "section": 15
  },
  {
    "id": "doc-251",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Feature\tPartitioning\tClustering Granularity\tCoarse (splits table into partitions)\tFine (organizes rows within partitions/tables) Basis\tOne column (DATE/TIMESTAMP/INTEGER)\tUp to 4 columns (any type) Performance\tSkips entire partitions\tSkips blocks of rows within a table Cost Efficiency\tReduces scan by entire partitions. Cost is predictable\tReduces scan via pruning but cost benefit varies Storage Layout\tLogical partitioning (physically separated partitions)\tPhysical sorting within storage blocks Best Used For\tTime-series or log data\tFrequently filtered or grouped columns with repetition Limitations\tMax 4000 partitions per table\tMax 4 clustering columns; no nested/repeated fields 7. Batch vs Streaming In data engineering and big data, there is usually large amounts of data being generated all the time. A data engineer will thus need to decide whether to process the data as it comes (streaming) or batch it up and process it as intervals.",
    "section": 16
  },
  {
    "id": "doc-252",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "How to decide between streaming and batch A good heuristic to follow is to only use streaming when there is an automated response to the data at the end of the pipeline instead of just a human analyst looking at the data (that'd be over engineering). As such, use cases for streaming include fraud detection, hacked account detection and surge pricing (uber). Batch is best for use cases where data is generated in large volumes but not continuously and can be processed in intervals. You can even use micro batching (15 and 60 minute batches) in case you have a lot of data but not enough to justify streaming. Streaming uses a pub-sub model where publishers publish data and subscribers read and process this data. Data is transmitted in packets known as topics and each topic stores it‚Äôs own timestamp. The use cases for streaming in analytical data is low (which is the main data that data engineers mostly use).",
    "section": 17
  },
  {
    "id": "doc-253",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Streaming is more like owning a server, website or rest API rather than a batch pipeline or offline process. It is much more complex and some organizations even have different names for batch and streaming data engineers. (e.g., at Netflix, Data engineers handle batch processing whereas SWE, data handle stream data processing). In terms of technology, Apache spark is used for batch processing, Apache Kafka is used for streaming and Apache Flink supports both batch and stream processing but it was built for stream processing. 8. Exposure: Visualization and Predictions In data engineering - especially in the context of modern data tooling like dbt - an exposure refers to the end-use or downstream dependency of data models that shows where and how the data is being used outside of the transformation layer. Example exposures can be assets such as dashboards, machine learning models, external reports and APIs. My exposure was, of course, the Looker Studio dashboard (shown below).",
    "section": 18
  },
  {
    "id": "doc-254",
    "source": "data_engineering_article",
    "title": "Data Engineering Concepts: A project based introduction",
    "content": "Image description I only used my final fact table in the dashboard as I had condensed all my previous staging and dimension tables into it using dbt. dbt data modelling Some of the insights I got were that: Software development is the highest paying industry. Top-paying skills: Sales, IT, Management, Manufacturing Average required experience: 6 years Conclusion I learned a lot during the Zoomcamp and capstone project. The hands-on nature, real-world tooling, and community support made the journey insightful and practical. If you're interested in data engineering, I highly recommend joining a live cohort of the DataTalks Club Zoomcamp to get the full experience and earn certification. This article was just a high-level tour of the data engineering landscape‚Äîfeel free to dig deeper into any concept that intrigued you. Bon voyage!",
    "section": 19
  },
  {
    "id": "doc-255",
    "source": "contact",
    "title": "Contact",
    "content": "<!-- ===================== Contact ===================== --> <section id=\"contact\" class=\"section\" data-aos=\"fade-up\">   <div class=\"section-header\">     <h2><i class=\"fa-solid fa-envelope\"></i> Contact Me</h2>     <p>Get in touch with me directly using the form below.</p>   </div>   <div class=\"contact-form-container\">     <form action=\"https://formsubmit.co/b.mugure@alustudent.com\" method=\"POST\" class=\"contact-form\">       <!-- Disable CAPTCHA -->       <input type=\"hidden\" name=\"_captcha\" value=\"false\">       <div class=\"form-group\">         <input type=\"text\" name=\"name\" placeholder=\"Your Name\" required>       </div>       <div class=\"form-group\">         <input type=\"email\" name=\"email\" placeholder=\"Your Email\" required>       </div>       <div class=\"form-group\">         <textarea name=\"message\" placeholder=\"Your Message\" rows=\"5\" required></textarea>       </div>       <div class=\"form-actions\">         <button type=\"submit\" class=\"btn\">Send</button>       </div>     </form>   </div> </section>",
    "section": 0
  },
  {
    "id": "doc-256",
    "source": "cmd_expansions_in_shell_article",
    "title": "What exactly happens when you type ls -l *.c and hit Enter in a shell",
    "content": "In this article, I discuss What exactly happens when you type ls -l .c and hit Enter in a shell To the uninitiated, the command ls -l .c seems like a short-hand to listing all c files in the directory in long format. However, what most don‚Äôt know is how the Linux shell interprets this command. In this article, I will take you on a step by step process of what happens when you type ls -l .c on your terminal. The basics are: The command is entered and if it‚Äôs length is non-null, then it is kept in history(memory) Parsing Checking for special characters like pipes Checking if built-in commands are asked for Handling pipes if present Executing system commands & libraries by forking a child and execvp Printing current directory name and asking for next input. Step 1: The command is entered and if it‚Äôs length is non-null, then it is kept in history(memory) The command is entered into the terminal and if it is not null (empty), then memory is allocated for it‚Äôs storage.",
    "section": 0
  },
  {
    "id": "doc-257",
    "source": "cmd_expansions_in_shell_article",
    "title": "What exactly happens when you type ls -l *.c and hit Enter in a shell",
    "content": "It is also stored as part of the log‚Äôs history in case you want to see what command was entered at a certain time. An example of a null command would be when you just click enter on the terminal. If this happens, then steps 2 to 6 will be skipped and the terminal will go straight to step 7. Step 2: Parsing To parse means to segment a sentence into its constituent parts and describe their syntactic roles. In computing, parse is used in strings, texts and commands. This is the stage at which the computer will differentiate ls from -l and both of them from .c. Step 3: Checking for special characters like pipes It is at this stage that pipes and other special characters are checked. Pipes are a form of redirection of the output of one process to another process instead of the terminal window (STDOUT) for further processing. Thus, by piping, the output of one process becomes the input of another process. It is mostly used to combine operations in UNIX/LINUX systems.",
    "section": 1
  },
  {
    "id": "doc-258",
    "source": "cmd_expansions_in_shell_article",
    "title": "What exactly happens when you type ls -l *.c and hit Enter in a shell",
    "content": "This is done by the piping character ‚Äò|‚Äô. Pipes are unidirectional and data flows from left to write. Step 4: Checking if built-in commands are asked for Built in commands are stored in a library to reduce repetition when you want to perform a task. This can be done locally through shell scripting with the use of a .sh file. A command like ls is inbuilt. When read it is expanded to list files. The ‚Äî sign before l is a syntax for the arguments of a command. It is expanded to mean long form. The .c is expanded to mean all files in the current directory that end with a .c extension. Step 5: Handling pipes if present Become a member As stated earlier, a pipe is a connection between two processes such that the standard output from one process becomes the standard input of the other process. They are mostly handled using the pipe() system call.",
    "section": 2
  },
  {
    "id": "doc-259",
    "source": "cmd_expansions_in_shell_article",
    "title": "What exactly happens when you type ls -l *.c and hit Enter in a shell",
    "content": "A few notes about pipes is: They are unidirectional The pipe can be used by the creating process, as well as all its child processes, for reading and writing. One process can write to this ‚Äúvirtual file‚Äù or pipe and another related process can read from it. If a process tries to read before something is written to the pipe, the process is suspended until something is written. The pipe system call finds the first two available positions in the process‚Äôs open file table and allocates them for the read and write ends of the pipe. Press enter or click to view image in full size The process works on a FIFO (First In First out Model). Size of read and write don‚Äôt have to match. One can write 512 bytes but in a pipe data is read byte-by-byte. Step 6: Executing system commands & libraries by forking a child and execvp. System commands and libraries are executed using system calls. System calls are a way for programs to interact with the Operating System.",
    "section": 3
  },
  {
    "id": "doc-260",
    "source": "cmd_expansions_in_shell_article",
    "title": "What exactly happens when you type ls -l *.c and hit Enter in a shell",
    "content": "A computer program makes a system call when it makes a request to the operating system‚Äôs kernel. System call provides the services of the operating system to the user programs via Application Program Interface(API). It provides an interface between a process and operating system to allow user-level processes to request services of the operating system. System calls are the only entry points into the kernel system. All programs needing resources must use system calls. Some of the services provided by system calls are: Process creation and management Main memory management File access, Directory and File system management Handling device I/O (Input/output) Protection Networking The fork() system call is used to create a new process called a child process that runs concurrently with the process that made it (parent process). After a new child process is created, both processes will execute the next instruction following the fork() system call.",
    "section": 4
  },
  {
    "id": "doc-261",
    "source": "cmd_expansions_in_shell_article",
    "title": "What exactly happens when you type ls -l *.c and hit Enter in a shell",
    "content": "A child process uses the same pc(program counter), same CPU registers, same open files which are used in the parent process. One can use the execvp () call to differentiate the processes run by the parent and child processes. Step 7: Printing current directory name and asking for next input Stages one to six ensure that the command entered is executed and kept in history. The last step is the display of the current directory name and a prompt for the next command(usually $). Authored by: Benson Mugure, Sumeiya Juma.",
    "section": 5
  },
  {
    "id": "doc-262",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "In this article, I discuss how The circular Economy can save both our economy and our environment When you throw something away, where is ‚Äòaway‚Äô? There is no ‚Äòaway‚Äô! In our current economy, we take materials from the Earth, make products from them, and eventually throw them away as waste ‚Äî the process is linear. { Take resources ‚Üí make goods ‚Üí waste } Economy This can be traced back to the Industrial revolution where we started mass production of goods. All these goods needed to be sold to keep the system going and so we convinced ourselves that the goods that we have needed to keep on getting replaced with newer, fancier and better products constantly. This system has allowed for goods to get increasingly cheaper and today, the poorest people are living lives several times more comfortable and wealthier than people did 50‚Äì100 years ago. Information is at the tip of our fingers and goods are continuously becoming cheaper and more accessible. So, what could be wrong?",
    "section": 0
  },
  {
    "id": "doc-263",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Problems with linear Economy It doesn‚Äôt make efficient use of resources (which aren‚Äôt infinite) ‚Äî Cars are parked 92% of the time, 60% of office space in Europe remains unused even during work days It is incredibly wasteful ‚Äî we throw away ‚Öì of all the food we make, 8 million tonnes of plastic enter the ocean each year Resources are getting harder and more expensive to exploit People are becoming aware of this but common responses are beach cleaning, using less, banning plastics and recycling. This, however, is treating the symptoms rather than the root cause. Besides, we only collect 14% of plastic material for recycling and of this only 2% is recycled in a closed loop fashion. This is because recycling is sometimes way more expensive and labour intensive than just making more plastic. Plastics actually has its origin in the fossil fuel industry and it was one of the products that was mass produced during WW2.",
    "section": 1
  },
  {
    "id": "doc-264",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "What is the Circular Economy A circular Economy is an economic model of production and consumption where resources, raw materials and products are kept at their highest level of value and utility for as long as possible through sharing, leasing, reusing, repairing, refurbishing and recycling existing materials and products as long as possible. It seeks to eliminate waste and pollution beginning at the product design stage, circulate products and materials at their highest value for as long as possible and regenerate nature. In a circular economy, we stop waste being produced in the first place. It calls on us to rethink product design, use and even ownership. What if we don‚Äôt own something and just license it from the manufacturer? It goes deeper than the 3Rs: Reuse, Reduce, Recycle. A circular economy is about getting the most value out of the resources that we have by keeping them at the highest utility and value.",
    "section": 2
  },
  {
    "id": "doc-265",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Key Circular Economy concepts include 2 cycles (Biological & Technical), 3 Principles and a bunch of key words (Recycle, Collaborative Economy, Functional Economy, Reverse logistics, cascading, Eco-design, Waste Valorization,Remanufacture, ‚Ä¶). Switching to green energy will only solve 55% of our Carbon Emissions. The rest is through how we make our products and grow our food. Circular Economy Principles The circular economy is based on three principles, driven by design: Eliminate waste and pollution (Design out Waste & Pollution) Circulate products and materials (at their highest value)- Keep products in use for as long as possible Regenerate nature i) Eliminate waste and pollution: For many products on the market, there is no onward path after they are used. Take a crisp packet, for example. These multi-material flexible plastic packages cannot be reused, recycled or composted, so end up as waste. For products like these, waste is built in. They are designed to be disposable.",
    "section": 3
  },
  {
    "id": "doc-266",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Although it sometimes seems like waste is inevitable in certain situations, waste is actually the result of design choices. There is no waste in nature, it is a concept we have introduced. From tiny, short-lived products, like crisp packets, all the way up to seemingly permanent structures like buildings and roads, the economy is filled with things that have been designed without asking: What happens to this at the end of its life? By shifting our mindset, we can treat waste as a design flaw. In a circular economy, a specification for any design is that the materials re-enter the economy at the end of their use. By doing this, we take the linear take-make-waste system and make it circular. Eco-design means the integration of environmental aspects at all stages of a product‚Äôs life cycle (from manufacturing, distribution and use to final recovery) Many products could be circulated by being maintained, shared, reused, repaired, refurbished, remanufactured, and, as a last resort, recycled.",
    "section": 4
  },
  {
    "id": "doc-267",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Food and other biological materials that are safe to return to nature can regenerate the land, fuelling the production of new food and materials. With a focus on design, we can eliminate the concept of waste ii) Circulate products and materials (at their highest value) The second principle of the circular economy is to circulate products and materials at their highest value. This means keeping materials in use, either as a product or, when that can no longer be used, as components or raw materials. This way, nothing becomes waste and the intrinsic value of products and materials are retained. This could include business models based on sharing, so users get access to a product rather than owning it and more people get to use it over time. It could involve reuse through resale. It could mean cycles of maintenance, repair, and refurbishment.",
    "section": 5
  },
  {
    "id": "doc-268",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Reuse by: Sign partnerships with NGOs to give another life to the materials you do not need anymore (like computers‚Ä¶) or to the product in a good shape that you cannot sell (like food‚Ä¶). Extend product life by repairing materials. Encourage customers to repair the product by providing repair services or repair workshops. iii) Regenerate nature By shifting our economy from linear to circular, we shift the focus from extraction to regeneration. Instead of continuously degrading nature, we build natural capital. We employ farming practices that allow nature to rebuild soils and increase biodiversity, and return biological materials to the earth. Currently, most of these materials are lost after use and the land used to grow them is depleted of nutrients.",
    "section": 6
  },
  {
    "id": "doc-269",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Waste Valorization (the process of reusing, recycling or composting waste materials and converting them into more useful products including materials, chemicals, fuels or other sources of energy) is achieved by: Transform food waste into compost. Segregate and collect materials to help the recycling. Use renewable energy and bio-based or fully recyclable inputs. Resource recovery: Recover useful resources out of materials, by-products, or waste. Use bioenergy, bio-based materials, biocatalysts, hydroponics and aeroponics resources. Transitioning to renewable energy alone will only tackle 55% of global greenhouse gas emissions. The rest comes from the way we make and use products and food, and manage land ‚Äî this is where the circular economy comes in. These regenerative food production practices include agroecology, conservation agriculture, and agroforestry (growing trees around or among crops or pasture).",
    "section": 7
  },
  {
    "id": "doc-270",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "The economic, health, and environmental benefits of a circular economy for food alone would be worth USD 2.7 trillion a year by 2050. By adopting circular economy principles, the food industry could halve its projected greenhouse gas emissions in 2050. The technical & biological cycles Our natural environment is made up of two types of materials: Biodegradable and non-biodegradable. These give rise to the biological (on the left) and technical (on the right) cycles respectively, as shown in the Butterfly diagram below. Press enter or click to view image in full size The Technical Cycle Become a member It is made up of materials that don‚Äôt biodegrade so metals, plastics, polymers (things that you‚Äôd want to recover in a circular economy and feed them back into the system through chemical/physical recycling).",
    "section": 8
  },
  {
    "id": "doc-271",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "The processes that help us to get maximum value out of these materials include: Maintenance Reuse/Redistribution Refurbishment/Remanufacture Recycling If designers thought about how their product could fit into the technical or biological cycles after use, that product could be made with that onward path in mind. For example, products destined for technical cycles would benefit from being easy to repair and maintain, easy to take apart, and made of modular components that can be replaced. They could be durable enough to withstand the wear and tear of many users. And they could be made from materials that are easily recycled. We need to redesign products so that valuable metals, alloys and polymers can maintain their quality and continue to be useful beyond the shelf-life of individual products. Speaking of design, it is at this stage of manufacturing that we can rethink the life cycle of the products we are making.",
    "section": 9
  },
  {
    "id": "doc-272",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Think of a phone for a moment, it‚Äôs very possible to recycle 80‚Äì100% of the materials that make up a phone but the value won‚Äôt be the same. In Recycling the value of the product itself is lost but the value of the materials isn‚Äôt. The value was in the phone so making a long lasting phone is better than making a better phone-recycling system. Thus, recycling is a last resort kind of effort. When it comes to remanufacturing, think of car engines. A car engine remade from a broken car engine has 80% less energy than the original engine. In this perspective, it would be thus better to re-engineer the production of engines for remanufacture or shelf-life rather than recycling. This would ensure there is more energy from engines that have been manufactured from broken ones since you would be changing much less parts and these would be engineered to be easily compatible. This would save so much money and resources.",
    "section": 10
  },
  {
    "id": "doc-273",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Cars spend most of the time parked (like 90% of the time) thus it might seem like a waste or even a loss to own one. Even when it is being used it has only got one or 2 people a majority of the time. It would thus make more sense to turn it into a service that one can access (ridesharing, leasing) rather than a product that one can own. This would reduce congestion, it would improve the efficiency of the entire transport system, enable those who can‚Äôt afford a car but can afford a ride to access the service. This could end up influencing how cars are manufactured to begin with e.g., car manufacturers will start seeing car sales drop as less people will want to own cars & therefore they will start to design cars that can be remanufactured & reassembled so that they can get maximum value out of them in their whole life cycle. Repair & maintenance keeps things in use at their highest original level for as long as possible.",
    "section": 11
  },
  {
    "id": "doc-274",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "It fixes things so that they do not need to be recycled or remanufactured. Some may want to repair their own stuff while others may not & these are ideas for designers & product managers to explore more of. Sharing does just need to be with people you know. It is mostly for things that we might own but rarely use. The sharing economy prioritizes performance & access rather than ownership of a resource that you‚Äôll barely use. People don‚Äôt simply buy products or services; they ‚Äúhire‚Äù them to make progress in specific circumstances. We call this progress their ‚ÄúJob to Be Done,‚Äù and understanding this opens a world of innovation possibilities. Ridesharing is a common example but also consider something like a drill, most people have one, they rarely use it & yet it is something that will break down after only a few uses because it is probably on the lower end of the market (rarely would one buy an expensive power drill).",
    "section": 12
  },
  {
    "id": "doc-275",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "In this example, you could have great quality power drills as a service that is shared when one needs it. In essence, sharing doesn‚Äôt need to be with people that you know, it can be done in a much more sophisticated manner. A higher level of this would be the example of Uber, AirBnB and even cloth/closet sharing that‚Äôs happening in China. The gig economy falls here in that you do not need to employ (a product) someone fully (owning them for 40 hours a week); You can hire them when you need to get something done (a service). Recycling is the action or process of converting waste into reusable material. Recycling begins at the end ‚Äî the ‚Äòget rid‚Äô stage of a product‚Äôs lifecycle. The circular economy, however, goes right back to the beginning to prevent waste and pollution from being created in the first place. In the face of our current environmental challenges, recycling won‚Äôt be enough to overcome the sheer amount of waste we produce. No item can be 100% recycled.",
    "section": 13
  },
  {
    "id": "doc-276",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Even aluminum with 90% recyclability still loses 10% each time it is recycled. Thus, recycling only slows down the loss of value. Recycling is what you might call ‚Äòend-of-pipe‚Äô, while a circular economy‚Äôs ‚Äòupstream‚Äô solutions address potential problems right at the source. While recycling is undoubtedly a necessary component, we need to ensure that products and materials are designed, from the outset, to be reused, repaired, and remanufactured. It‚Äôs the consequences of decisions made at the design stage that determine around 80% of environmental impacts. The Biological Cycle Biological cycle is made up of biodegradable materials (food, wood, cotton). Biodegradable products such as food & wood-based products can be cycled in biological cycles.",
    "section": 14
  },
  {
    "id": "doc-277",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Normally these materials would return to the environment naturally after use but today this cycle is being interfered with in the way the design stage of materials is being handled e.g., by combining technical & biological materials in manners that make it difficult to recover either e.g., in polycotton clothes. Further value can be created by cascading them for additional applications in different value streams. Cascading comes in handy here where a wood can be made into a table (maintained for long because it has a purpose) then at the end of its life as a table it can be used to make a particle board/panel and finally it can go down as compost if it is intelligently designed from the onset. In a bio-refinery, conversion processes can produce high value chemicals and fuels. Organic material that can‚Äôt be used further can be composted/anaerobically digested to extract valuable nutrients such as Nitrogen, Phosphorous, Potassium & micro-nutrients.",
    "section": 15
  },
  {
    "id": "doc-278",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "The concept is to use materials for as long as possible. Think of a cotton cloth. With intelligent design during manufacture, it could serve as padding or sound-insulation then finally if it‚Äôs non-toxic, organic & biodegradable, it could feed back into that biological system by decomposition/compost. If you use toxic/non-biodegradable dye on it, however, you break the cycle & it can thus only be linear. The same goes for if you write using non-biodegradable ink on paper. The paper cannot be recycled into a cereal box, for example. Regenerative mindset comes into play here when we try to make it all come full circle instead of only being consumptive and exploitive. If products like wooden furniture were designed ‚Äî as well as to be easy to maintain and repair ‚Äî with the biological cycle in mind, their biodegradable materials (like wood) would be easily separated from their technical materials (like screws) and if glues and paints were used they would be biodegradable.",
    "section": 16
  },
  {
    "id": "doc-279",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Other products, like takeaway food containers, can be designed to be compostable after one use so that they increase the chances of the food scraps they contain returning to the soil. In order for products to successfully be circulated in either the biological or the technical cycle, it is essential they have been designed with their eventual circulation in mind. There are many products in our current economy that cannot be circulated in either cycle and end up as waste. There are products that fuse technical and biological materials in such a way that we can‚Äôt separate them and circulate them ‚Äî for example, textiles that blend natural and plastic fibres. Emerging Circular Economy Business Models Press enter or click to view image in full size Sharing Economy (Collaborative Economy) ‚Äî Collaborative economy means connecting product users to one another and encouraging shared use, access, or ownership to increase product use.",
    "section": 17
  },
  {
    "id": "doc-280",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Examples include ridesharing services such as Lyft and Uber Service, not Products (Economy of Functionality) ‚Äî Includes laundromats. The main difference between between a sharing economy (platform) and an economy of functionality is that in an economy of functionality there is centralization of ownership (mostly a single company) of the product being provided as a service whereas in a collaborative economy the company is just a platform to coordinate access to the resources but does not necessarily own them.",
    "section": 18
  },
  {
    "id": "doc-281",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Resource Recovery and Recycling (Remanufacture & reverse logistics) ‚Äî Reverse logistics refers to the activities that a company engages in in order to have their product returned to them at the end of its life cycle Product Life extension ‚Äî This is incorporated at the product design stage for longer shelf life as well as ease of repair Circular Supplies ‚Äî The manufacture of biodegradable plastics as well as materials that are easy to remanufacture and recycle Press enter or click to view image in full size Circular Economy Business Model Tenets Circular sourcing ‚Äî Sourcing recycled or renewable materials that can be returned to either the technical or biological cycle. Co-product recovery ‚Äî Residual/Secondary outputs from one process (or value chain) become inputs for another process (or value chain). Re-make ‚Äî Manufacturing steps acting on an end-of-life cycle part or product to return it to like-new or better performance, with warranty to match.",
    "section": 19
  },
  {
    "id": "doc-282",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "Re-condition ‚Äî Fixing of a fault/aesthetic improvement of a product, but with no new/additional warranty on the product. Includes repair and refurbishment. Performance ‚Äî Focus on guaranteed performance level or outcome based on the functionality of a product/asset. Typically provided as a product-service bundle. Access/Economy of Functionality ‚Äî Providing end-users with access to the functionality of products/assets instead of ownership. Resource recovery/Reverse Logistics ‚Äî Materials or products at end-of-cycle are incorporated into different products or used as feedstock/inputs for another process (or value chain). Opportunities in Africa (Low lying fruits): Press enter or click to view image in full size The Circular Economy has lots of benefits, not just for the environment but for society as well. Concepts such as the Collaborative economy allow middle to low-income households to access services that they were previously not consuming. In this way they create a market out of them.",
    "section": 20
  },
  {
    "id": "doc-283",
    "source": "circular_economy_article",
    "title": "The circular Economy: How to save both our economy and our environment",
    "content": "This is not the only way the circular economy coalesces with market creating innovation. The concept of jobs to be done is also ingrained into the service, not product model of the functional economy that is a segment of the circular economy. Although the concept of circular economy may appeal more to entrepreneurs, intrapreneurs should also be involved. Click here for a visual of organizations that are embracing the circular economy in Africa and here for resources on how to incorporate circular economy concepts in your current organization. For the savvy ones who might want to go even further, here is a full report on the circular economy in Africa.",
    "section": 21
  },
  {
    "id": "doc-284",
    "source": "certification",
    "title": "Certification",
    "content": "<!-- ===================== Certification ===================== --> <section id=\"Certification\" class=\"section\" data-aos=\"fade-up\">   <div class=\"section-header\">     <h2>üìú Certification</h2>     <a class=\"view-all\" href=\"https://www.linkedin.com/in/benson-mugure-017153196/details/certifications/\" target=\"_blank\" rel=\"noopener\">All certificates ‚Üí</a>   </div>   {% assign certificates_count = site.data.certificates | size %}   {% if certificates_count > 4 %}     <div class=\"carousel-wrapper\">       <button class=\"scroll-btn left\" data-target=\"certificates-track\" aria-label=\"Scroll certificates left\">‚Äπ</button>       <div id=\"certificates-track\" class=\"carousel-track\" role=\"region\" aria-label=\"certificates list\">         {% for item in site.data.certificates %}         <article class=\"card\" data-aos=\"zoom-in\" data-aos-delay=\"100\">           <a class=\"thumb\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"Open video\">             <img src=\"{{ item.image | default: '/assets/images/placeholder_video.jpg' | relative_url }}\"                  alt=\"{{ item.title | escape }} thumbnail\"                  loading=\"lazy\"                  {% if item.preview_gif %}data-preview=\"{{ item.preview_gif | relative_url }}\"{% endif %}>           </a>           <div class=\"card-body\">             <h3 class=\"card-title\"><a href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>             {% if item.note %}<p class=\"card-text\">{{ item.note }}</p>{% endif %}             <div class=\"card-actions\">               {% if item.screenshot %}<a href=\"\" class=\"btn ghost\" data-lightbox-src=\"{{ item.screenshot | relative_url }}\">Preview</a>{% endif %}               <a class=\"btn\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">View</a>             </div>           </div>         </article>         {% endfor %}       </div>       <button class=\"scroll-btn right\" data-target=\"certificates-track\" aria-label=\"Scroll certificates right\">‚Ä∫</button>     </div>   {% else %}     <div class=\"gallery\">       {% for item in site.data.certificates %}       <article class=\"card\">         <a class=\"thumb\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"Open video\">           <img src=\"{{ item.image | default: '/assets/images/placeholder_video.jpg' | relative_url }}\"                alt=\"{{ item.title | escape }} thumbnail\" loading=\"lazy\">         </a>         <div class=\"card-body\">           <h3 class=\"card-title\"><a href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>           {% if item.note %}<p class=\"card-text\">{{ item.note }}</p>{% endif %}           <div class=\"card-actions\">             {% if item.screenshot %}<a href=\"\" class=\"btn ghost\" data-lightbox-src=\"{{ item.screenshot | relative_url }}\">Preview</a>{% endif %}             <a class=\"btn\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">Watch</a>           </div>         </div>       </article>       {% endfor %}     </div>   {% endif %} </section>",
    "section": 0
  },
  {
    "id": "doc-285",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "In this article, I discuss how Deploy Your Site in Seconds Using AWS Amplify In today‚Äôs fast-paced digital world, deploying a web application quickly and reliably is crucial. AWS Amplify provides a seamless way to get your site online in seconds. To demonstrate its power, I built Choicepool, an interactive web app designed to simplify two-way door decision-making through fun games like coin flips, dice rolls, and rock-paper-scissors. The project combines interactivity with ease of deployment, showcasing how AWS Amplify can power robust web applications. This post will introduce the Choicepool project, compare various hosting options, highlight the benefits of AWS Amplify, and guide you through deploying your site using Amplify and its integrations. About Choicepool Choicepool is a simple yet engaging web app built with HTML, CSS, and JavaScript. It allows users to input their choices and randomly pick one through gamified experiences.",
    "section": 0
  },
  {
    "id": "doc-286",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "By leveraging games, Choicepool makes decision-making both efficient and fun. The app was deployed using AWS Amplify, illustrating how simple and fast deployment can be when using the right tools. You can find choicepool here Hosting Options for Web Apps When deploying a web app, choosing the right hosting provider is vital. Below are common hosting options, along with their pros and cons: 1. GitHub Pages Pros: Free for public repositories. Easy to deploy static websites. Well-integrated with GitHub repositories. Cons: Limited to static sites; no backend support. Lacks advanced scalability options. 2. Netlify Pros: Simple CI/CD for static and serverless apps. Built-in features like form handling and serverless functions. Cons: Costs can increase with higher traffic. Less powerful integration with backend services. 3. Vercel Pros: Optimized for React and Next.js apps. Automatic builds and global CDN. Cons: Limited support for backend integrations.",
    "section": 1
  },
  {
    "id": "doc-287",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "Pricing tiers can become costly for large teams. 4. AWS Amplify AWS Amplify is a game-changer for hosting web apps, offering seamless deployment and integration with other AWS services. Pros: Ease and Speed of Deployment: Amplify makes deployment fast, whether through uploading zipped files or connecting a Git repository. Powerful Integrations: Supports services like Amazon S3 for file storage, DynamoDB for databases, and Lambda for serverless functions. Scalability: Automatically scales with traffic demands. Built-in CI/CD Pipelines: Automates builds and deployments directly from your Git repositories. Rich Feature Set: Includes hosting, authentication, analytics, and AI/ML capabilities via other AWS services. Cons: Initial learning curve for those new to AWS services. Costs can increase with extensive feature usage. Step-by-Step Guide to Hosting with AWS Amplify Amplify simplifies the deployment process, making it accessible to both beginners and experienced developers.",
    "section": 2
  },
  {
    "id": "doc-288",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "Here‚Äôs how you can deploy your site: 1. Prepare Your Web App Ensure your web app is ready for deployment. This involves: Testing your app locally. Ensuring all assets (CSS, JavaScript, images) are included. 2. Create an AWS Account If you don‚Äôt already have an AWS account, create one at AWS. Navigate to the AWS Management Console and search for Amplify. 3. Deploy Your Site Option 1: Upload a Zipped File Compress your project folder into a .zip file. Go to the Amplify console and select Host a Web App. Upload your zipped file. Amplify will handle the rest, generating a live URL for your site. Option 2: Deploy from GitHub, GitLab, or Bitbucket In the Amplify console, select Host a Web App. Connect your GitHub, GitLab, or Bitbucket account. Choose the repository and branch you want to deploy. Amplify will build and deploy your app. 4. View Your Hosted Site Once deployed, Amplify generates a live URL for your site. You can share this link or configure a custom domain.",
    "section": 3
  },
  {
    "id": "doc-289",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "Enhancing Deployments with Amazon Q Developer Amazon Q Developer, a powerful tool for AWS users, enhances Amplify‚Äôs deployment capabilities by integrating intelligence into the CI/CD pipeline. Advantages of Amazon Q Developer Automated Insights: Identify potential issues during deployment. Optimized Resource Allocation: Suggests configurations to reduce costs. Simplified Integration: Easily integrates with other AWS services. How to Gain Access Amazon Q Developer is available through the AWS Management Console. To use it: Navigate to the AWS Marketplace. Search for Amazon Q Developer. Follow the instructions to enable it for your account. Why AWS Amplify Stands Out Amplify-hosted sites are highly versatile and capable. As demonstrated by Choicepool, a simple app built entirely with HTML, CSS, and JavaScript, Amplify can host interactive applications with minimal setup.",
    "section": 4
  },
  {
    "id": "doc-290",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "Examples of Amplify‚Äôs Power: API Calls to ML Models: With JavaScript, you can make API calls to AWS SageMaker endpoints, enabling advanced AI capabilities in your app. Database Integrations: Amplify supports direct integration with AWS DynamoDB, allowing real-time data storage and retrieval. Standalone Apps: Amplify handles hosting and scaling, so even complex apps can run independently without additional backend infrastructure. What‚Äôs Next for Choicepool AWS Amplify continues to push the boundaries of what‚Äôs possible with web app hosting. For Choicepool, future updates might include: Sound effects and animations for an even more interactive experience. Support for multiple languages to reach a broader audience. User preference tracking through Amplify‚Äôs built-in analytics and backend integrations. Conclusion AWS Amplify is a powerful tool for developers seeking fast, scalable, and feature-rich hosting.",
    "section": 5
  },
  {
    "id": "doc-291",
    "source": "aws_amplify_article",
    "title": "Deploy Your Site in Seconds Using AWS Amplify",
    "content": "By enabling rapid deployment and seamless integration with AWS services, Amplify allows developers to focus on building impactful applications. Whether it‚Äôs a simple decision-making app like Choicepool or a complex AI-driven platform, Amplify ensures your project is ready to reach the world in seconds. Ready to deploy your site? Head to AWS Amplify and get started today!",
    "section": 6
  },
  {
    "id": "doc-292",
    "source": "articles",
    "title": "Articles",
    "content": "<!-- ===================== Articles ===================== --> <section id=\"articles\" class=\"section\" data-aos=\"fade-up\">   <div class=\"section-header\">     <h2>‚úçÔ∏è Articles</h2>     <a class=\"view-all\" href=\"https://dev.to/virgoalpha\" target=\"_blank\" rel=\"noopener\">dev.to ‚Üí</a>   </div>   {% assign articles_count = site.data.articles | size %}   {% if articles_count > 4 %}     <div class=\"carousel-wrapper\">       <button class=\"scroll-btn left\" data-target=\"articles-track\" aria-label=\"Scroll articles left\">‚Äπ</button>       <div id=\"articles-track\" class=\"carousel-track\" role=\"region\" aria-label=\"Articles list\">         {% for item in site.data.articles %}         <article class=\"card\" data-aos=\"zoom-in\" data-aos-delay=\"100\">           <a class=\"thumb\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"Open article\">             <img src=\"{{ item.image | default: '/assets/images/placeholder_article.jpg' | relative_url }}\"                  alt=\"{{ item.title | escape }} thumbnail\"                  loading=\"lazy\"                  {% if item.preview_gif %}data-preview=\"{{ item.preview_gif | relative_url }}\"{% endif %}>           </a>           <div class=\"card-body\">             <h3 class=\"card-title\"><a href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>             {% if item.subtitle %}<p class=\"card-text\">{{ item.subtitle }}</p>{% endif %}             <div class=\"card-actions\">               {% if item.screenshot %}<a href=\"\" class=\"btn ghost\" data-lightbox-src=\"{{ item.screenshot | relative_url }}\">Preview</a>{% endif %}               <a class=\"btn\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">Read</a>             </div>           </div>         </article>         {% endfor %}       </div>       <button class=\"scroll-btn right\" data-target=\"articles-track\" aria-label=\"Scroll articles right\">‚Ä∫</button>     </div>   {% else %}     <div class=\"gallery\">       {% for item in site.data.articles %}       <article class=\"card\">         <a class=\"thumb\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\" aria-label=\"Open article\">           <img src=\"{{ item.image | default: '/assets/images/placeholder_article.jpg' | relative_url }}\"                alt=\"{{ item.title | escape }} thumbnail\" loading=\"lazy\">         </a>         <div class=\"card-body\">           <h3 class=\"card-title\"><a href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">{{ item.title }}</a></h3>           {% if item.subtitle %}<p class=\"card-text\">{{ item.subtitle }}</p>{% endif %}           <div class=\"card-actions\">             {% if item.screenshot %}<a href=\"\" class=\"btn ghost\" data-lightbox-src=\"{{ item.screenshot | relative_url }}\">Preview</a>{% endif %}             <a class=\"btn\" href=\"{{ item.link }}\" target=\"_blank\" rel=\"noopener\">Read</a>           </div>         </div>       </article>       {% endfor %}     </div>   {% endif %} </section>",
    "section": 0
  }
]